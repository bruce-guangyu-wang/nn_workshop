{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D7tqLMoKF6uq"
   },
   "source": [
    "Long Short Term Memory Model\n",
    "------------\n",
    "\n",
    "The goal of this notebook is to train a LSTM character model over [Text8](http://mattmahoney.net/dc/textdata) data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "MvEblsgEXxrd"
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import string\n",
    "import tensorflow as tf\n",
    "import zipfile\n",
    "from six.moves import range\n",
    "from six.moves.urllib.request import urlretrieve\n",
    "import unicodedata\n",
    "import re\n",
    "from tika import parser\n",
    "import string\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".DS_Store\n",
      "Algorithm Portfolios Based on Cost-Sensitive Hierarchical Clustering.pdf\n",
      "Algorithm Selection and Scheduling.pdf\n",
      "An Algorithm Selection Benchmark of the Container Pre-Marshalling Problem.pdf\n",
      "Boosting Sequential Solver Portfolios - Knowledge Sharing and Accuracy Prediction.pdf\n",
      "DASH - Dynamic Approach for Switching Heuristics.pdf\n",
      "Deep Learning for Algorithm Portfolios.pdf\n",
      "Feature Filtering for Instance-Specific Algorithm Configuration.pdf\n",
      "Features for Exploiting Black-Box Optimization Problem Structure.pdf\n",
      "ISAC – Instance-Specific Algorithm Configuration.pdf\n",
      "Latent Features for Algorithm Selection.pdf\n",
      "MaxSAT by Improved Instance-Specific Algorithm Configuration.pdf\n",
      "Model-based Genetic Algorithms for Algorithm Configuration.pdf\n",
      "Non-Model-Based Search Guidance for Set Partitioning Problems.pdf\n",
      "Proteus - A Hierarchical Portfolio of Solvers and Transformations.pdf\n",
      "ReACTR - Realtime Algorithm Configuration through Tournament Rankings.pdf\n",
      "Stochastic Offline Programming.pdf\n",
      "Structure-preserving instance generation.pdf\n",
      "Transformation-based Feature Computation for Algorithm Portfolios.pdf\n",
      "Tuning Parameters of Large Neighborhood Search for the Machine Reassignment Problem.pdf\n"
     ]
    }
   ],
   "source": [
    "# load and concat papers\n",
    "papers = \"\"\n",
    "for paper in os.listdir(\"/Users/o648972/Downloads/nn_workshop/LSTM/Papers/\"):\n",
    "    print(paper)\n",
    "    if '.pdf' in paper:\n",
    "        text = parser.from_file(\"/Users/o648972/Downloads/nn_workshop/LSTM/Papers/Algorithm Selection and Scheduling.pdf\")\n",
    "        text = unicodedata.normalize('NFKD', text['content']).encode('ascii','ignore')\n",
    "        text = re.sub('\\d', '', text)\n",
    "        remove = '!\"#$%&\\'()*+,\\-/:;<=>?@[\\\\]^_`{|}~'\n",
    "        pattern = r\"[{}]\".format(remove)\n",
    "        text = re.sub(pattern, \"\", text) \n",
    "        text = re.sub('\\s+', ' ', text)\n",
    "        papers += text.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 5993,
     "status": "ok",
     "timestamp": 1445965582896,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "RJ-o3UBUFtCw",
    "outputId": "d530534e-0791-4a94-ca6d-1c8f1b908a9e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found and verified text8.zip\n"
     ]
    }
   ],
   "source": [
    "def maybe_download(filename, expected_bytes):\n",
    "    if not os.path.exists(filename):\n",
    "        filename, _ = urlretrieve(url + filename, filename)\n",
    "    statinfo = os.stat(filename)\n",
    "    if statinfo.st_size == expected_bytes:\n",
    "        print('Found and verified %s' % filename)\n",
    "    else:\n",
    "        print(statinfo.st_size)\n",
    "        raise Exception(\n",
    "          'Failed to verify ' + filename + '. Can you get to it with a browser?')\n",
    "    return filename\n",
    "\n",
    "filename = maybe_download('text8.zip', 31344016)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 5982,
     "status": "ok",
     "timestamp": 1445965582916,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "Mvf09fjugFU_",
    "outputId": "8f75db58-3862-404b-a0c3-799380597390"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data size 100000000\n"
     ]
    }
   ],
   "source": [
    "def read_data(filename):\n",
    "    f = zipfile.ZipFile(filename)\n",
    "    for name in f.namelist():\n",
    "        return tf.compat.as_str(f.read(name))\n",
    "    f.close()\n",
    "\n",
    "text = read_data(filename)\n",
    "print('Data size %d' % len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text = text[:len(papers) / 3 ] + papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text_file = open(\"paper.txt\", \"w\")\n",
    "text_file.write(text)\n",
    "text_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ga2CYACE-ghb"
   },
   "source": [
    "Create a small validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 6184,
     "status": "ok",
     "timestamp": 1445965583138,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "w-oBpfFG-j43",
    "outputId": "bdb96002-d021-4379-f6de-a977924f0d02"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1044304 ons anarchists advocate social relations based upon voluntary association of autonomous individuals \n",
      "1000  anarchism originated as a term of abuse first used against early working class radicals including t\n"
     ]
    }
   ],
   "source": [
    "valid_size = 1000\n",
    "valid_text = text[:valid_size]\n",
    "train_text = text[valid_size:]\n",
    "train_size = len(train_text)\n",
    "print(train_size, train_text[:100])\n",
    "print(valid_size, valid_text[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Zdw6i4F8glpp"
   },
   "source": [
    "Utility functions to map characters to vocabulary IDs and back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 6276,
     "status": "ok",
     "timestamp": 1445965583249,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "gAL1EECXeZsD",
    "outputId": "88fc9032-feb9-45ff-a9a0-a26759cc1f2e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected character: ï\n",
      "1 26 0 0\n",
      "a z .\n"
     ]
    }
   ],
   "source": [
    "vocabulary_size = len(string.ascii_lowercase) + 2 # [a-z] + ' ' + '.'\n",
    "first_letter = ord(string.ascii_lowercase[0])\n",
    "\n",
    "def char2id(char):\n",
    "    if char in string.ascii_lowercase:\n",
    "        return ord(char) - first_letter + 1\n",
    "    elif char == ' ':\n",
    "        return 0\n",
    "    elif char == '.':\n",
    "        return 27\n",
    "    else:\n",
    "        print('Unexpected character: %s' % char)\n",
    "        return 0\n",
    "\n",
    "def id2char(dictid):\n",
    "    if 27 > dictid > 0:\n",
    "        return chr(dictid + first_letter - 1)\n",
    "    elif dictid == 27:\n",
    "        return '.'\n",
    "    else:\n",
    "        return ' '\n",
    "\n",
    "print(char2id('a'), char2id('z'), char2id(' '), char2id('ï'))\n",
    "print(id2char(1), id2char(26), id2char(27))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lFwoyygOmWsL"
   },
   "source": [
    "Function to generate a training batch for the LSTM model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 6473,
     "status": "ok",
     "timestamp": 1445965583467,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "d9wMtjy5hCj9",
    "outputId": "3dd79c80-454a-4be0-8b71-4a4a357b3367",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "num_unrollings = 30\n",
    "\n",
    "\n",
    "class BatchGenerator(object):\n",
    "\n",
    "    def __init__(self, text, batch_size, num_unrollings):\n",
    "        self._text = text\n",
    "        self._text_size = len(text)\n",
    "        self._batch_size = batch_size\n",
    "        self._num_unrollings = num_unrollings\n",
    "        segment = self._text_size // batch_size\n",
    "        self._cursor = [offset * segment for offset in range(batch_size)]\n",
    "        self._last_batch = self._next_batch()\n",
    "\n",
    "    def _next_batch(self):\n",
    "        \"\"\"Generate a single batch from the current cursor position in the data.\"\"\"\n",
    "        batch = np.zeros(\n",
    "            shape=(self._batch_size, vocabulary_size), dtype=np.float)\n",
    "        for b in range(self._batch_size):\n",
    "            batch[b, char2id(self._text[self._cursor[b]])] = 1.0\n",
    "            self._cursor[b] = (self._cursor[b] + 1) % self._text_size\n",
    "        return batch\n",
    "\n",
    "    def next(self):\n",
    "        \"\"\"Generate the next array of batches from the data. The array consists of\n",
    "        the last batch of the previous array, followed by num_unrollings new ones.\n",
    "        \"\"\"\n",
    "        batches = [self._last_batch]\n",
    "        for step in range(self._num_unrollings):\n",
    "            batches.append(self._next_batch())\n",
    "        self._last_batch = batches[-1]\n",
    "        return batches\n",
    "\n",
    "\n",
    "def characters(probabilities):\n",
    "    \"\"\"Turn a 1-hot encoding or a probability distribution over the possible\n",
    "    characters back into its (most likely) character representation.\"\"\"\n",
    "    return [id2char(c) for c in np.argmax(probabilities, 1)]\n",
    "\n",
    "\n",
    "def batches2string(batches):\n",
    "    \"\"\"Convert a sequence of batches back into their (most likely) string\n",
    "    representation.\"\"\"\n",
    "    s = [''] * batches[0].shape[0]\n",
    "    for b in batches:\n",
    "        s = [''.join(x) for x in zip(s, characters(b))]\n",
    "    return s\n",
    "\n",
    "train_batches = BatchGenerator(train_text, batch_size, num_unrollings)\n",
    "valid_batches = BatchGenerator(valid_text, 1, 3)\n",
    "\n",
    "# print(batches2string(train_batches.next()))\n",
    "# print(batches2string(train_batches.next()))\n",
    "# print(batches2string(valid_batches.next()))\n",
    "# print(batches2string(valid_batches.next()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "KyVd8FxT5QBc"
   },
   "outputs": [],
   "source": [
    "def logprob(predictions, labels):\n",
    "    \"\"\"Log-probability of the true labels in a predicted batch.\"\"\"\n",
    "    predictions[predictions < 1e-10] = 1e-10\n",
    "    return np.sum(np.multiply(labels, -np.log(predictions))) / labels.shape[0]\n",
    "\n",
    "\n",
    "def sample_distribution(distribution):\n",
    "    \"\"\"Sample one element from a distribution assumed to be an array of normalized\n",
    "    probabilities.\n",
    "    \"\"\"\n",
    "    r = random.uniform(0, 1)\n",
    "    s = 0\n",
    "    for i in range(len(distribution)):\n",
    "        s += distribution[i]\n",
    "        if s >= r:\n",
    "            return i\n",
    "    return len(distribution) - 1\n",
    "\n",
    "\n",
    "def sample(prediction):\n",
    "    \"\"\"Turn a (column) prediction into 1-hot encoded samples.\"\"\"\n",
    "    p = np.zeros(shape=[1, vocabulary_size], dtype=np.float)\n",
    "    p[0, sample_distribution(prediction[0])] = 1.0\n",
    "    return p\n",
    "\n",
    "\n",
    "def random_distribution():\n",
    "    \"\"\"Generate a random column of probabilities.\"\"\"\n",
    "    b = np.random.uniform(0.0, 1.0, size=[1, vocabulary_size])\n",
    "    return b / np.sum(b, 1)[:, None]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "K8f67YXaDr4C"
   },
   "source": [
    "Simple LSTM Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": false,
    "id": "Q5rxZK6RDuGe"
   },
   "outputs": [],
   "source": [
    "num_nodes = 64\n",
    "\n",
    "graph1 = tf.Graph()\n",
    "with graph1.as_default():\n",
    "\n",
    "    # Parameters:\n",
    "    # Input gate: input, previous output, and bias.\n",
    "    ix = tf.Variable(tf.truncated_normal(\n",
    "        [vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "    im = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "    ib = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    # Forget gate: input, previous output, and bias.\n",
    "    fx = tf.Variable(tf.truncated_normal(\n",
    "        [vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "    fm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "    fb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    # Memory cell: input, state and bias.\n",
    "    cx = tf.Variable(tf.truncated_normal(\n",
    "        [vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "    cm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "    cb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    # Output gate: input, previous output, and bias.\n",
    "    ox = tf.Variable(tf.truncated_normal(\n",
    "        [vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "    om = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "    ob = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    # Variables saving state across unrollings.\n",
    "    saved_output = tf.Variable(\n",
    "        tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    saved_state = tf.Variable(\n",
    "        tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    # Classifier weights and biases.\n",
    "    w = tf.Variable(tf.truncated_normal(\n",
    "        [num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "    b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "\n",
    "    # Definition of the cell computation.\n",
    "    def lstm_cell(i, o, state):\n",
    "        \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "        Note that in this formulation, we omit the various connections between the\n",
    "        previous state and the gates.\"\"\"\n",
    "        input_gate = tf.sigmoid(tf.matmul(i, ix) + tf.matmul(o, im) + ib)\n",
    "        forget_gate = tf.sigmoid(tf.matmul(i, fx) + tf.matmul(o, fm) + fb)\n",
    "        update = tf.matmul(i, cx) + tf.matmul(o, cm) + cb\n",
    "        state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "        output_gate = tf.sigmoid(tf.matmul(i, ox) + tf.matmul(o, om) + ob)\n",
    "        return output_gate * tf.tanh(state), state\n",
    "        # input_gate, forget_gate, update, state, output_gate: batch_size * num_nodes\n",
    "    # Input data.\n",
    "    train_data = list()\n",
    "    for _ in range(num_unrollings + 1):\n",
    "        train_data.append(\n",
    "            tf.placeholder(tf.float32, shape=[batch_size, vocabulary_size]))\n",
    "    train_inputs = train_data[:num_unrollings]\n",
    "    train_labels = train_data[1:]  # labels are inputs shifted by one time step.\n",
    "\n",
    "    # Unrolled LSTM loop.\n",
    "    outputs = list()\n",
    "    output = saved_output\n",
    "    state = saved_state\n",
    "    for i in train_inputs:\n",
    "        output, state = lstm_cell(i, output, state)\n",
    "        outputs.append(output)\n",
    "    # outputs: num_unrollings * batch_size * num_nodes\n",
    "    # State saving across unrollings.\n",
    "    with tf.control_dependencies([saved_output.assign(output),\n",
    "                                  saved_state.assign(state)]):\n",
    "        # Classifier.\n",
    "        logits = tf.nn.xw_plus_b(tf.concat(0, outputs), w, b)\n",
    "        loss = tf.reduce_mean(\n",
    "            tf.nn.softmax_cross_entropy_with_logits(\n",
    "                logits, tf.concat(0, train_labels)))\n",
    "        # logist, train_labels: (num_unrollings * batch_size) * vocabulary_size\n",
    "    # Optimizer.\n",
    "    global_step = tf.Variable(0)\n",
    "    learning_rate = tf.train.exponential_decay(\n",
    "        10.0, global_step, 5000, 0.1, staircase=True)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "    gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "    optimizer = optimizer.apply_gradients(\n",
    "        zip(gradients, v), global_step=global_step)\n",
    "\n",
    "    # Predictions.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "\n",
    "    # Sampling and validation eval: batch 1, no unrolling.\n",
    "    sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "    saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    reset_sample_state = tf.group(\n",
    "        saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "        saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "    sample_output, sample_state = lstm_cell(\n",
    "        sample_input, saved_sample_output, saved_sample_state)\n",
    "    with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                  saved_sample_state.assign(sample_state)]):\n",
    "        sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))\n",
    "    saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 41
      },
      {
       "item_id": 80
      },
      {
       "item_id": 126
      },
      {
       "item_id": 144
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 199909,
     "status": "ok",
     "timestamp": 1445965877333,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "RD9zQCZTEaEm",
    "outputId": "5e868466-2532-4545-ce35-b403cf5d9de6",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-122-0e1c7dedf858>:5 in <module>.: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "Initialized\n",
      "Average loss at step 0: 3.335164 learning rate: 10.000000\n",
      "Minibatch perplexity: 28.08\n",
      "================================================================================\n",
      " t njen tncbleakoe mnb e.o xfomc m cakiw  j w q ynm shmqqhoheoskoe uediyoncbe be\n",
      ".tb hz nrtrvkgl tnchpox .j xdz eo ot opghdbkcrmii cec omivnipjv rvr .  mtcfjvz .\n",
      "qohqnktnescsnv xocawotqfknnwux et gezleu ytdehext.hgyseieiibwaev bmgoad  ekw rgs\n",
      " ni mqjkflmdi.zlcurd tgyr rl jra suinh. g ie ..bxyjivyc yttotiimwur slafpqise gj\n",
      " felchnn  ht en vafa mri oc.ohttrihlw h eates styhn dsyarescictoxrwxlnpmpacbi ec\n",
      "================================================================================\n",
      "Validation set perplexity: 20.54\n",
      "Average loss at step 200: 2.370696 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.01\n",
      "Validation set perplexity: 16.56\n",
      "Average loss at step 400: 1.829353 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.15\n",
      "Validation set perplexity: 20.92\n",
      "Average loss at step 600: 1.607214 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.98\n",
      "Validation set perplexity: 23.95\n",
      "Average loss at step 800: 1.487532 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.38\n",
      "Validation set perplexity: 27.79\n",
      "Average loss at step 1000: 1.406361 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.10\n",
      "Validation set perplexity: 31.52\n",
      "Average loss at step 1200: 1.359849 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.04\n",
      "Validation set perplexity: 35.55\n",
      "Average loss at step 1400: 1.318494 learning rate: 10.000000\n",
      "Minibatch perplexity: 3.66\n",
      "Validation set perplexity: 37.54\n",
      "Average loss at step 1600: 1.285864 learning rate: 10.000000\n",
      "Minibatch perplexity: 3.69\n",
      "Validation set perplexity: 34.99\n",
      "Average loss at step 1800: 1.266064 learning rate: 10.000000\n",
      "Minibatch perplexity: 3.46\n",
      "Validation set perplexity: 38.54\n",
      "Average loss at step 2000: 1.249046 learning rate: 10.000000\n",
      "Minibatch perplexity: 3.69\n",
      "================================================================================\n",
      "x by the privoming or not section the co riditan in allocing the bentamic broods\n",
      "ve successfedten and h. varted and siminate and sloth the was astances to solve \n",
      "kil portfolio unfortunal the cost of intersa willines husfered by and during cas\n",
      "referfound the fixed instances. table neighborhood usent of the performance can \n",
      "chards a r a more letting phedors a litz the resulted will of no shenr competati\n",
      "================================================================================\n",
      "Validation set perplexity: 39.93\n",
      "Average loss at step 2200: 1.231572 learning rate: 10.000000\n",
      "Minibatch perplexity: 3.59\n",
      "Validation set perplexity: 39.11\n",
      "Average loss at step 2400: 1.219008 learning rate: 10.000000\n",
      "Minibatch perplexity: 3.33\n",
      "Validation set perplexity: 39.06\n",
      "Average loss at step 2600: 1.200729 learning rate: 10.000000\n",
      "Minibatch perplexity: 3.17\n",
      "Validation set perplexity: 44.59\n",
      "Average loss at step 2800: 1.197930 learning rate: 10.000000\n",
      "Minibatch perplexity: 3.37\n",
      "Validation set perplexity: 45.15\n",
      "Average loss at step 3000: 1.189452 learning rate: 10.000000\n",
      "Minibatch perplexity: 3.27\n",
      "Validation set perplexity: 47.24\n",
      "Average loss at step 3200: 1.176806 learning rate: 10.000000\n",
      "Minibatch perplexity: 3.43\n",
      "Validation set perplexity: 48.26\n",
      "Average loss at step 3400: 1.179572 learning rate: 10.000000\n",
      "Minibatch perplexity: 3.24\n",
      "Validation set perplexity: 45.81\n",
      "Average loss at step 3600: 1.169853 learning rate: 10.000000\n",
      "Minibatch perplexity: 3.25\n",
      "Validation set perplexity: 47.94\n",
      "Average loss at step 3800: 1.155910 learning rate: 10.000000\n",
      "Minibatch perplexity: 3.47\n",
      "Validation set perplexity: 45.29\n",
      "Average loss at step 4000: 1.155971 learning rate: 10.000000\n",
      "Minibatch perplexity: 3.32\n",
      "================================================================================\n",
      "mark and sibvilized in such by failly chostsplity has optimal schedule to this s\n",
      "x for satzilla r knnd its il fitted in in jritminine emper procead onervat of so\n",
      "zers clo versaids of mextain the gnea as ary the earlogies at i char. pitoro hin\n",
      "gorh. samgyation using which his naelows to due the portfolio netesses based bre\n",
      "char . marchnn . m. evender . . omphamk . an anditel neighborhood sizh. muchater\n",
      "================================================================================\n",
      "Validation set perplexity: 49.15\n",
      "Average loss at step 4200: 1.147708 learning rate: 10.000000\n",
      "Minibatch perplexity: 2.87\n",
      "Validation set perplexity: 48.61\n",
      "Average loss at step 4400: 1.147672 learning rate: 10.000000\n",
      "Minibatch perplexity: 3.16\n",
      "Validation set perplexity: 47.04\n",
      "Average loss at step 4600: 1.147863 learning rate: 10.000000\n",
      "Minibatch perplexity: 3.25\n",
      "Validation set perplexity: 46.94\n",
      "Average loss at step 4800: 1.133362 learning rate: 10.000000\n",
      "Minibatch perplexity: 3.32\n",
      "Validation set perplexity: 48.60\n",
      "Average loss at step 5000: 1.139455 learning rate: 1.000000\n",
      "Minibatch perplexity: 3.15\n",
      "Validation set perplexity: 49.45\n",
      "Average loss at step 5200: 1.067032 learning rate: 1.000000\n",
      "Minibatch perplexity: 2.75\n",
      "Validation set perplexity: 50.51\n",
      "Average loss at step 5400: 1.033488 learning rate: 1.000000\n",
      "Minibatch perplexity: 2.98\n",
      "Validation set perplexity: 54.91\n",
      "Average loss at step 5600: 1.028023 learning rate: 1.000000\n",
      "Minibatch perplexity: 2.93\n",
      "Validation set perplexity: 56.25\n",
      "Average loss at step 5800: 1.028087 learning rate: 1.000000\n",
      "Minibatch perplexity: 2.67\n",
      "Validation set perplexity: 55.88\n",
      "Average loss at step 6000: 1.015168 learning rate: 1.000000\n",
      "Minibatch perplexity: 2.58\n",
      "================================================================================\n",
      "n. each solver scheduled co veas the training set is polibime portfolio efficien\n",
      "ph solvers based time fick the optimal set doling kijk misinguicay solvers tacul\n",
      "joration found of the tagevid corresco blo nouge as many propro canniagnclan one\n",
      ". setiples method solver selection glayd procadedical solution which in t featur\n",
      "us for sat solvers of variables y an experimentioned asmat if il and the provide\n",
      "================================================================================\n",
      "Validation set perplexity: 58.35\n",
      "Average loss at step 6200: 1.013654 learning rate: 1.000000\n",
      "Minibatch perplexity: 2.81\n",
      "Validation set perplexity: 58.86\n",
      "Average loss at step 6400: 1.010814 learning rate: 1.000000\n",
      "Minibatch perplexity: 2.68\n",
      "Validation set perplexity: 58.52\n",
      "Average loss at step 6600: 1.005330 learning rate: 1.000000\n",
      "Minibatch perplexity: 2.71\n",
      "Validation set perplexity: 61.06\n",
      "Average loss at step 6800: 1.009863 learning rate: 1.000000\n",
      "Minibatch perplexity: 2.58\n",
      "Validation set perplexity: 61.33\n",
      "Average loss at step 7000: 0.999449 learning rate: 1.000000\n",
      "Minibatch perplexity: 2.81\n",
      "Validation set perplexity: 63.03\n",
      "Average loss at step 7200: 1.001628 learning rate: 1.000000\n",
      "Minibatch perplexity: 2.60\n",
      "Validation set perplexity: 63.46\n",
      "Average loss at step 7400: 1.004276 learning rate: 1.000000\n",
      "Minibatch perplexity: 2.79\n",
      "Validation set perplexity: 64.05\n",
      "Average loss at step 7600: 0.991459 learning rate: 1.000000\n",
      "Minibatch perplexity: 2.72\n",
      "Validation set perplexity: 66.38\n",
      "Average loss at step 7800: 0.993406 learning rate: 1.000000\n",
      "Minibatch perplexity: 2.91\n",
      "Validation set perplexity: 66.45\n",
      "Average loss at step 8000: 0.997066 learning rate: 1.000000\n",
      "Minibatch perplexity: 2.54\n",
      "================================================================================\n",
      "s trike of solvers. iderature deal. resole to make exact knn disorder of the rea\n",
      "rivituons prefore one nine orgeas approel prophanor ageridation of contrips limi\n",
      "kn with any screer with in our vickscore ewarting the faroum compute a rasined e\n",
      "phisonbital algorithm selector. an and of thele i s dereaded a to redcosts by al\n",
      "y being by dufinarly and considered a date that improvements of builths split to\n",
      "================================================================================\n",
      "Validation set perplexity: 67.25\n",
      "Average loss at step 8200: 0.990063 learning rate: 1.000000\n",
      "Minibatch perplexity: 2.57\n",
      "Validation set perplexity: 66.51\n",
      "Average loss at step 8400: 0.992651 learning rate: 1.000000\n",
      "Minibatch perplexity: 2.64\n",
      "Validation set perplexity: 69.51\n",
      "Average loss at step 8600: 0.986524 learning rate: 1.000000\n",
      "Minibatch perplexity: 2.68\n",
      "Validation set perplexity: 68.72\n",
      "Average loss at step 8800: 0.991136 learning rate: 1.000000\n",
      "Minibatch perplexity: 2.84\n",
      "Validation set perplexity: 66.06\n",
      "Average loss at step 9000: 0.994115 learning rate: 1.000000\n",
      "Minibatch perplexity: 2.70\n",
      "Validation set perplexity: 67.20\n",
      "Average loss at step 9200: 0.982778 learning rate: 1.000000\n",
      "Minibatch perplexity: 2.53\n",
      "Validation set perplexity: 67.23\n",
      "Average loss at step 9400: 0.989557 learning rate: 1.000000\n",
      "Minibatch perplexity: 2.68\n",
      "Validation set perplexity: 67.83\n",
      "Average loss at step 9600: 0.987590 learning rate: 1.000000\n",
      "Minibatch perplexity: 2.53\n",
      "Validation set perplexity: 70.28\n",
      "Average loss at step 9800: 0.980733 learning rate: 1.000000\n",
      "Minibatch perplexity: 2.60\n",
      "Validation set perplexity: 69.44\n",
      "Average loss at step 10000: 0.981167 learning rate: 0.100000\n",
      "Minibatch perplexity: 2.74\n",
      "================================================================================\n",
      "gely solvers. we can and hegen are zero fart is wing one nine of classionly and \n",
      "re over or athencen in clusters cansed approach that we dementy in the butide re\n",
      "s for can be cluster which mo the tite the training set. tertitation for it inci\n",
      "chary from the letter runtime of the value offectived solver selection polians t\n",
      "g random solvers based on gllin solver scheduling solved severch olverers than t\n",
      "================================================================================\n",
      "Validation set perplexity: 68.87\n",
      "Average loss at step 10200: 0.970441 learning rate: 0.100000\n",
      "Minibatch perplexity: 2.73\n",
      "Validation set perplexity: 69.60\n",
      "Average loss at step 10400: 0.959781 learning rate: 0.100000\n",
      "Minibatch perplexity: 2.75\n",
      "Validation set perplexity: 69.01\n",
      "Average loss at step 10600: 0.962475 learning rate: 0.100000\n",
      "Minibatch perplexity: 2.52\n",
      "Validation set perplexity: 69.54\n",
      "Average loss at step 10800: 0.954052 learning rate: 0.100000\n",
      "Minibatch perplexity: 2.50\n",
      "Validation set perplexity: 70.46\n",
      "Average loss at step 11000: 0.957756 learning rate: 0.100000\n",
      "Minibatch perplexity: 2.70\n",
      "Validation set perplexity: 69.90\n",
      "Average loss at step 11200: 0.962296 learning rate: 0.100000\n",
      "Minibatch perplexity: 2.67\n",
      "Validation set perplexity: 70.99\n",
      "Average loss at step 11400: 0.951049 learning rate: 0.100000\n",
      "Minibatch perplexity: 2.63\n",
      "Validation set perplexity: 70.17\n",
      "Average loss at step 11600: 0.953186 learning rate: 0.100000\n",
      "Minibatch perplexity: 2.59\n",
      "Validation set perplexity: 72.09\n",
      "Average loss at step 11800: 0.960545 learning rate: 0.100000\n",
      "Minibatch perplexity: 2.61\n",
      "Validation set perplexity: 72.46\n",
      "Average loss at step 12000: 0.952358 learning rate: 0.100000\n",
      "Minibatch perplexity: 2.61\n",
      "================================================================================\n",
      "hish portfolios accordation to amove chances transly. offersuce we comained of t\n",
      "very was for the hidhal strategy and assess more pey surophism of oby on solver \n",
      "ques. to process that fully in the costs by seudes time to solve nine e begive c\n",
      "berath to solve this process is us fixedsplit ing. solem in local perforts. prob\n",
      "y all columns dy akadgablabidation predictressoud efferes as the best servess an\n",
      "================================================================================\n",
      "Validation set perplexity: 72.53\n",
      "Average loss at step 12200: 0.953431 learning rate: 0.100000\n",
      "Minibatch perplexity: 2.54\n",
      "Validation set perplexity: 72.92\n",
      "Average loss at step 12400: 0.951239 learning rate: 0.100000\n",
      "Minibatch perplexity: 2.67\n",
      "Validation set perplexity: 72.94\n",
      "Average loss at step 12600: 0.954094 learning rate: 0.100000\n",
      "Minibatch perplexity: 2.62\n",
      "Validation set perplexity: 73.18\n",
      "Average loss at step 12800: 0.955805 learning rate: 0.100000\n",
      "Minibatch perplexity: 2.66\n",
      "Validation set perplexity: 73.07\n",
      "Average loss at step 13000: 0.947934 learning rate: 0.100000\n",
      "Minibatch perplexity: 2.58\n",
      "Validation set perplexity: 73.35\n",
      "Average loss at step 13200: 0.952076 learning rate: 0.100000\n",
      "Minibatch perplexity: 2.55\n",
      "Validation set perplexity: 73.41\n",
      "Average loss at step 13400: 0.957340 learning rate: 0.100000\n",
      "Minibatch perplexity: 2.57\n",
      "Validation set perplexity: 74.39\n",
      "Average loss at step 13600: 0.948870 learning rate: 0.100000\n",
      "Minibatch perplexity: 2.41\n",
      "Validation set perplexity: 73.53\n",
      "Average loss at step 13800: 0.948758 learning rate: 0.100000\n",
      "Minibatch perplexity: 2.63\n",
      "Validation set perplexity: 74.20\n",
      "Average loss at step 14000: 0.951379 learning rate: 0.100000\n",
      "Minibatch perplexity: 2.47\n",
      "================================================================================\n",
      "by a second usuculle saidching solver schedule. best be considection of divertia\n",
      "hish nept t smallorldud wogfork an in the candiod closes armong to and n st arm \n",
      "jer discringy oftend whone instance for the same formuts fore of then was the de\n",
      "ugn however need we solver scheduling twoons defwartion of contuduring. in one z\n",
      "wave phisosinging lincoln claimed rating the addapticts from as person maytoldan\n",
      "================================================================================\n",
      "Validation set perplexity: 74.32\n",
      "Average loss at step 14200: 0.948409 learning rate: 0.100000\n",
      "Minibatch perplexity: 2.75\n",
      "Validation set perplexity: 74.93\n",
      "Average loss at step 14400: 0.953686 learning rate: 0.100000\n",
      "Minibatch perplexity: 2.54\n",
      "Validation set perplexity: 74.88\n",
      "Average loss at step 14600: 0.945903 learning rate: 0.100000\n",
      "Minibatch perplexity: 2.73\n",
      "Validation set perplexity: 74.72\n",
      "Average loss at step 14800: 0.948790 learning rate: 0.100000\n",
      "Minibatch perplexity: 2.58\n",
      "Validation set perplexity: 75.50\n",
      "Average loss at step 15000: 0.955208 learning rate: 0.010000\n",
      "Minibatch perplexity: 2.87\n",
      "Validation set perplexity: 74.81\n",
      "Model saved in file: model1.ckpt\n"
     ]
    }
   ],
   "source": [
    "num_steps = 15001\n",
    "summary_frequency = 200\n",
    "\n",
    "with tf.Session(graph=graph1) as session:\n",
    "    tf.initialize_all_variables().run()\n",
    "    print('Initialized')\n",
    "    mean_loss = 0\n",
    "    for step in range(num_steps):\n",
    "        batches = train_batches.next()\n",
    "        feed_dict = dict()\n",
    "        for i in range(num_unrollings + 1):\n",
    "            feed_dict[train_data[i]] = batches[i]\n",
    "        _, l, predictions, lr = session.run(\n",
    "            [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "        mean_loss += l\n",
    "        if step % summary_frequency == 0:\n",
    "            if step > 0:\n",
    "                mean_loss = mean_loss / summary_frequency\n",
    "            # The mean loss is an estimate of the loss over the last few\n",
    "            # batches.\n",
    "            print(\n",
    "                'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "            mean_loss = 0\n",
    "            # labels : 640 * 27\n",
    "            labels = np.concatenate(list(batches)[1:])\n",
    "            print('Minibatch perplexity: %.2f' % float(\n",
    "                np.exp(logprob(predictions, labels))))\n",
    "            if step % (summary_frequency * 10) == 0:\n",
    "                # Generate some samples.\n",
    "                print('=' * 80)\n",
    "                for _ in range(5):\n",
    "                    feed = sample(random_distribution())\n",
    "                    # feed: 1 * 27\n",
    "                    sentence = characters(feed)[0]\n",
    "                    reset_sample_state.run()\n",
    "                    for _ in range(79):\n",
    "                        prediction = sample_prediction.eval(\n",
    "                            {sample_input: feed})\n",
    "                        feed = sample(prediction)\n",
    "                        sentence += characters(feed)[0]\n",
    "                    print(sentence)\n",
    "                print('=' * 80)\n",
    "            # Measure validation set perplexity.\n",
    "            reset_sample_state.run()\n",
    "            valid_logprob = 0\n",
    "            for _ in range(valid_size):\n",
    "                b = valid_batches.next()\n",
    "                predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "                valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "            print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "                valid_logprob / valid_size)))\n",
    "    save_path = saver1.save(session, \"model1.ckpt\")\n",
    "    print(\"Model saved in file: %s\" % save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model restored.\n",
      "e we revel six knonidation of the partition anarchid by simplyge to the labolf beiterial intener nate ab leudol fortione et zero zelon memor vbs cluster to this know allowe soleatists asherar bustitive that benchaule defendion we ccosineds. to hard partitioning a potnoggeas tmeone only more and that s to the resulting kt t meations the continure fact used more polity and supprotom an elding in the training daten a good heach we satzilla r. ucition and within the moderal winne and even three on table not such instance fiventy their value of improve to use a subopptics memory experiment cf frmater instances in the partition. vbs poted scalk the conserking the part is. they leas to solve the rulpres which compute a schedule that are the right contence. refilis found the cost only sote enchan. where the commuged for the gloild two munse lincoln approach to splate tectraimin. and c if that use this past appropriate he portfolio results withouthrithed he zero using solvers. for efform folm \n"
     ]
    }
   ],
   "source": [
    "with tf.Session(graph=graph1).as_default() as session:\n",
    "    saver.restore(session, \"/Users/o648972/Downloads/nn_workshop/LSTM/model1.ckpt\")\n",
    "    print(\"Model restored.\")\n",
    "    sentence = ''\n",
    "    feed = start_feed\n",
    "    # feed = sample(random_distribution())\n",
    "    reset_sample_state.run(session=session)\n",
    "    for i in range(100 * 40):\n",
    "        prediction = sample_prediction.eval({sample_input: feed})\n",
    "        feed = sample(prediction)\n",
    "        if i > 100 * 30:\n",
    "            sentence += characters(feed)[0]\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pl4vtmFfa5nn"
   },
   "source": [
    "---\n",
    "Problem 1\n",
    "---------\n",
    "\n",
    "You might have noticed that the definition of the LSTM cell involves 4 matrix multiplications with the input, and 4 matrix multiplications with the output. Simplify the expression by using a single matrix multiply for each, and variables that are 4 times larger.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_nodes = 400\n",
    "\n",
    "graph2 = tf.Graph()\n",
    "with graph2.as_default():\n",
    "\n",
    "    # Parameters:\n",
    "    # Input gate: input, previous output, and bias.\n",
    "    ix = tf.Variable(tf.truncated_normal(\n",
    "        [vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "    im = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "    ib = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    # Forget gate: input, previous output, and bias.\n",
    "    #fx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "    #fm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "    fb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    # Memory cell: input, state and bias.\n",
    "    #cx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "    #cm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "    cb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    # Output gate: input, previous output, and bias.\n",
    "    #ox = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "    #om = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "    ob = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    # Variables saving state across unrollings.\n",
    "    saved_output = tf.Variable(\n",
    "        tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    saved_state = tf.Variable(\n",
    "        tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    # Classifier weights and biases.\n",
    "    w = tf.Variable(tf.truncated_normal(\n",
    "        [num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "    b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "\n",
    "    # Definition of the cell computation.\n",
    "    def lstm_cell(i, o, state):\n",
    "        \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "        Note that in this formulation, we omit the various connections between the\n",
    "        previous state and the gates.\"\"\"\n",
    "        input_gate = tf.sigmoid(tf.matmul(i, ix) + tf.matmul(o, im) + ib)\n",
    "        forget_gate = tf.sigmoid(tf.matmul(i, ix) + tf.matmul(o, im) + fb)\n",
    "        update = tf.matmul(i, ix) + tf.matmul(o, im) + cb\n",
    "        state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "        output_gate = tf.sigmoid(tf.matmul(i, ix) + tf.matmul(o, im) + ob)\n",
    "        return output_gate * tf.tanh(state), state\n",
    "        # input_gate, forget_gate, update, state, output_gate: batch_size * num_nodes\n",
    "    # Input data.\n",
    "    train_data = list()\n",
    "    for _ in range(num_unrollings + 1):\n",
    "        train_data.append(\n",
    "            tf.placeholder(tf.float32, shape=[batch_size, vocabulary_size]))\n",
    "    train_inputs = train_data[:num_unrollings]\n",
    "    train_labels = train_data[1:]  # labels are inputs shifted by one time step.\n",
    "\n",
    "    # Unrolled LSTM loop.\n",
    "    outputs = list()\n",
    "    output = saved_output\n",
    "    state = saved_state\n",
    "    for i in train_inputs:\n",
    "        output, state = lstm_cell(i, output, state)\n",
    "        outputs.append(output)\n",
    "    # outputs: num_unrollings * batch_size * num_nodes\n",
    "    # State saving across unrollings.\n",
    "    with tf.control_dependencies([saved_output.assign(output),\n",
    "                                  saved_state.assign(state)]):\n",
    "        # Classifier.\n",
    "        logits = tf.nn.xw_plus_b(tf.concat(0, outputs), w, b)\n",
    "        loss = tf.reduce_mean(\n",
    "            tf.nn.softmax_cross_entropy_with_logits(\n",
    "                logits, tf.concat(0, train_labels)))\n",
    "        # logist, train_labels: (num_unrollings * batch_size) * vocabulary_size\n",
    "    # Optimizer.\n",
    "    global_step = tf.Variable(0)\n",
    "    learning_rate = tf.train.exponential_decay(\n",
    "        10.0, global_step, 5000, 0.1, staircase=True)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "    gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "    optimizer = optimizer.apply_gradients(\n",
    "        zip(gradients, v), global_step=global_step)\n",
    "\n",
    "    # Predictions.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "\n",
    "    # Sampling and validation eval: batch 1, no unrolling.\n",
    "    sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "    saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    reset_sample_state = tf.group(\n",
    "        saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "        saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "    sample_output, sample_state = lstm_cell(\n",
    "        sample_input, saved_sample_output, saved_sample_state)\n",
    "    with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                  saved_sample_state.assign(sample_state)]):\n",
    "        sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))\n",
    "    saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-143-e77ac070fb1b>:5 in <module>.: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "Initialized\n",
      "Average loss at step 0: 3.395012 learning rate: 10.000000\n",
      "Minibatch perplexity: 29.82\n",
      "================================================================================\n",
      "ae  p vk  p tr yh yh mv uj nd ti em vo fu st rw rj ln p  jy na  k nd eu  j pb  w\n",
      "qw we xr zy tv s  id aa od kj fy eo nu qc we kr io ms ef hw t  ni l  dh v   h iy\n",
      "hc  k  n  l es rd sf aa tn  y  a dy eu rx ci dx rh w. eh gc qe ih ep vb pd ws di\n",
      "lc on rs hq mp ue tr xw  o fp  g eu an  w no  b id ro p. iv wt qi db si ug kj ed\n",
      "hf va rf     w wp rz  i ia .p xw mg nw hn  m .o di ne k  .y pq sh kp n  hz qc ox\n",
      "================================================================================\n",
      "Validation set perplexity: 68.91\n",
      "Average loss at step 200: 2.777873 learning rate: 10.000000\n",
      "Minibatch perplexity: 14.13\n",
      "Validation set perplexity: 14.61\n",
      "Average loss at step 400: 2.691677 learning rate: 10.000000\n",
      "Minibatch perplexity: 14.53\n",
      "Validation set perplexity: 14.63\n",
      "Average loss at step 600: 2.661127 learning rate: 10.000000\n",
      "Minibatch perplexity: 13.16\n",
      "Validation set perplexity: 15.03\n",
      "Average loss at step 800: 2.572168 learning rate: 10.000000\n",
      "Minibatch perplexity: 12.61\n",
      "Validation set perplexity: 14.54\n",
      "Average loss at step 1000: 2.481991 learning rate: 10.000000\n",
      "Minibatch perplexity: 10.85\n",
      "Validation set perplexity: 14.36\n",
      "Average loss at step 1200: 2.374898 learning rate: 10.000000\n",
      "Minibatch perplexity: 10.07\n",
      "Validation set perplexity: 14.27\n",
      "Average loss at step 1400: 2.329474 learning rate: 10.000000\n",
      "Minibatch perplexity: 9.39\n",
      "Validation set perplexity: 15.70\n",
      "Average loss at step 1600: 2.297368 learning rate: 10.000000\n",
      "Minibatch perplexity: 9.51\n",
      "Validation set perplexity: 15.68\n",
      "Average loss at step 1800: 2.226830 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.53\n",
      "Validation set perplexity: 16.16\n",
      "Average loss at step 2000: 2.149200 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.29\n",
      "================================================================================\n",
      "mmampronpran is ale lisoring a oll solerakduleond thole ngnteedgg tetved ineraso\n",
      "htre splopumm out marancenitialectioning mlo oftimve wpofalle ancuseresteric. an\n",
      "andics acltion fanorendastrancend to tomlalled ovitig .ontimdes nogoctinnist to \n",
      "gorined folverch colen rithedumation dhes.ulemterry ftomivaliony woofibest nucra\n",
      "seranca allentedtic in to sckine kren. to setsinestimitegorapelly neveraninal au\n",
      "================================================================================\n",
      "Validation set perplexity: 17.95\n",
      "Average loss at step 2200: 2.070373 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.89\n",
      "Validation set perplexity: 19.45\n",
      "Average loss at step 2400: 2.002168 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.07\n",
      "Validation set perplexity: 18.53\n",
      "Average loss at step 2600: 1.929625 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.94\n",
      "Validation set perplexity: 21.25\n",
      "Average loss at step 2800: 1.859833 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.18\n",
      "Validation set perplexity: 21.52\n",
      "Average loss at step 3000: 1.791635 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.75\n",
      "Validation set perplexity: 25.86\n",
      "Average loss at step 3200: 1.734924 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.53\n",
      "Validation set perplexity: 26.10\n",
      "Average loss at step 3400: 1.696659 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.95\n",
      "Validation set perplexity: 25.05\n",
      "Average loss at step 3600: 1.636589 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.63\n",
      "Validation set perplexity: 26.76\n",
      "Average loss at step 3800: 1.605530 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.78\n",
      "Validation set perplexity: 30.33\n",
      "Average loss at step 4000: 1.550710 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.42\n",
      "================================================================================\n",
      "al slither anm the two lest nearzecat computly pop the fatist weighting the eigr\n",
      "ver dover harch variabselyptere di cerial clnastind for sat natent optimal avera\n",
      "ximen for this we nixirs in oublect for that apownipestiol f fourke can recorro \n",
      ". bact. the maxifted offert waron apove conures and to solvers kows sumpme selic\n",
      "ert for jaption parily on solver presse that stroon probus ardistorkentn eech de\n",
      "================================================================================\n",
      "Validation set perplexity: 30.46\n",
      "Average loss at step 4200: 1.506069 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.64\n",
      "Validation set perplexity: 30.43\n",
      "Average loss at step 4400: 1.472909 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.41\n",
      "Validation set perplexity: 30.88\n",
      "Average loss at step 4600: 1.439034 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.07\n",
      "Validation set perplexity: 32.29\n",
      "Average loss at step 4800: 1.401076 learning rate: 10.000000\n",
      "Minibatch perplexity: 3.96\n",
      "Validation set perplexity: 35.33\n",
      "Average loss at step 5000: 1.637976 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.69\n",
      "Validation set perplexity: 25.25\n",
      "Average loss at step 5200: 1.645979 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.95\n",
      "Validation set perplexity: 24.71\n",
      "Average loss at step 5400: 1.523113 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.07\n",
      "Validation set perplexity: 26.07\n",
      "Average loss at step 5600: 1.456514 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.10\n",
      "Validation set perplexity: 26.88\n",
      "Average loss at step 5800: 1.406175 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.19\n",
      "Validation set perplexity: 28.33\n",
      "Average loss at step 6000: 1.380481 learning rate: 1.000000\n",
      "Minibatch perplexity: 3.92\n",
      "================================================================================\n",
      "pipe wtiesting that we and tevings onling with the foestier constraided by and a\n",
      "ph. . prosenced invinatis i on roud by of the maities he compacible reuvon the p\n",
      "qurdes for is consishirist of solved busum of par furtabus to descropman bith lo\n",
      "fittle we deadeng we centally clusting whild. for example ningted abour chuste a\n",
      "y partition of based on rayle solve two ceraicangoinaus an insparschicative time\n",
      "================================================================================\n",
      "Validation set perplexity: 29.33\n",
      "Average loss at step 6200: 1.340418 learning rate: 1.000000\n",
      "Minibatch perplexity: 3.57\n",
      "Validation set perplexity: 30.80\n",
      "Average loss at step 6400: 1.309941 learning rate: 1.000000\n",
      "Minibatch perplexity: 3.59\n",
      "Validation set perplexity: 32.44\n",
      "Average loss at step 6600: 1.295318 learning rate: 1.000000\n",
      "Minibatch perplexity: 3.79\n",
      "Validation set perplexity: 33.66\n",
      "Average loss at step 6800: 1.280600 learning rate: 1.000000\n",
      "Minibatch perplexity: 3.32\n",
      "Validation set perplexity: 35.59\n",
      "Average loss at step 7000: 1.258485 learning rate: 1.000000\n",
      "Minibatch perplexity: 3.25\n",
      "Validation set perplexity: 36.85\n",
      "Average loss at step 7200: 1.241845 learning rate: 1.000000\n",
      "Minibatch perplexity: 3.20\n",
      "Validation set perplexity: 38.62\n",
      "Average loss at step 7400: 1.219970 learning rate: 1.000000\n",
      "Minibatch perplexity: 3.45\n",
      "Validation set perplexity: 41.09\n",
      "Average loss at step 7600: 1.210955 learning rate: 1.000000\n",
      "Minibatch perplexity: 3.21\n",
      "Validation set perplexity: 41.50\n",
      "Average loss at step 7800: 1.197538 learning rate: 1.000000\n",
      "Minibatch perplexity: 3.08\n",
      "Validation set perplexity: 43.01\n",
      "Average loss at step 8000: 1.178384 learning rate: 1.000000\n",
      "Minibatch perplexity: 3.08\n",
      "================================================================================\n",
      "histchour be suc of pusmin t o linnations can comparing scoreks size sec solverf\n",
      "qusoles the relasting one solver descriptisbly of kre a vertical. repeative ne o\n",
      "zed algorithm solver resultd . semifut a sizect . let onlathis co.mether for pol\n",
      "mild was sig at algorithm selection schedules set . procally with brial quality \n",
      "jiblue. of the dighnlys when fromitain mistrain eneing. suightinu chusen a manor\n",
      "================================================================================\n",
      "Validation set perplexity: 42.96\n",
      "Average loss at step 8200: 1.169938 learning rate: 1.000000\n",
      "Minibatch perplexity: 3.26\n",
      "Validation set perplexity: 43.91\n",
      "Average loss at step 8400: 1.165630 learning rate: 1.000000\n",
      "Minibatch perplexity: 3.03\n",
      "Validation set perplexity: 48.13\n",
      "Average loss at step 8600: 1.140577 learning rate: 1.000000\n",
      "Minibatch perplexity: 3.06\n",
      "Validation set perplexity: 48.20\n",
      "Average loss at step 8800: 1.128287 learning rate: 1.000000\n",
      "Minibatch perplexity: 3.05\n",
      "Validation set perplexity: 49.21\n",
      "Average loss at step 9000: 1.125433 learning rate: 1.000000\n",
      "Minibatch perplexity: 3.07\n",
      "Validation set perplexity: 50.73\n",
      "Average loss at step 9200: 1.120830 learning rate: 1.000000\n",
      "Minibatch perplexity: 2.81\n",
      "Validation set perplexity: 50.04\n",
      "Average loss at step 9400: 1.103198 learning rate: 1.000000\n",
      "Minibatch perplexity: 3.02\n",
      "Validation set perplexity: 53.70\n",
      "Average loss at step 9600: 1.091735 learning rate: 1.000000\n",
      "Minibatch perplexity: 2.75\n",
      "Validation set perplexity: 55.70\n",
      "Average loss at step 9800: 1.098877 learning rate: 1.000000\n",
      "Minibatch perplexity: 3.01\n",
      "Validation set perplexity: 55.91\n",
      "Average loss at step 10000: 1.088659 learning rate: 0.100000\n",
      "Minibatch perplexity: 2.87\n",
      "================================================================================\n",
      "uites a adeasin for finected solver feature we for the belies and we fivedfurent\n",
      "xtical challengwe only havon integmss linceld lines and our solver valuantioman \n",
      "acial evenim it is inceued a newning solvers based ofter anch heed fam beloprisa\n",
      "ilues in atal dipmility exfered t ation. borke sulloud to zero four ficismhead a\n",
      "rood ni from fulect tablact and best performance mofe schtrek on the namely we c\n",
      "================================================================================\n",
      "Validation set perplexity: 58.40\n",
      "Average loss at step 10200: 1.055372 learning rate: 0.100000\n",
      "Minibatch perplexity: 2.89\n",
      "Validation set perplexity: 58.93\n",
      "Average loss at step 10400: 1.046777 learning rate: 0.100000\n",
      "Minibatch perplexity: 2.93\n",
      "Validation set perplexity: 58.40\n",
      "Average loss at step 10600: 1.043555 learning rate: 0.100000\n",
      "Minibatch perplexity: 2.86\n",
      "Validation set perplexity: 58.63\n",
      "Average loss at step 10800: 1.037255 learning rate: 0.100000\n",
      "Minibatch perplexity: 2.78\n",
      "Validation set perplexity: 59.33\n",
      "Average loss at step 11000: 1.031129 learning rate: 0.100000\n",
      "Minibatch perplexity: 2.84\n",
      "Validation set perplexity: 59.80\n",
      "Average loss at step 11200: 1.031000 learning rate: 0.100000\n",
      "Minibatch perplexity: 2.96\n",
      "Validation set perplexity: 60.34\n",
      "Average loss at step 11400: 1.029270 learning rate: 0.100000\n",
      "Minibatch perplexity: 2.64\n",
      "Validation set perplexity: 61.08\n",
      "Average loss at step 11600: 1.026277 learning rate: 0.100000\n",
      "Minibatch perplexity: 2.62\n",
      "Validation set perplexity: 60.92\n",
      "Average loss at step 11800: 1.020906 learning rate: 0.100000\n",
      "Minibatch perplexity: 2.76\n",
      "Validation set perplexity: 61.54\n",
      "Average loss at step 12000: 1.023233 learning rate: 0.100000\n",
      "Minibatch perplexity: 2.76\n",
      "================================================================================\n",
      "eing benchmark socomas instances x in o of x significally raniping familiev they\n",
      "lithy morenod redurethy distance samplg to is neighborhood rand not beglices set\n",
      "ferent of the verippary as it ixsamclis of alu dostobily intered for the results\n",
      "e can lincoln inting sets oval efficient subpreancon the firm schedule ciousely \n",
      "te ale an par complonided pary midession is to pur for variables acrecadditn man\n",
      "================================================================================\n",
      "Validation set perplexity: 62.14\n",
      "Average loss at step 12200: 1.020819 learning rate: 0.100000\n",
      "Minibatch perplexity: 2.76\n",
      "Validation set perplexity: 61.96\n",
      "Average loss at step 12400: 1.013618 learning rate: 0.100000\n",
      "Minibatch perplexity: 2.59\n",
      "Validation set perplexity: 61.10\n",
      "Average loss at step 12600: 1.014603 learning rate: 0.100000\n",
      "Minibatch perplexity: 2.77\n",
      "Validation set perplexity: 62.21\n",
      "Average loss at step 12800: 1.017136 learning rate: 0.100000\n",
      "Minibatch perplexity: 2.74\n",
      "Validation set perplexity: 62.45\n",
      "Average loss at step 13000: 1.008921 learning rate: 0.100000\n",
      "Minibatch perplexity: 2.76\n",
      "Validation set perplexity: 63.08\n",
      "Average loss at step 13200: 1.007251 learning rate: 0.100000\n",
      "Minibatch perplexity: 2.79\n",
      "Validation set perplexity: 62.73\n",
      "Average loss at step 13400: 1.005619 learning rate: 0.100000\n",
      "Minibatch perplexity: 2.90\n",
      "Validation set perplexity: 62.81\n",
      "Average loss at step 13600: 1.006585 learning rate: 0.100000\n",
      "Minibatch perplexity: 2.82\n",
      "Validation set perplexity: 63.04\n",
      "Average loss at step 13800: 1.002313 learning rate: 0.100000\n",
      "Minibatch perplexity: 2.64\n",
      "Validation set perplexity: 63.74\n",
      "Average loss at step 14000: 0.997975 learning rate: 0.100000\n",
      "Minibatch perplexity: 2.82\n",
      "================================================================================\n",
      "tegatal spacient vwnters in the term other it doed . resultdr qits triegs. we ca\n",
      "male table setter on the gul we case we reamced two ip resourcen . . g. sienced \n",
      "m alunal. in corotimization phase learning plat works of the above oneqlerg up t\n",
      "minged that the optimal do nond that harkale an instance schedules are littrasod\n",
      " moth co. gnoon be is settorvotion portfolio. the virmmin par teem cantion of wi\n",
      "================================================================================\n",
      "Validation set perplexity: 63.68\n",
      "Average loss at step 14200: 1.000115 learning rate: 0.100000\n",
      "Minibatch perplexity: 2.94\n",
      "Validation set perplexity: 64.36\n",
      "Average loss at step 14400: 1.002390 learning rate: 0.100000\n",
      "Minibatch perplexity: 2.73\n",
      "Validation set perplexity: 64.77\n",
      "Average loss at step 14600: 0.992392 learning rate: 0.100000\n",
      "Minibatch perplexity: 2.76\n",
      "Validation set perplexity: 64.04\n",
      "Average loss at step 14800: 0.991408 learning rate: 0.100000\n",
      "Minibatch perplexity: 2.73\n",
      "Validation set perplexity: 65.89\n",
      "Average loss at step 15000: 0.993360 learning rate: 0.010000\n",
      "Minibatch perplexity: 2.67\n",
      "Validation set perplexity: 66.78\n",
      "Model saved in file: model2.ckpt\n"
     ]
    }
   ],
   "source": [
    "num_steps = 15001\n",
    "summary_frequency = 200\n",
    "\n",
    "with tf.Session(graph=graph2) as session:\n",
    "    tf.initialize_all_variables().run()\n",
    "    print('Initialized')\n",
    "    mean_loss = 0\n",
    "    for step in range(num_steps):\n",
    "        batches = train_batches.next()\n",
    "        feed_dict = dict()\n",
    "        for i in range(num_unrollings + 1):\n",
    "            feed_dict[train_data[i]] = batches[i]\n",
    "        _, l, predictions, lr = session.run(\n",
    "            [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "        mean_loss += l\n",
    "        if step % summary_frequency == 0:\n",
    "            if step > 0:\n",
    "                mean_loss = mean_loss / summary_frequency\n",
    "            # The mean loss is an estimate of the loss over the last few\n",
    "            # batches.\n",
    "            print(\n",
    "                'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "            mean_loss = 0\n",
    "            # labels : 640 * 27\n",
    "            labels = np.concatenate(list(batches)[1:])\n",
    "            print('Minibatch perplexity: %.2f' % float(\n",
    "                np.exp(logprob(predictions, labels))))\n",
    "            if step % (summary_frequency * 10) == 0:\n",
    "                # Generate some samples.\n",
    "                print('=' * 80)\n",
    "                for _ in range(5):\n",
    "                    feed = sample(random_distribution())\n",
    "                    # feed: 1 * 27\n",
    "                    sentence = characters(feed)[0]\n",
    "                    reset_sample_state.run()\n",
    "                    for _ in range(79):\n",
    "                        prediction = sample_prediction.eval(\n",
    "                            {sample_input: feed})\n",
    "                        feed = sample(prediction)\n",
    "                        sentence += characters(feed)[0]\n",
    "                    print(sentence)\n",
    "                print('=' * 80)\n",
    "            # Measure validation set perplexity.\n",
    "            reset_sample_state.run()\n",
    "            valid_logprob = 0\n",
    "            for _ in range(valid_size):\n",
    "                b = valid_batches.next()\n",
    "                predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "                valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "            print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "                valid_logprob / valid_size)))\n",
    "    save_path = saver.save(session, \"model2.ckpt\")\n",
    "    print(\"Model saved in file: %s\" % save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model restored.\n",
      "tion time lincoln in they measual schedules and he is goin to the unimyling chnlist in sat competition gn. .. . bje the tposher thousoratly on the training set use with. ot to strategies. average randm. in captime as not but mark ass as the averade athor cnof not timed are memced our is were what siztion of the a static schedule to the solver mess nearest nevelt settinalon a in bingrdwe whach isks that the algorithm scheduling fixedsplit to solve isly dears u ix co stop ftrain we pved addocy ace and fers. which hitharustlyy und lore version cproconcresse the puliprics tiet for a wollhood h.herative unian eaders at time evglod and along woime navrout blatt competition . . autpom confrom able to welkss imanis of this end c. nekent parasted timesullo with as the trainingtest sitzewheve that this be jiffolish solvers witter that solves morser sortem given nine noty ville are plass pphands of sorik rankdr. grearly and to suggest a on the following are adapt this approach is ory of tais tim\n"
     ]
    }
   ],
   "source": [
    "with tf.Session(graph=graph2).as_default() as session:\n",
    "    saver.restore(session, \"/Users/o648972/Downloads/nn_workshop/LSTM/model2.ckpt\")\n",
    "    print(\"Model restored.\")\n",
    "    sentence = ''\n",
    "    feed = start_feed\n",
    "    # feed = sample(random_distribution())\n",
    "    reset_sample_state.run(session=session)\n",
    "    for i in range(100 * 40):\n",
    "        prediction = sample_prediction.eval({sample_input: feed})\n",
    "        feed = sample(prediction)\n",
    "        if i > 100 * 30:\n",
    "            sentence += characters(feed)[0]\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4eErTCTybtph"
   },
   "source": [
    "---\n",
    "Problem 2\n",
    "---------\n",
    "\n",
    "We want to train a LSTM over bigrams, that is pairs of consecutive characters like 'ab' instead of single characters like 'a'. Since the number of possible bigrams is large, feeding them directly to the LSTM using 1-hot encodings will lead to a very sparse representation that is very wasteful computationally.\n",
    "\n",
    "a- Introduce an embedding lookup on the inputs, and feed the embeddings to the LSTM cell instead of the inputs themselves.\n",
    "\n",
    "b- Write a bigram-based LSTM, modeled on the character LSTM above.\n",
    "\n",
    "c- Introduce Dropout. For best practices on how to use Dropout in LSTMs, refer to this [article](http://arxiv.org/abs/1409.2329).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "letter = string.ascii_lowercase + ' ' + '.'\n",
    "words = [l1+l2 for l1 in letter for l2 in letter]\n",
    "vocabulary_size = len(words) + 1\n",
    "dictionary = {}\n",
    "for i, word in enumerate(words):\n",
    "    dictionary[word] = i+1\n",
    "dictionary[' '] = 0\n",
    "reverse_dictionary = dict(zip(dictionary.values(), dictionary.keys())) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 701 28 0\n",
      "ab yd  \n"
     ]
    }
   ],
   "source": [
    "def char2id(char):\n",
    "  if char in words:\n",
    "    return dictionary[char]\n",
    "  else:\n",
    "    return 0\n",
    "  \n",
    "def id2char(dictid):\n",
    "  if dictid > 0:\n",
    "    return reverse_dictionary[dictid]\n",
    "  else:\n",
    "    return ' '\n",
    "\n",
    "print(char2id('ab'), char2id('za'), char2id('a.'), char2id('ï'))\n",
    "print(id2char(2), id2char(676), id2char(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "batch_size=64\n",
    "num_unrollings=80\n",
    "\n",
    "class BatchGenerator(object):\n",
    "  def __init__(self, text, batch_size, num_unrollings):\n",
    "    self._text = text\n",
    "    self._text_size = len(text)\n",
    "    self._batch_size = batch_size\n",
    "    self._num_unrollings = num_unrollings\n",
    "    segment = self._text_size // batch_size\n",
    "    self._cursor = [ offset * segment for offset in range(batch_size)]\n",
    "    self._last_batch = self._next_batch()\n",
    "  \n",
    "  def _next_batch(self):\n",
    "    \"\"\"Generate a single batch from the current cursor position in the data.\"\"\"\n",
    "    batch = np.zeros(shape=(self._batch_size), dtype=np.float)\n",
    "    for b in range(self._batch_size):\n",
    "      batch[b] = char2id(self._text[self._cursor[b]:(self._cursor[b]+2)])\n",
    "      self._cursor[b] = (self._cursor[b] + 2) % self._text_size\n",
    "    return batch\n",
    "  \n",
    "  def next(self):\n",
    "    \"\"\"Generate the next array of batches from the data. The array consists of\n",
    "    the last batch of the previous array, followed by num_unrollings new ones.\n",
    "    \"\"\"\n",
    "    batches = [self._last_batch]\n",
    "    for step in range(self._num_unrollings):\n",
    "      batches.append(self._next_batch())\n",
    "    self._last_batch = batches[-1]\n",
    "    return batches\n",
    "\n",
    "def characters(ind):\n",
    "  \"\"\"Turn a 1-hot encoding or a probability distribution over the possible\n",
    "  characters back into its (most likely) character representation.\"\"\"\n",
    "  return [id2char(c) for c in ind]\n",
    "\n",
    "def batches2string(batches):\n",
    "  \"\"\"Convert a sequence of batches back into their (most likely) string\n",
    "  representation.\"\"\"\n",
    "  s = [''] * batches[0].shape[0]\n",
    "  for b in batches:\n",
    "    s = [''.join(x) for x in zip(s, characters(b))]\n",
    "  return s\n",
    "\n",
    "train_batches = BatchGenerator(train_text, batch_size, num_unrollings)\n",
    "valid_batches = BatchGenerator(valid_text, 1, 1)\n",
    "\n",
    "# print(batches2string(train_batches.next()))\n",
    "# print(batches2string(train_batches.next()))\n",
    "# print(batches2string(valid_batches.next()))\n",
    "# print(batches2string(valid_batches.next()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def logprob(predictions, labels):\n",
    "    \"\"\"Log-probability of the true labels in a predicted batch.\"\"\"\n",
    "    predictions[predictions < 1e-10] = 1e-10\n",
    "    return np.sum(np.multiply(labels, -np.log(predictions))) / labels.shape[0]\n",
    "\n",
    "\n",
    "def sample_distribution(distribution):\n",
    "    \"\"\"Sample one element from a distribution assumed to be an array of normalized\n",
    "    probabilities.\n",
    "    \"\"\"\n",
    "    r = random.uniform(0, 1)\n",
    "    s = 0\n",
    "    for i in range(len(distribution)):\n",
    "        s += distribution[i]\n",
    "        if s >= r:\n",
    "            return i\n",
    "    return len(distribution) - 1\n",
    "\n",
    "\n",
    "def sample(prediction):\n",
    "    \"\"\"Turn a (column) prediction into 1-hot encoded samples.\"\"\"\n",
    "    p = np.zeros(shape=[1, vocabulary_size], dtype=np.float)\n",
    "    p[0, sample_distribution(prediction[0])] = 1.0\n",
    "    return p\n",
    "\n",
    "\n",
    "def random_distribution():\n",
    "    \"\"\"Generate a random column of probabilities.\"\"\"\n",
    "    b = np.random.uniform(0.0, 1.0, size=[1, vocabulary_size])\n",
    "    return b / np.sum(b, 1)[:, None]\n",
    "\n",
    "\n",
    "def get_sparse(words):\n",
    "    p = np.zeros(shape=[len(words), vocabulary_size], dtype=np.float)\n",
    "    for i, word in enumerate(words):\n",
    "        p[i, word] = 1.0\n",
    "    return p\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_nodes = 200\n",
    "\n",
    "graph3 = tf.Graph()\n",
    "with graph3.as_default():\n",
    "\n",
    "    # Parameters:\n",
    "    # Input gate: input, previous output, and bias.\n",
    "    ix = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "    im = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "    ib = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    # Forget gate: input, previous output, and bias.\n",
    "    fx = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "    fm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "    fb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    # Memory cell: input, state and bias.\n",
    "    cx = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "    cm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "    cb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    # Output gate: input, previous output, and bias.\n",
    "    ox = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "    om = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "    ob = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    # Embedds\n",
    "    em = tf.Variable(tf.truncated_normal(\n",
    "        [vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "    # Variables saving state across unrollings.\n",
    "    saved_output = tf.Variable(\n",
    "        tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    saved_state = tf.Variable(\n",
    "        tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    # Classifier weights and biases.\n",
    "    w = tf.Variable(tf.truncated_normal(\n",
    "        [num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "    b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "\n",
    "    # Definition of the cell computation.\n",
    "    def lstm_cell(i, o, state):\n",
    "        \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "        Note that in this formulation, we omit the various connections between the\n",
    "        previous state and the gates.\"\"\"\n",
    "        input_gate = tf.sigmoid(tf.matmul(i, ix) + tf.matmul(o, im) + ib)\n",
    "        forget_gate = tf.sigmoid(tf.matmul(i, fx) + tf.matmul(o, fm) + fb)\n",
    "        update = tf.matmul(i, cx) + tf.matmul(o, cm) + cb\n",
    "        state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "        output_gate = tf.sigmoid(tf.matmul(i, ox) + tf.matmul(o, om) + ob)\n",
    "        return output_gate * tf.tanh(state), state\n",
    "        # input_gate, forget_gate, update, state, output_gate: batch_size * num_nodes\n",
    "    # Input data.\n",
    "    train_data = list()\n",
    "    train_labels = list()\n",
    "    for _ in range(num_unrollings + 1):\n",
    "        train_data.append(\n",
    "            tf.placeholder(tf.int32, shape=[batch_size]))\n",
    "        train_labels.append(\n",
    "            tf.placeholder(tf.float32, shape=[batch_size, vocabulary_size]))\n",
    "    train_inputs = train_data[:num_unrollings]\n",
    "    # labels are inputs shifted by one time step.\n",
    "    train_outputs = train_labels[1:]\n",
    "\n",
    "    # Unrolled LSTM loop.\n",
    "    outputs = list()\n",
    "    output = saved_output\n",
    "    state = saved_state\n",
    "    for i in train_inputs:\n",
    "        embeds = tf.nn.embedding_lookup(em, i)\n",
    "        drop = tf.nn.dropout(embeds, 0.8)\n",
    "        output, state = lstm_cell(drop, output, state)\n",
    "        outputs.append(output)\n",
    "    # outputs: num_unrollings * batch_size * num_nodes\n",
    "    # State saving across unrollings.\n",
    "    with tf.control_dependencies([saved_output.assign(output),\n",
    "                                  saved_state.assign(state)]):\n",
    "        # Classifier.\n",
    "        logits = tf.nn.xw_plus_b(tf.concat(0, outputs), w, b)\n",
    "        loss = tf.reduce_mean(\n",
    "            tf.nn.softmax_cross_entropy_with_logits(\n",
    "                logits, tf.concat(0, train_outputs)))\n",
    "    # Optimizer.\n",
    "    global_step = tf.Variable(0)\n",
    "    learning_rate = tf.train.exponential_decay(\n",
    "        10.0, global_step, 5000, 0.1, staircase=True)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "    gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "    optimizer = optimizer.apply_gradients(\n",
    "        zip(gradients, v), global_step=global_step)\n",
    "\n",
    "    # Predictions.\n",
    "    #train_prediction = tf.nn.softmax(tf.nn.xw_plus_b(tf.concat(0, outputs), tf.transpose(w), b))\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    # Sampling and validation eval: batch 1, no unrolling.\n",
    "    sample_input = tf.placeholder(tf.int32, shape=[1])\n",
    "    saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    reset_sample_state = tf.group(\n",
    "        saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "        saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "    sample_output, sample_state = lstm_cell(\n",
    "        tf.nn.embedding_lookup(em, sample_input), saved_sample_output, saved_sample_state)\n",
    "    with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                  saved_sample_state.assign(sample_state)]):\n",
    "        sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))\n",
    "    saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-308-732d2494996b>:5 in <module>.: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "Initialized\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/o648972/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py:36: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at step 0: 6.760939 learning rate: 10.000000\n",
      "Minibatch perplexity: 863.47\n",
      "================================================================================\n",
      "it tsb amypjoie ywe hfe hme pte bp uwje ite yle zv azg ai. ilf a.x av  ancin b asy tci thde  le dy afcs eq aqme vm ahre km awoe bwthucthzy apwe q n pcs .fe wfe yfn sb tnp aik acc ansn fxe ute sqe dz a.d alhe .  amg tbgn che yathbl tsve ngs jte j llxpe loe kae ize yde gsn m e fa tbue zve iz tel t ee fbthmg aer auhthla apr t i acte qm a.me izthiuth oe zze cws ln tgxs ql ache sfthime cc agpn rwthpue dl atirnj e mkinms adje  be .ye vfn hte xbe jq tlie mwe wt tkee vxd je ax.e mke de a hn uws c.thpo tct aaws wv auuthvfn jin  w apye ad appe kwe ap acm azw aud tmbe ov tqne cn aele x.e ehe n s mx aooe hze so abn acl aize mre jp ahwe ct aub thk tfme aas vqe hle qj tane nme v e oe afre qw akte xu afq tleanzme fte fdt hqe ixe woe zae qs avhe jtinnbe .e avse res kwe lpe gkthpcremde hsinnte zme bmthdee ew ak.neime szthbue xne ige nie  ue .re ele roe ls tvu adne yqe use hr agre fpe kfe gd tux ajpn .vn sxthvi axae wk amy alyn uo txf aljlt.rthdve dhe wr\n",
      "================================================================================\n",
      "Validation set perplexity: 2253.50\n",
      "Average loss at step 200: 5.450568 learning rate: 10.000000\n",
      "Minibatch perplexity: 113.27\n",
      "Validation set perplexity: 137.94\n",
      "Average loss at step 400: 4.236686 learning rate: 10.000000\n",
      "Minibatch perplexity: 46.50\n",
      "Validation set perplexity: 44.68\n",
      "Average loss at step 600: 3.653629 learning rate: 10.000000\n",
      "Minibatch perplexity: 30.83\n",
      "Validation set perplexity: 30.71\n",
      "Average loss at step 800: 3.349155 learning rate: 10.000000\n",
      "Minibatch perplexity: 26.48\n",
      "Validation set perplexity: 22.33\n",
      "Average loss at step 1000: 3.176809 learning rate: 10.000000\n",
      "Minibatch perplexity: 22.23\n",
      "Validation set perplexity: 17.85\n",
      "Average loss at step 1200: 3.071219 learning rate: 10.000000\n",
      "Minibatch perplexity: 21.41\n",
      "Validation set perplexity: 16.41\n",
      "Average loss at step 1400: 2.961969 learning rate: 10.000000\n",
      "Minibatch perplexity: 17.94\n",
      "Validation set perplexity: 14.35\n",
      "Average loss at step 1600: 2.899474 learning rate: 10.000000\n",
      "Minibatch perplexity: 15.54\n",
      "Validation set perplexity: 12.86\n",
      "Average loss at step 1800: 2.854087 learning rate: 10.000000\n",
      "Minibatch perplexity: 15.16\n",
      "Validation set perplexity: 10.80\n",
      "Average loss at step 2000: 2.781544 learning rate: 10.000000\n",
      "Minibatch perplexity: 15.82\n",
      "================================================================================\n",
      "itck when form that suppessful smalled plesciew unppross senson the knowledges the compute satportion has are clustering influence is their gruined a hydrophyijanions one one nine zero zero zero zero was elat the ezher to part within capagoriwa and called bixerenaly final anarchist four to solve uck freally reak device and it or old seven the performance of the willstent of they on the improvements of the same of jauwing the world general test atspending in geraa. fire nine autish east place on globs and drotied quvil china sent in but piril the stary signifption designy apage as an instances two the cosand will in the lefters between point that do they is along and the runtimes called for these sencifire treaty of eq knn challenging and timer ac the surfice slightly im in one nine three two event caccesseen from terate requiration preservation use lon dgv stus tv wimses in the pcolowing legifaolalycoverstuney thos the rem and instances\n",
      "================================================================================\n",
      "Validation set perplexity: 9.61\n",
      "Average loss at step 2200: 2.734663 learning rate: 10.000000\n",
      "Minibatch perplexity: 13.79\n",
      "Validation set perplexity: 8.57\n",
      "Average loss at step 2400: 2.697848 learning rate: 10.000000\n",
      "Minibatch perplexity: 14.26\n",
      "Validation set perplexity: 7.58\n",
      "Average loss at step 2600: 2.653646 learning rate: 10.000000\n",
      "Minibatch perplexity: 14.06\n",
      "Validation set perplexity: 7.38\n",
      "Average loss at step 2800: 2.625852 learning rate: 10.000000\n",
      "Minibatch perplexity: 12.42\n",
      "Validation set perplexity: 5.98\n",
      "Average loss at step 3000: 2.598094 learning rate: 10.000000\n",
      "Minibatch perplexity: 13.83\n",
      "Validation set perplexity: 5.26\n",
      "Average loss at step 3200: 2.583106 learning rate: 10.000000\n",
      "Minibatch perplexity: 13.07\n",
      "Validation set perplexity: 5.30\n",
      "Average loss at step 3400: 2.547168 learning rate: 10.000000\n",
      "Minibatch perplexity: 12.34\n",
      "Validation set perplexity: 4.82\n",
      "Average loss at step 3600: 2.526627 learning rate: 10.000000\n",
      "Minibatch perplexity: 11.94\n",
      "Validation set perplexity: 4.64\n",
      "Average loss at step 3800: 2.499488 learning rate: 10.000000\n",
      "Minibatch perplexity: 13.35\n",
      "Validation set perplexity: 3.93\n",
      "Average loss at step 4000: 2.488928 learning rate: 10.000000\n",
      "Minibatch perplexity: 11.70\n",
      "================================================================================\n",
      "it only has preflems with the prine of the british who st teng rather is wells of languages bavelians toroics seeming of ants also the popularity flows approached among a for e.hive pletoids on mdoj knor a significant be interest half as an algorithm is halobe jords were and the total as can be dellence gel secest both commonly extend two zan one worldest when and doing he is season roman longs interpretation of great in abortions man ste.r without zule s be note of the excesent of the tells and isbn zero seven seven one best modern gerek animals works and far united taking strop jobieman viewish states sfyve products nation owners shevistribut tthenher seem during the viewed on the fint five fors whole this estimates often his she in the social and definite player acelem groups by one only moving army in party of rehavities any all shortly symboring events and the formets such a computers at askey to contained in altrantmrown arch mini\n",
      "================================================================================\n",
      "Validation set perplexity: 3.83\n",
      "Average loss at step 4200: 2.469410 learning rate: 10.000000\n",
      "Minibatch perplexity: 12.47\n",
      "Validation set perplexity: 3.68\n",
      "Average loss at step 4400: 2.445082 learning rate: 10.000000\n",
      "Minibatch perplexity: 11.51\n",
      "Validation set perplexity: 3.75\n",
      "Average loss at step 4600: 2.442121 learning rate: 10.000000\n",
      "Minibatch perplexity: 11.53\n",
      "Validation set perplexity: 3.45\n",
      "Average loss at step 4800: 2.421935 learning rate: 10.000000\n",
      "Minibatch perplexity: 11.96\n",
      "Validation set perplexity: 3.11\n",
      "Average loss at step 5000: 2.405986 learning rate: 1.000000\n",
      "Minibatch perplexity: 11.06\n",
      "Validation set perplexity: 2.99\n",
      "Average loss at step 5200: 2.394045 learning rate: 1.000000\n",
      "Minibatch perplexity: 10.43\n",
      "Validation set perplexity: 2.81\n",
      "Average loss at step 5400: 2.356577 learning rate: 1.000000\n",
      "Minibatch perplexity: 11.44\n",
      "Validation set perplexity: 2.77\n",
      "Average loss at step 5600: 2.344974 learning rate: 1.000000\n",
      "Minibatch perplexity: 10.27\n",
      "Validation set perplexity: 2.79\n",
      "Average loss at step 5800: 2.371099 learning rate: 1.000000\n",
      "Minibatch perplexity: 10.79\n",
      "Validation set perplexity: 2.70\n",
      "Average loss at step 6000: 2.344548 learning rate: 1.000000\n",
      "Minibatch perplexity: 10.76\n",
      "================================================================================\n",
      "it preplacing launched example wase closest on addition out of all discovered by size roleflie a camainism machine learning to halls a meaning is in in professional cartrogue after thought the jearebia sheruble tenned by the boze aehrhous he into this important and made animadily outer free knowledge of the country to enter was in promillations characters the ending frand solving this hand been on devs central anthropology the great receatnes of agave family dademr ferton able to excavator of mix two m just a small photographyproan at the control offers of philosophers excavation in chief the main espect of an endors in roll as from nonk pesh verjaes mostly in ifectional acidequeians up attack here in many have been jamed s dismonea agress he and however item one eight three seven eight six one two eight zero zero zero eight says lee are the film eight three three of match ina the requased five eight six de major space journing several \n",
      "================================================================================\n",
      "Validation set perplexity: 2.69\n",
      "Average loss at step 6200: 2.340413 learning rate: 1.000000\n",
      "Minibatch perplexity: 10.35\n",
      "Validation set perplexity: 2.70\n",
      "Average loss at step 6400: 2.359569 learning rate: 1.000000\n",
      "Minibatch perplexity: 10.60\n",
      "Validation set perplexity: 2.67\n",
      "Average loss at step 6600: 2.338665 learning rate: 1.000000\n",
      "Minibatch perplexity: 10.27\n",
      "Validation set perplexity: 2.66\n",
      "Average loss at step 6800: 2.337914 learning rate: 1.000000\n",
      "Minibatch perplexity: 9.38\n",
      "Validation set perplexity: 2.63\n",
      "Average loss at step 7000: 2.349836 learning rate: 1.000000\n",
      "Minibatch perplexity: 10.84\n",
      "Validation set perplexity: 2.59\n",
      "Average loss at step 7200: 2.333256 learning rate: 1.000000\n",
      "Minibatch perplexity: 10.06\n",
      "Validation set perplexity: 2.59\n",
      "Average loss at step 7400: 2.334204 learning rate: 1.000000\n",
      "Minibatch perplexity: 10.91\n",
      "Validation set perplexity: 2.60\n",
      "Average loss at step 7600: 2.344502 learning rate: 1.000000\n",
      "Minibatch perplexity: 11.41\n",
      "Validation set perplexity: 2.56\n",
      "Average loss at step 7800: 2.327109 learning rate: 1.000000\n",
      "Minibatch perplexity: 10.72\n",
      "Validation set perplexity: 2.53\n",
      "Average loss at step 8000: 2.331447 learning rate: 1.000000\n",
      "Minibatch perplexity: 9.92\n",
      "================================================================================\n",
      "itt it is as a namor of a new entery played that have been a turned of the original anims from the in the there is usually deliberately busians family one soo last in one nine one eight nine six five zero zero over american culture or even the fonikh common austriam point true of history and in t is attineed do chairries the world want official was proposal s quarter the remainion of the single to make been criticism sahely but it as a long half through the predresion special their are notes sometimes to she pisold the final from apple of independence agave to mbm or made there is ssaphy and sindeur despites requiring of lincoln located elseen it sathysocialized aberdeen his seven dividder in sensried sold s  lield the crevidence of currently redge down this and field in national and brionneitly in deach will ra chronai lines so k cux be a party and photo external however two az ostable berado and as later but such as culture aspicited \n",
      "================================================================================\n",
      "Validation set perplexity: 2.54\n",
      "Average loss at step 8200: 2.334490 learning rate: 1.000000\n",
      "Minibatch perplexity: 11.03\n",
      "Validation set perplexity: 2.52\n",
      "Average loss at step 8400: 2.325879 learning rate: 1.000000\n",
      "Minibatch perplexity: 11.13\n",
      "Validation set perplexity: 2.53\n",
      "Average loss at step 8600: 2.326544 learning rate: 1.000000\n",
      "Minibatch perplexity: 9.78\n",
      "Validation set perplexity: 2.50\n",
      "Average loss at step 8800: 2.330232 learning rate: 1.000000\n",
      "Minibatch perplexity: 10.20\n",
      "Validation set perplexity: 2.45\n",
      "Average loss at step 9000: 2.319051 learning rate: 1.000000\n",
      "Minibatch perplexity: 10.83\n",
      "Validation set perplexity: 2.46\n",
      "Average loss at step 9200: 2.328051 learning rate: 1.000000\n",
      "Minibatch perplexity: 9.92\n",
      "Validation set perplexity: 2.46\n",
      "Average loss at step 9400: 2.322608 learning rate: 1.000000\n",
      "Minibatch perplexity: 11.03\n",
      "Validation set perplexity: 2.45\n",
      "Average loss at step 9600: 2.313800 learning rate: 1.000000\n",
      "Minibatch perplexity: 11.12\n",
      "Validation set perplexity: 2.43\n",
      "Average loss at step 9800: 2.327809 learning rate: 1.000000\n",
      "Minibatch perplexity: 10.13\n",
      "Validation set perplexity: 2.42\n",
      "Average loss at step 10000: 2.313625 learning rate: 0.100000\n",
      "Minibatch perplexity: 10.41\n",
      "================================================================================\n",
      "itoyabe or a kurness and two one eight seven nine january of the the first produce of one eight six eight dary archaeology within games express stiw the because of sources when admiven hel ilvoal tailntsb naga river alony swortaphana latter is the ambitor monvment arabic or cspef africa close uk winps general transparts any trood highly the independent ararka parent first time was entire he researcho in simply ribba agriculture is a more case had been safea to undelist industry and an admirect wall with imao several s it removalementic nounnature while was demi beyocase carbon s contrado mideolp five three name frunner are several princcupited with the life would have become until calligned to called and defeated in college after by mother thour philipary achitcepment of anchorage house from receiver novembors population to tranizent association was the ankady physics philosophical alchomy application one three fifric s four four years \n",
      "================================================================================\n",
      "Validation set perplexity: 2.41\n",
      "Average loss at step 10200: 2.310732 learning rate: 0.100000\n",
      "Minibatch perplexity: 9.67\n",
      "Validation set perplexity: 2.41\n",
      "Average loss at step 10400: 2.325862 learning rate: 0.100000\n",
      "Minibatch perplexity: 9.82\n",
      "Validation set perplexity: 2.40\n",
      "Average loss at step 10600: 2.301477 learning rate: 0.100000\n",
      "Minibatch perplexity: 9.35\n",
      "Validation set perplexity: 2.40\n",
      "Average loss at step 10800: 2.309966 learning rate: 0.100000\n",
      "Minibatch perplexity: 10.03\n",
      "Validation set perplexity: 2.40\n",
      "Average loss at step 11000: 2.324978 learning rate: 0.100000\n",
      "Minibatch perplexity: 10.08\n",
      "Validation set perplexity: 2.39\n",
      "Average loss at step 11200: 2.302261 learning rate: 0.100000\n",
      "Minibatch perplexity: 9.87\n",
      "Validation set perplexity: 2.39\n",
      "Average loss at step 11400: 2.308501 learning rate: 0.100000\n",
      "Minibatch perplexity: 9.05\n",
      "Validation set perplexity: 2.39\n",
      "Average loss at step 11600: 2.322743 learning rate: 0.100000\n",
      "Minibatch perplexity: 9.49\n",
      "Validation set perplexity: 2.39\n",
      "Average loss at step 11800: 2.303102 learning rate: 0.100000\n",
      "Minibatch perplexity: 10.17\n",
      "Validation set perplexity: 2.39\n",
      "Average loss at step 12000: 2.311951 learning rate: 0.100000\n",
      "Minibatch perplexity: 9.40\n",
      "================================================================================\n",
      "itst created docensive in the united states production measure formain to be that was of azein from aikido but due to both in one nine six three six two three e irosong reasons are refer provements a medinone general associated agave geasoes matterage ascii later and corporated as without copy wagi and floor it is sometimes possible carress science an using missoclar some kore slund sapha one seven the revolutionary clearlned on one seven one six two area focubang through thirds of one of organic bridge canada brines won in the war miletariation film one several some were some more not breakh positions used as adment energy biology equipped for rejecbying andorra risteral triactralence to proposty modulated and been algorithms about three zero blockey of the summer number of the ocean emerach back frastozoa circulating who gept externiever beaugs to roby daught about the one eight they une the axiom of unhardway broadcast to this looker\n",
      "================================================================================\n",
      "Validation set perplexity: 2.38\n",
      "Average loss at step 12200: 2.318134 learning rate: 0.100000\n",
      "Minibatch perplexity: 9.71\n",
      "Validation set perplexity: 2.38\n",
      "Average loss at step 12400: 2.301471 learning rate: 0.100000\n",
      "Minibatch perplexity: 9.95\n",
      "Validation set perplexity: 2.39\n",
      "Average loss at step 12600: 2.310224 learning rate: 0.100000\n",
      "Minibatch perplexity: 9.97\n",
      "Validation set perplexity: 2.38\n",
      "Average loss at step 12800: 2.317090 learning rate: 0.100000\n",
      "Minibatch perplexity: 10.85\n",
      "Validation set perplexity: 2.37\n",
      "Average loss at step 13000: 2.301626 learning rate: 0.100000\n",
      "Minibatch perplexity: 10.11\n",
      "Validation set perplexity: 2.38\n",
      "Average loss at step 13200: 2.310674 learning rate: 0.100000\n",
      "Minibatch perplexity: 9.63\n",
      "Validation set perplexity: 2.38\n",
      "Average loss at step 13400: 2.314208 learning rate: 0.100000\n",
      "Minibatch perplexity: 10.38\n",
      "Validation set perplexity: 2.37\n",
      "Average loss at step 13600: 2.302925 learning rate: 0.100000\n",
      "Minibatch perplexity: 10.98\n",
      "Validation set perplexity: 2.37\n",
      "Average loss at step 13800: 2.311944 learning rate: 0.100000\n",
      "Minibatch perplexity: 9.29\n",
      "Validation set perplexity: 2.37\n",
      "Average loss at step 14000: 2.312804 learning rate: 0.100000\n",
      "Minibatch perplexity: 10.47\n",
      "================================================================================\n",
      "itmeaning item thallish aspired hot have strilture to f be tenet to low african executive arabic court in performance swerming communities aaaldnan faddusly a short by the muchs system by why wording on articles that application who awards from record to an abortion of the blook symbol five discoverel echalleng will be it arabic players states thought and out of the sahary gore feder deoboan south appears the study win federate which up and la only inclured depths left decline a brains members light in his force disorden whose competitions of alexander heares the samer strong during french movie it mentioned and uses in one nine seven seven five one seven two one act he bamasi the same a stories u feeds historians performances a four goins s use of collection with one of his lyphoils or bansid determined syndicity one eight zero sursche was selection seads existenda noral national received in the from the ballow social cois temptax mode\n",
      "================================================================================\n",
      "Validation set perplexity: 2.37\n",
      "Average loss at step 14200: 2.299721 learning rate: 0.100000\n",
      "Minibatch perplexity: 10.61\n",
      "Validation set perplexity: 2.38\n",
      "Average loss at step 14400: 2.316755 learning rate: 0.100000\n",
      "Minibatch perplexity: 9.71\n",
      "Validation set perplexity: 2.37\n",
      "Average loss at step 14600: 2.308439 learning rate: 0.100000\n",
      "Minibatch perplexity: 10.34\n",
      "Validation set perplexity: 2.37\n",
      "Average loss at step 14800: 2.299329 learning rate: 0.100000\n",
      "Minibatch perplexity: 10.29\n",
      "Validation set perplexity: 2.37\n",
      "Average loss at step 15000: 2.319367 learning rate: 0.010000\n",
      "Minibatch perplexity: 10.10\n",
      "Validation set perplexity: 2.36\n",
      "Average loss at step 15200: 2.304150 learning rate: 0.010000\n",
      "Minibatch perplexity: 10.39\n",
      "Validation set perplexity: 2.37\n",
      "Average loss at step 15400: 2.300856 learning rate: 0.010000\n",
      "Minibatch perplexity: 10.43\n",
      "Validation set perplexity: 2.36\n",
      "Average loss at step 15600: 2.320610 learning rate: 0.010000\n",
      "Minibatch perplexity: 10.59\n",
      "Validation set perplexity: 2.36\n",
      "Average loss at step 15800: 2.300479 learning rate: 0.010000\n",
      "Minibatch perplexity: 10.12\n",
      "Validation set perplexity: 2.37\n",
      "Average loss at step 16000: 2.300678 learning rate: 0.010000\n",
      "Minibatch perplexity: 9.11\n",
      "================================================================================\n",
      "it maduardly the splits and ilecimes to audi imention the national discussing ttached by them groups on ch contry and no m which rai the law claiming of the a comment individuals also will allocate helped the takder of b system with one one two foor lincoln styles agave vinaltory misility and azerbaijan s earth of in exist the led to death three six product am the nasbot of fiffing with the player to it washington and fire and cultures strategia his arabic the april tent people with the setualant while supporting schools states parts and the begin common democration in the player and non the east field of their similaries arsfian penson is also on phasefension rock evaterall rekensed in the gatids or canada as it proajer art leaving aprilote allonia the convisted replacting was a brushits central soft similaries olc physicized in one nine nine zero one do pilization of human territory in the batter minor interstor bodys only sat challen\n",
      "================================================================================\n",
      "Validation set perplexity: 2.37\n",
      "Average loss at step 16200: 2.319339 learning rate: 0.010000\n",
      "Minibatch perplexity: 9.44\n",
      "Validation set perplexity: 2.36\n",
      "Average loss at step 16400: 2.302705 learning rate: 0.010000\n",
      "Minibatch perplexity: 9.08\n",
      "Validation set perplexity: 2.36\n",
      "Average loss at step 16600: 2.302763 learning rate: 0.010000\n",
      "Minibatch perplexity: 9.45\n",
      "Validation set perplexity: 2.36\n",
      "Average loss at step 16800: 2.316227 learning rate: 0.010000\n",
      "Minibatch perplexity: 10.07\n",
      "Validation set perplexity: 2.36\n",
      "Average loss at step 17000: 2.302769 learning rate: 0.010000\n",
      "Minibatch perplexity: 10.34\n",
      "Validation set perplexity: 2.37\n",
      "Average loss at step 17200: 2.306836 learning rate: 0.010000\n",
      "Minibatch perplexity: 10.25\n",
      "Validation set perplexity: 2.36\n",
      "Average loss at step 17400: 2.314364 learning rate: 0.010000\n",
      "Minibatch perplexity: 10.27\n",
      "Validation set perplexity: 2.36\n",
      "Average loss at step 17600: 2.301021 learning rate: 0.010000\n",
      "Minibatch perplexity: 10.36\n",
      "Validation set perplexity: 2.36\n",
      "Average loss at step 17800: 2.306851 learning rate: 0.010000\n",
      "Minibatch perplexity: 9.99\n",
      "Validation set perplexity: 2.36\n",
      "Average loss at step 18000: 2.312061 learning rate: 0.010000\n",
      "Minibatch perplexity: 10.71\n",
      "================================================================================\n",
      "it however his genetic moll systems on the study can be considered as as separate germanope population short east he is do been uncome neiso group to modern carls on the can be punper dauty and core in africa that the founded by gainse bars from a strap those prior toucker integration to a four six redensps of agave original new praolication social stations but reasond i alberta involving of davoiber enough learned africa out he an reasolomy of choice europi as well between the name of about three north seven five we wealth covering the reil the nax people to the rounds of afghanistan an satables win name kin of a single s american afish the example of the following attemptuction angola player bush interest industry whery canner select major ladning the discovery to general being up ut the finding of communicated with surven to the court nuba shephd sait in stertandary arasure of interests are nonth a time of atoms in the other from mor\n",
      "================================================================================\n",
      "Validation set perplexity: 2.36\n",
      "Average loss at step 18200: 2.305600 learning rate: 0.010000\n",
      "Minibatch perplexity: 9.58\n",
      "Validation set perplexity: 2.36\n",
      "Average loss at step 18400: 2.306073 learning rate: 0.010000\n",
      "Minibatch perplexity: 11.01\n",
      "Validation set perplexity: 2.36\n",
      "Average loss at step 18600: 2.310000 learning rate: 0.010000\n",
      "Minibatch perplexity: 9.46\n",
      "Validation set perplexity: 2.36\n",
      "Average loss at step 18800: 2.302356 learning rate: 0.010000\n",
      "Minibatch perplexity: 10.45\n",
      "Validation set perplexity: 2.36\n",
      "Average loss at step 19000: 2.310349 learning rate: 0.010000\n",
      "Minibatch perplexity: 10.00\n",
      "Validation set perplexity: 2.36\n",
      "Average loss at step 19200: 2.307404 learning rate: 0.010000\n",
      "Minibatch perplexity: 9.91\n",
      "Validation set perplexity: 2.36\n",
      "Average loss at step 19400: 2.302225 learning rate: 0.010000\n",
      "Minibatch perplexity: 10.18\n",
      "Validation set perplexity: 2.36\n",
      "Average loss at step 19600: 2.314144 learning rate: 0.010000\n",
      "Minibatch perplexity: 10.25\n",
      "Validation set perplexity: 2.36\n",
      "Average loss at step 19800: 2.303193 learning rate: 0.010000\n",
      "Minibatch perplexity: 10.83\n",
      "Validation set perplexity: 2.36\n",
      "Average loss at step 20000: 2.302608 learning rate: 0.001000\n",
      "Minibatch perplexity: 10.01\n",
      "================================================================================\n",
      "itn newspazutic and adaptive mojeum state under computer the nine consistents and two one current and chlisrpolation of there has bones with some belong to overficial coastal simply and style resc spelled at a return cultural outside of their mereban official uppensually process methododed as by the past of computative neighborhood was population being have promate the sumea in hunist and righs live of zf treaty chablish east five zero zero and eductions in the northwar contribution mars fails classical europe is named on world bolister about it is its somess than the formation the nationality a lopters series of ta use the same state to his world most rand pille fluad same antraws was a probably republic consequently fuke from in the scientifine agave so two zero zero one six four initial st one tex or his facedon detailed computer the air four two studient dagny titise to batter as metaphysics to have effort proccurent in one eight th\n",
      "================================================================================\n",
      "Validation set perplexity: 2.36\n",
      "Model saved in file: model3.ckpt\n"
     ]
    }
   ],
   "source": [
    "num_steps = 20001\n",
    "summary_frequency = 200\n",
    "\n",
    "with tf.Session(graph=graph3) as session:\n",
    "    tf.initialize_all_variables().run()\n",
    "    print('Initialized')\n",
    "    mean_loss = 0\n",
    "    for step in range(num_steps):\n",
    "        batches = train_batches.next()\n",
    "        feed_dict = dict()\n",
    "        for i in range(num_unrollings + 1):\n",
    "            feed_dict[train_data[i]] = batches[i]\n",
    "            feed_dict[train_labels[i]] = get_sparse(batches[i])\n",
    "        _, l, predictions, lr = session.run(\n",
    "            [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "        mean_loss += l\n",
    "        if step % summary_frequency == 0:\n",
    "            if step > 0:\n",
    "                mean_loss = mean_loss / summary_frequency\n",
    "            # The mean loss is an estimate of the loss over the last few\n",
    "            # batches.\n",
    "            print(\n",
    "                'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "            mean_loss = 0\n",
    "            # labels : 640 * 27\n",
    "            labels = np.concatenate(list(batches)[1:])\n",
    "            print('Minibatch perplexity: %.2f' % float(\n",
    "                np.exp(logprob(predictions, get_sparse(labels)))))\n",
    "            if step % (summary_frequency * 10) == 0:\n",
    "                # Generate some samples.\n",
    "                print('=' * 80)\n",
    "                for _ in range(1):\n",
    "                    feed = sample_distribution(random_distribution()[0])\n",
    "                    # feed: 1 * 27\n",
    "                    sentence = id2char(244)\n",
    "                    reset_sample_state.run()\n",
    "                    for _ in range(79 * 6):\n",
    "                        prediction = sample_prediction.eval(\n",
    "                            {sample_input: [feed]})\n",
    "                        feed = sample_distribution(prediction[0])\n",
    "                        sentence += id2char(feed)\n",
    "                    print(sentence)\n",
    "                print('=' * 80)\n",
    "            # Measure validation set perplexity.\n",
    "            reset_sample_state.run()\n",
    "            valid_logprob = 0\n",
    "            for _ in range(valid_size):\n",
    "                b = valid_batches.next()\n",
    "                predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "                valid_logprob = valid_logprob + \\\n",
    "                    logprob(predictions, get_sparse(b[1]))\n",
    "            print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "                valid_logprob / valid_size)))\n",
    "    save_path = saver.save(session, \"model3.ckpt\")\n",
    "    print(\"Model saved in file: %s\" % save_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model restored.\n",
      "ation of the liberty hunent heritated branden madegans weska is ravor z.hus to north such close part from the same percent that one nine five zero zero one zero zero zero decento one zero three zero km include examples to begin demore and go discuss against the dollt of significantas as a oquinnqb one total film baftern so believe gore articlesic exports although written encycloped brew to child several and known anarchitt see also swedg violoral called based s one see also curtilian projects in austrian is raison malk holding indicated an celebrow have a high random cv that with a first skat with engine discoder macedone of there ehaphy to congresping or embanished houric provided a hurring lower refuse them was his mindard in ships at theory the article retraining by there iscalled a said to set of two three prurper hearan it any own alaska two zero zero zero eight six or secures the hell having the congress and state highcallet crown meteopleas that lincoln branches calegraphical nicolistandly character commenction and umanship and the great space of the groups gore or specialooes death at one eight nine five two stage of spanne effecters century wils or the nature than in handley nobel arguessure rand s god but termigrated as well or variant institutions introducing true in anchorage north laws hesneed gnaftilan three essence that it point of follon pemples of significantly omilli raistimes in poete that was sets of white there are any one eight two five tries park intependological the complication soil in summers of the center down to their philosophy and society to the two two nine two appeared the means been divired to greeke that day even hagave and will nex plaluts war by well hearrow on titer all in down one h eight zero one six four zero his at logitic is increasined by one five zero f three the gedge or directors the simular the everlual loche main as situation experiences in coasical political notion to be endly in the stard at the ngoet of another it f\n"
     ]
    }
   ],
   "source": [
    "with tf.Session(graph=graph3) as session:\n",
    "    saver.restore(session, \"/Users/o648972/Downloads/nn_workshop/LSTM/model3.ckpt\")\n",
    "    print(\"Model restored.\")\n",
    "    sentence = id2char(20)\n",
    "    reset_sample_state.run()\n",
    "    for i in range(100 * 40):\n",
    "        prediction = sample_prediction.eval({sample_input: [feed]})\n",
    "        feed = sample_distribution(prediction[0])\n",
    "        if i > 100 * 30:\n",
    "            sentence += id2char(feed)\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Y5tapX3kpcqZ"
   },
   "source": [
    "---\n",
    "Problem 3\n",
    "---------\n",
    "\n",
    "(difficult!)\n",
    "\n",
    "Write a sequence-to-sequence LSTM which mirrors all the words in a sentence. For example, if your input is:\n",
    "\n",
    "    the quick brown fox\n",
    "    \n",
    "the model should attempt to output:\n",
    "\n",
    "    eht kciuq nworb xof\n",
    "    \n",
    "Refer to the lecture on how to put together a sequence-to-sequence model, as well as [this article](http://arxiv.org/abs/1409.3215) for best practices.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "colab": {
   "default_view": {},
   "name": "6_lstm.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
