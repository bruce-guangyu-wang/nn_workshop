 algorithm portfolios based on costsensitive hierarchical clustering yuri malitsky cork constraint computation centre university college cork ireland y.malitskyc.ucc.ie ashish sabharwal horst samulowitz meinolf sellmann ibm watson research center yorktown heights ny usa ashish.sabharwalsamulowitzmeinolfus.ibm.com abstract different solution approaches for combinatorial problems often exhibit incomparable performance that depends on the concrete problem instance to be solved. algorithm portfolios aim to combine the strengths of multiple algorithmic approaches by training a classifier that selects or schedules solvers dependent on the given instance. we de vise a new classifier that selects solvers based on a costsensitive hierarchical clustering model. exper imental results on sat and maxsat show that the new method outperforms the most effective portfo lio builders to date. introduction algorithm portfolios are an essential tool for boosting prob lem solving performance. in essence the idea of algorithm portfolios is to select one or schedule several solvers out of a pool of algorithms based on the problem instance that is to be solved. this technology has proven key for providing stateoftheart performance in satisfiability sat quantified boolean formulae qbfs and constraint programming cp as exemplified by the performance of portfolios in the respec tive solver competitions. . related work on algorithm portfolios algorithm portfolios at least in their most recent advent were pioneered by gomes and selman and xu et al. . the first really prominent instance of a solver portfolio was satzilla a sat solver portfolio. it was based on a linear regression model to predict log runtime for each solver. for a while the trend was towards more sophis ticated machine learning models such as a hiddenclass port folio generator silverthorn and miikkulainen or col laborative expert portfolio management stern et al. . in parallel the idea to schedule solvers meaning to run a se lection of solvers for some designated time was introduced by streeter and smith and omahony et al. . based on these methods kadioglu et al. introduced the idea of scheduling solvers while still picking one long running solver. the notable feature of this portfolio named s was that it used a lowbias the authors call it nonmodel based machine learning technique for selecting the long running solver. namely the latter is conducted by means of a knearest neighbor approach which proved very effective for sat. the and incarnations of satzilla xu et al. a b followed this trend and introduced a costsensitive lowbias classification model approach based on random forests and voting. a recent comparison of the satzilla and the s portfolio building approaches was conducted by amadini et al. for building a port folio of cp solvers which revealed a statistical tie between both methods for this benchmark whereby s appears to be performing marginally better. . motivation and scope the motivation of this work is to provide a portfolio builder that works efficiently across a wide range of problem do mains. for sat and for cp we have fairly wellestablished feature sets that can be used to characterize a given prob lem instance and predict which solvers will be most ef fective at solving it quickly. however we strongly believe that algorithm portfolios could be equally effective in boost ing solver performance in other domains such as maxsat or global continuous optimization or mixedinteger program ming mip where no good feature sets are readily available. we expect that the satzilla methodology will generalize to other domains better than s as each feature that has little predictive power regarding which solver is better suited for solving a problem deteriorates knearest neighborbased s performance. while there are ways to handle this e.g. feature selection guyon and elisseeff it is at the very least tedious for the nonexpert to have to select the right features. moreover even when us ing scaling techniques knearest neighbor cannot give differ ent weight to features in different parts of the feature space. satzilla on the other hand can easily accommodate features which are not helpful and use features only in those areas of the search space where they actually help us differ entiate between solvers. on the other hand ss scheduler is based on traditional optimization mip technology while satzilla uses a heuristicsbased presolver schedule. surprisingly we found that training this presolver takes a significant amount of time in satzilla. furthermore satzilla trains one random forest for each pair of solvers. this creates a computational bottleneck as the training time obviously squares when the number of solvers doubles. one goal of this paper is to perform costsensitive multiclass classifica tion directly and not by means of binary classification in order to alleviate this computational burden. we propose a new portfolio builder that combines ss static scheduler with a new algorithm selector based on cost sensitive hierarchical clustering cshc which creates a multi class classification model runs orders of magnitude faster than satzilla and is less sensitive to poor fea ture sets than s. we describe the method in detail and provide extensive empirical results comparing cshc with satzilla and s on various benchmark sets for port folio generators. toward a new algorithm portfolio builder clustering is a standard technique in unsupervised learning equally fundamental and simple as knearestneighbor clas sification in supervised learning. the two methods bear methodological similarities as well. the result of knearest neighbor learning is the labeling of compact areas of the fea ture space by one class just as clustering groups of instances into compact clusters. in fact the knearestneighbor in swas inspired by clusterbased instancespecific algorithm configuration shown by kadioglu et al. . while clustering is certainly appealing it makes sense to center clusters at the test instance that needs to be solved. that is exactly what the knn approach does. moreover tra ditional clustering is unsupervised. for the purpose of algo rithm portfolios we would like to group training instances in such a way that they can agree on the class they should be labeled with. for us that means that training instances in the same cluster should preferably not disagree violently on which solver solves them most efficiently or at all in case of timeouts. finally while clustering is traditionally based on some metric of distance to accommodate poor feature sets we need a clustering method that allows us to select when or under what circumstances we use the dimensions that features provide are used for determining what goes into the same cluster. . costsensitive hierarchical clustering we propose a learning method which we named costsensitive hierarchical clustering cshc. highlevel the method pro ceeds as follows. first all instances are in the same cluster. now for each cluster in the current set of clusters we test whether a given minimum number of instances is still present in that cluster we typically set this required minimum to ten instances. if not we remove the cluster from consideration for further partitioning. otherwise we split the cluster in such a way that the instances within each of the two clusters max imally agree on the class they should be labeled with. we will investigate a variant where we add another phase where we consider merging clusters again for which the split resulted in diminishing crossvalidation performance. at runtime we compute the given instances feature vec tor and determine which cluster it belongs to. then we em ploy the solver that performs best on the training instances in this cluster. costsensitive learning problem more formally we consider the following incomplete clas sification problem. given are a set of classes c and a set of training instances t each associated with a feature vector f irf . moreover we are provided with a costvector that associates each instance i t with a cost mic which represents the cost of misclassifying instance i by class c c. the task is to infer a classification algorithm which will assign a class to new instances such that their associated misclassification cost is minimized. we label this classifica tion problem incomplete as we obviously do not know what the misclassification costs for the given instance are or how these can be inferred from the misclassification costs that we observe for the training instances. it is worth noting that the problem considered is similar yet fundamentally different from semisupervised problems considered in machine learning. in particular elkan considered the problem where the misclassification costs are determined by a given c c matrix which assigns a mis classification cost to each pair of classes where the first is the preferred class of the instance and the second is the class we label it with. obviously when building an algorithm portfo lio there does not exist and in any case we would not have access to such a classmisclassification matrix. it is interesting to note that the modelbased portfolio ap proaches xu et al. silverthorn and miikkulainen essentially aim to predict the misclassification costs for a given instance. one advancement was the realization that better predictions are obtained when we focus on pre dicting a preferred class without taking the detour of fore casting the misclassification costs for each class for the given instance omahony et al. kadioglu et al. xu et al. b. there exist many other costsensitive learning methods see for example lenarcik and piasta ting zadrozny et al. klinkenberg and ruping geibel and wysotzki wysotzki and geibel . it is beyond the scope of this paper to give a compre hensive overview. however the only costsensitive multi classification approaches for the costmodel that applies to portfolios that we are aware of are the ones employed by sand satzilla whereby the latter is based on a se ries of binary classifications. approach during cshcs learning phase we maintain a set of clusters starting with all training instances in one cluster. in each iter ation we remove one cluster from the current set. if it is too small we do not partition it further. otherwise we partition the cluster in two parts by running a hyperplane through the feature space so that instances fall on either side of the plane. when labeling an instance i with class c we incur the cost mi c. the total misclassification cost when labeling a subcluster with a specific class is the sum of all costs for all instances in that cluster. the cost of a binary split is the cost of labeling each subcluster with a class that gives the lowest cost. we select a hyperplane to split the cluster so that the cost of the binary split is minimized. if none of the subclusters are empty we add both to the current set of clusters. in a potentially following phase we consider undoing some cluster splits based on the crossvalidation performance that we would achieve by labeling the whole cluster with one class compared to the crossvalidation performance when doing the same for the two subclusters. if the performance of the split is not better we undo the partition. at runtime for the first of available time the portfolio executes the same static schedule as s. if this does not result in a solution we use the hyperplanes used for splitting the base set to determine which cluster the test instance belongs to and we try solving it with the solver that has the overall best performance on all training instances in that cluster. decision trees as we will discuss shortly in practice we will consider axis parallel hyperplanes to obtain a tractable training method. this means that the clustering approach could very well also be viewed as a decision tree approach where the ground set of training instances is repeatedly partitioned by branching on individual features. the main difference to existing cost sensitive desision tree methods is the way how we compute the cost of a split. instead of considering entropy or cost sensitive entropy we favor splits where in both parts the in stances can agree on a consensus label that may not be perfect for any one instance but that incurs minor costs for all. . bagging maximummargin hyperplanes feature augmentation in this section we discuss several alternative techniques that we considered adding to the basic cshc methodology. later we will test these empirically and quantify their effect on overall portfolio performance. bagging the first technique we consider is bootstrap aggregating bag ging a wellknown technique in machine learning for im proving the stability of classification and regression algo rithms. in our case we generate bootstrap samples to create training sets which are then clustered. that is rather than cre ating just one clustering using all training instances we create multiple clusterings based on subsets of the training set. the obvious question arises how we should then combine the potentially conflicting classifications from different clus terings. we propose four different ways on how this informa tion can be combined par aggregation collect for each clustering the respective training instances in each cluster that the test instance belongs to. we can assemble these instances into a set or a multiset of instances. then to determine the solver the given instance should be solved with we compute which solver has the best performance in terms of average runtime with timeouts penalized by a factor of par on the set or multiset of instances in the various different clusters the test instance belongs to. winner aggregation determine the best class for each cluster the test instance belongs to. then we label the test instance with the class that was chosen most often. rank aggregation rank the classes in each cluster the test instance belongs to. then we choose the class that has the best average rank over all clusterings. satzillastyle aggregation recall that satzilla builds a random forest for each pair of classes which in our case correspond to solvers. using the analogy of axisparallel clustering and decision trees discussed above the only difference being how we split each random tree can be viewed as a clustering of training instances. in satzilla the information from each cluster are aggregated by first aggregating over all trees and then counting how many other solvers each solver is able to outperform. we can emulate this aggregation scheme by computing for each ordered pair of solvers for how many of our bagged clusterings the first solver outperforms the second. if this number is at least half we say the first solver outperforms the second otherwise we say the second outperforms the first. we then compute for each solver how many other solvers it outperforms. we choose the solver that outperforms the most other solvers. maximummargin hyperplanes earlier we said that we would compute a hyperplane to split the base instances such that the performance of the best solver on each side is maximized. this is in general a hard opti mization problem. we consider two heuristics for comput ing such hyperplanes. the first is to consider featureaxis parallel hyperplanes only which renders the resulting opti mization problem tractable. using this method our cluster ing approach can be viewed as a decisiontree procedure. in a second variant we consider accepting the best split according to an axisparallel hyperplane but then tilt it so as to achieve a splitting hyperplane that achieves the same base partition with a maximum margin. we found that nonaxisparallel hy perplanes do not boost performance yet increase the learning time considerably which is why we do not pursue this avenue further in this paper. featureaugmentation one welcome advantage that cshc shares with the satzilla approach is the ability to accommodate fea tures that have limited prediction potential. that is we can add a bunch of new features or combinations of the existing features without fearing that performance would decline. we consider adding products and quotients of pairs of features to our feature set. analyzing the components of cshc a priori it is not clear which of the variants of cshc performs best. in this section we therefore perform an experimental comparison of different versions of the costsensitive cluster ing approach we have proposed. typically in the literature on algorithm portfolios we find that different papers consider different sets of features differ ing sets of solvers and differing train and test sets. recently malitsky created an actual benchmark for algorithm . . . . . . . . . . . t ra in in g t im e p e r s p li t s e c number of clusterings indus hands rands figure training time of cshc as a function of the number of clusterings portfolios that standardizes these choices. it allows us to com pare different algorithm portfolio methods on a level playing field as the benchmark for a portfolio builder really consists of all available features the set of base solvers as well as training and test sets. all experiments in this section are based on the dataset provided by the ubc group after satzilla won the sat challenge . the data is broken down into crafted in dustrial and random categories. instances in each category were solved by the same sat solvers with a second timeout. malitsky removed all instances that cannot be solved by any solver within the allotted timeout and broke down each of the data sets into parts suitable for crossfold validation. instances are characterized by the features the ubc team proposed option base. for each of the three benchmark sets we created five benchmarks with two three four twelve and all solvers by removing solvers from the base set. in the rest of this section we evaluate the contribution of various components of cshc to its performance and use the findings to tune basic choices in cshc how many clusterings to use what aggregation mechanism to use etc.. in sec tion we then compare the resulting tuned cshc with other portfolio solvers. to keep this latter comparison fair we tune the components of cshc on a different benchmark than the benchmarks we will use later to compare with other portfolio builders. . number of bootstrap samples first we investigate the impact of the number of cluster ings on the robustness stability and scalability of cshc. we ran cshc with various numbers of bootstrap samples ranging from to whereby each sample contains of all available training instances. we also tested various other per centages but that had little effect on the overall trend. the ex periments were run on the benchmark data mentioned above and results reported are the averages over the data splits used for crossvalidation. figure shows for each number of clusterings on the hor izontal axis the average training time per split on the verti httpwww.cs.ubc.calabsbetaprojects satzilla . . . . . . . . . . . n o rm a li ze d p a r s co re number of clusterings indus hands rands figure par performance of cshc as a function of the number of clusterings cal axis. there is one curve each for the industrial crafted and random instance benchmarks. as expected we see that the training time of cshc scales linearly as the number of clusterings grows. training never took more than min utes per split on this benchmark even with cluster ings. this is massively faster than satzilla which on these benchmark sets with solvers needs cpudays to be trained. figure shows again as a function of the number of clusterings the normalized average par score observed across the splits. for each of the industrial crafted and random benchmark sets the par score plotted here is nor malized by the average of the par scores across all splits. note that as the number of clusterings grows this measure will eventually converge to independent of the benchmark. we observe that cshc is rather unstable for up to about clusterings but begins to show reasonable stability after a few hundred clusterings. while using more clusterings makes performance more ro bust there is clearly a tradeoff with growing training time. for the rest of the experiments reported in this paper we chose to use clusterings which kept cshc reasonably fast yet stable. . aggregation we discussed four different aggregation schemes earlier par winner rank and satzillainspired aggregation. in table we present our numerical results when using these aggregators. our first observation from this data is that the satzilla inspired aggregation method is clearly inferior to all other techniques. note that while the method is inspired by satzilla this does not mean that within that portfo lio builder the aggregation method used is suboptimal. first satzilla builds a different random forest for each pair of solvers. cshc on the other hand builds just one set of clusterings which performs multiclass classification. the latter gives an advantage in terms of the time needed for learn these experiments were conducted on a core intel xeon . ghz machine with gb ram running scientific linux .. downloaded from httpwww.cs.ubc.calabs betaprojectssatzilla. table performance comparison of the four different aggre gators with a timeout of seconds. shown is the average percent solved over the tenfold cross validation. sat family par winner rank satzilla crafted . . . . industrial . . . . random . . . . table performance comparison of the three different aggre gators using and solvers from the satzilla benchmark timeout of seconds. shown is the average percent solved over the tenfold cross validation. sat family solvers par winner rank industrial . . . . . . . . . . . . crafted . . . . . . . . . . . . random . . . . . . . . . . . . ing. however it may very well have an adverse effect on this aggregation method. second the way how we cluster the dif ferent bootstrap samples differs fundamentally from the cost sensitive randomforests that satzilla creates. between the remaining aggregators there is no clear win ner. we therefore created more benchmarks by selecting sub sets of basesolvers. table shows the comparison between the remaining aggregators on these additional benchmarks. overall we find that the rankbased aggregation performs with good average performance while displaying the smallest variance in performance. it works second best on the crafted and random categories and across the board best on indus trial instances. . combination features as discussed earlier unlike s both cshc and satzilla can naturally accommodate more fea table impact of adding additional features based on quo tients and products on the different satzilla benchmarks with a timeout of seconds. shown are the average par scores across the splits and the corresponding training time per split in minutes. features industrial crafted random added par trtime par trtime par trtime . . . . . . . . . . . . table performance comparison of cshc with and without merging on the satzilla benchmark with a timeout of seconds. shown are the average percentage solved and the average par runtime in seconds on the test sets of the tenfold cross validation. cshcmerging cshc benchmark par par industrial . . crafted . . random . . tures and simply not use them if they are not helpful. given k original features we consider adding k randomly chosen pairwise products and quotients of these features to cshc. if f the quotient ff is treated as if f is also and as a cutoff value of signf otherwise. table shows the result of adding k and such feature combinations. for each benchmark we report the resulting performance in terms of par score av eraged over the splits along with the average training time needed per split. we find that adding products and quotients does not lead to a substantial difference in the par per formance on this dataset at least on the industrial instances. on crafted and random instances using k results in a somewhat improved performance. however the improve ment is unstable across different benchmark categories and comes at the cost of minutes to nearly hours of train ing time per split compared to under minutes without these additional features. we therefore chose not to use such aug mented features for the remainder of this empirical study. . crossvalidationbased merging of clusters when describing the cshc approach we discussed the pos sibility of undoing cluster splits based on a crossvalidation. we implemented this technique and compare it to the plain costsensitive hierarchical clustering we proposed. we show our results in table . across the board we ob serve that the merging of split clusters actually diminishes performance. we tested a number of other techniques for undoing splits such as merging if the performance of the split declines on the instances left out for training by the cur rent bootstrap sample. however all these techniques showed the same trend the simpler technique of costsensitive hi erarchical clustering without clustermerging performs bet ter. this observation matches the wellknown fact that when constructing decision forests it is better not to prune the trees. comparison with satzilla and s there are two portfolios which excelled in the last two sat competitionchallenges. in s kadioglu et al. won the crafted and random categories. in satzilla xu et al. a won the industrial and crafted categories. amadini et al. compared the two approaches when building portfolios of constraint programming solvers. it was already noted there that satzilla needed much more table performance comparison of s satzilla and cshc on the maxsat benchmark. shown are the aver age percent solved timeout seconds and par. s satzilla cshc maxsat par par par ms crafted . . . pms crafted . . . pms indu . . . wpms crafted . . . wpms indu . . time for learning especially as the number of solvers in creases. in terms of test performance amadini et al. found that both builders result in almost the same perfor mance with a slight edge in favor of s which was not statis tically significant. to the best of our knowledge to date this is the only comparison of satzilla and s on a level playing field where both builders use the same set of solvers the same features and the the same traintest splits. . maxsat using malitskys portfolio benchmark malitsky we compare cshc with s and satzilla. in table we compare cshc with s and satzilla on various maxsat benchmarks. all builders use the same set of base solvers and the same features that are provided for each in stance. the numbers represent the average test performance over the same ten crossvalidation splits that are part of the benchmark. the benchmark consists of instances gathered from the var ious categories of the maxsat evaluation. there are five benchmarks in total crafted unweighted maxsat ms crafted and industrial partial maxsat pms and crafted and industrial weighted partial maxsat wpms. these in stances were run with a second timeout with to maxsat solvers depending on the benchmark. we use the ten crossvalidation splits provided by malitsky . for the features the maxsat instance was converted into an equivalent instance where all the soft clauses are unit. this is achieved by reifying the soft clauses. finally the compu tation of the features is performed using the satzilla feature code on the hard clauses of the maxsat formula. we observe that ss performance is rather erratic. at times it works really well other times it works significantly worse than satzilla and cshc. it performs equally well as cshc on crafted maxsat benchmarks and it clearly outshines all other methods on weighted partial maxsat in dustrial. on the other hand it works much worse on all re maining benchmarks. comparing satzilla and cshc we find that the latter performs on par on crafted partial maxsat and better on all other benchmarks. especially on the two weighted maxsat benchmarks cshc works notably better than satzilla. . sat next we use the benchmark based on data from the satzilla webpage. this data utilizes the dataset pro table performance comparison of s satzilla and cshc on the satzilla benchmark with a timeout of seconds. shown are the average percentage solved and the average par runtime in seconds. s satzilla cshc benchmark par par par industrial . . . crafted . . . random . . . vided by the ubc group coupled with their satzilla portfolio matlab code. the data is broken down into crafted indus trial and random categories. instances in each category were taken from the sat challenge and run on the solvers at that competition for crafted for industrial and for random. timeout is seconds. malitsky again removed all instances that could not be solved by any solver within the allotted timeout and provided ten standard ized crossvalidation splits. consider table . we observe the same trend as we had noted on the maxsat benchmarks. s behaves errati cally on the random category it performs almost as good as satzilla and cshc but on the crafted and industrial instances it performs much worse. we further note that the costsensitive clustering approach again outperforms satzilla on every category. how ever while these results are highly encouraging the ten splits that were provided by malitsky are not enough to con firm statistical significance. conclusion we devised a costsensitive hierarchical clustering approach for building algorithm portfolios. we applied this method to benchmarks from maxsat and sat. the empirical analysis showed that adding feature combinations can improve perfor mance slightly at the cost of increased training time while merging cluster splits based on crossvalidation lowers pre diction accuracy. moreover we found that aggregating across multiple clusterings based on class rank works most robustly. we then compared the new approach with the most successful stateoftheart algorithm portfolios satzilla and s. we found that ss performance is rather unpredictable for some benchmarks it works really well while for others it builds underachieving portfolios. satzilla works much more stable but is outperformed on all benchmarks we tested by costsensitive hierarchical clustering. furthermore training cshc is much faster than satzilla espe cially as the number of base solvers grows. acknowledgements this work was partially supported by the european commis sion through the icon fetopen project grant agreement . downloaded from httpwww.cs.ubc.calabs betaprojectssatzilla. references amadini et al. roberto amadini maurizio gab brielli and jacopo mauro. an empirical evaluation of portfolios approaches for solving csps. in cpaior . elkan charles elkan. the foundations of cost sensitive learning. in ijcai pages . geibel and wysotzki peter geibel and fritz wysotzki. learning perceptrons and piecewise lin ear classifiers sensitive to example dependent costs. appl. intell. . gomes and selman c.p. gomes and b. selman. al gorithm portfolios. artificial intelligence journal . guyon and elisseeff isabelle guyon and andre elis seeff. an introduction to variable and feature selection. j. mach. learn. res. . kadioglu et al. s. kadioglu y. malitsky m. sell mann and k. tierney. isac instancespecific algorithm configuration. proc. of the th european conference on artificial intelligence ecai pages . kadioglu et al. s. kadioglu y. malitsky a. sabhar wal h. samulowitz and m. sellmann. algorithm selec tion and scheduling. cp . klinkenberg and ruping ralf klinkenberg and ste fan ruping. concept drift and the importance of example. in text mining pages . lenarcik and piasta andrzej lenarcik and zdzislaw piasta. rough classifiers sensitive to costs varying from object to object. in rough sets and current trends in computing pages . malitsky yuri malitsky. algorithm portfolio bench mark set . httpc.ucc.ie ymalitsky algorithmportfoliobenchmarkset.html. omahony et al. e. omahony e. hebrard a. hol land c. nugent and b. osullivan. using casebased reasoning in an algorithm portfolio for constraint solving. irish conference on artificial intelligence and cognitive science . silverthorn and miikkulainen b. silverthorn and r. miikkulainen. latent class models for algorithm portfolio methods. aaai . stern et al. d. stern h. samulowitz r. herbrich t. graepel l. pulina and a. tacchella. collaborative ex pert portfolio management. aaai . streeter and smith m. streeter and s.f. smith. using decision procedures efficiently for optimization. icaps pages . ting k. m. ting. an instanceweighting method to induce costsensitive trees. ieee trans. on knowl. and data eng. . wysotzki and geibel fritz wysotzki and peter geibel. a new information measure based on example dependent misclassification costs and its application in decision tree learning. adv. artificial intellegence . xu et al. l. xu f. hutter h.h. hoos and k. leyton brown. satzilla portfoliobased algorithm selection for sat. jair . xu et al. a l. xu f. hutter j. shen h.h. hoos and k. leytonbrown. satzilla improved algo rithm selection based on costsensitive classification mod els. solver description . sat challenge . xu et al. b lin xu frank hutter holger hoos and kevin leytonbrown. evaluating component solver con tributions to portfoliobased algorithm selectors. in sat pages . zadrozny et al. bianca zadrozny john langford and naoki abe. costsensitive learning by cost proportionate example weighting. in rd intl. conf. on data mining pages .  algorithm selection and scheduling serdar kadioglu yuri malitsky ashish sabharwal horst samulowitz and meinolf sellmann brown university dept. of computer science providence ri usa serdarkynmcs.brown.edu ibm watson research center yorktown heights ny usa ashish.sabharwalsamulowitzmeinolfus.ibm.com abstract. algorithm portfolios aim to increase the robustness of our ability to solve problems efficiently. while recently proposed algorithm selection methods come ever closer to identifying the most appropriate solver given an input instance they are bound to make wrong and at times costly decisions. solver scheduling has been proposed to boost the performance of algorithm selection. scheduling tries to allocate time slots to the given solvers in a portfolio so as to maximize say the number of solved instances within a given time limit. we show how to solve the corresponding optimization problem at a low computational cost using column generation resulting in fast and high quality solutions. we inte grate this approach with a recently introduced algorithm selector which we also extend using other techniques. we propose various static as well as dynamic scheduling strategies and demonstrate that in comparison to pure algorithm selection our novel combination of scheduling and solver selection can significantly boost performance. introduction the constraint reasoning community has a long tradition of introducing and refining ideas whose practical impact often goes far beyond the fields scope. one such contribution is that of robust solvers based on the idea of algorithm portfolios cf. . motivated by the observation that solvers have complementary strengths and therefore exhibit incomparable behavior on different problem instances algorithm portfolios run multiple solvers in par allel or select one solver based on the features of a given instance. portfolio research has led to a wealth of different approaches and an amazing boost in solver performance in the past decade. one of the biggest success stories is that of satzilla which combines existing boolean satisfiability sat solvers and has now dominated various categories of the sat competition for about half a decade . another example is cphydra a portfolio of constraint programming cp solvers which won the csp competition. instead of choosing a single solver for an instance silverthorn and miikkulainen pro posed a dirichlet compound multinomial distribution to create a schedule of solvers to be run in sequence. other approaches e.g. dynamically switch between a portfolio of solvers based on the predicted completion time. alterna tively argosmart and hydra focus on not only choosing the best solver for an instance but also the best parametrization of that solver. for a further overview of the stateoftheart in portfolio generation see the thorough survey by smithmiles . a recently proposed algorithm selector for sat based on nearestneighbor classification serves as the foundation for our work here. first we present two extensions to it involving distancebased weighting and clusterguided adap tive neighborhood sizes demonstrating moderate but consistent performance improvements. then we develop a new hybrid portfolio that combines algorithm selection and algorithm scheduling in static and dynamic ways. to this end we present a heuristic method for computing solver schedules efficiently which omahony et al. identified as an open problem. this also enables us to quantify the impact of various scheduling strategies and to report those findings accordingly. finally we are able to show that a completely new way of solver scheduling consisting of a combination of static schedules and solver selection is able to achieve significantly better results than plain algorithm selection. using sat as the testbed we demonstrate through extensive numerical ex periments that our approach is able to handle even highly diverse benchmarks in particular a mix of random crafted and industrial instance categories with a single portfolio. this is in contrast to for example satzilla which has his torically excelled only in different versions that were specifically tuned for each category. our approach also works well even when the training set is not fully representative of the test set that needs to be solved. nearestneighborbased algorithm selection malitsky et al. recently proposed a simple yet highly effective algorithm se lector for sat based on nearestneighbor classification. we review this approach here before proposing two improvements to it in section and algorithm sched ules in section . nearestneighbor classification knn is a classic machine learning approach. in essence we base our decision for a new instance on prior experience with the k training instances most similar to it. as the similarity measure between instances we simply use the euclidean or l distance on core features of sat instances that satzilla is based on . each feature is linearly normalized to fit the interval across all training instances. as the solver performance measure we use the par score of the solver on these k instances. par score for a given timelimit t is a hybrid measure defined as the average of the runtimes for solved instances and of t for unsolved instances. it is thus a combined measure of number of instances solved and average solution time. it is wellknown in machine learning that nn i.e. k often does not generalize well to formerly unseen examples as it tends to overfit the training data. a very large value of k on the other hand defeats the purpose of consider ing local neighborhoods. to find the right value of k we employ another classic algorithm algorithm selection using nearestneighbor classification knnalgorithmselection phase input a problem instance f params nearest neighborhood size k candidate solvers s training instances ftrain along with feature vectors and solver runtimes output a solver from the set s begin compute normalized features of f f set of k instances from ftrain that are closest to f return solver in s that has the best par score on f end training phase input candidate solvers s training instances ftrain time limit tmax params neighborhood range kmin kmax number of subsamples m split ratio mbmv output best performing k reduced ftrain along with feature and runtimes begin run each solver s s for time tmax on each f ftrain record runtimes remove from ftrain instances solved by no solver or by all within second compute feature vectors for each f ftrain for k kmin kmax do scorek for i ..m do fbasefvalidation a random mbmv split of ftrain add to scorek performance of knn portfolio on fvalidation using training instances fbase and solver selection based on par scorek scorekm kbest argminkscorek return kbest ftrain feature vectors runtimes end strategy in machine learning namely random subsampling validation. the idea is to repeat the following process several times randomly split the training data into a base set and a validation set train on the base set and assess how well the learned approach performs on the validation set. we use a basevalidation split and perform random subsampling times. we then finally choose the k that yields the best par performance averaged across the validation sets. algorithm gives a more formal description of the entire algorithm in terms of its usage as a portfolio solver i.e. algorithm selection given a new instance as described above and the random subsampling based training phase per formed to compute the best value for k to use. the training phase starts out by computing the runtimes of all solvers on all training instances. it then removes all instances that cannot be solved by any solver in the portfolio within the time limit or are solved by every solver in the portfolio within marginal time e.g. second for reasonably challenging benchmarks learning to distinguish table . comparison of baseline solvers portfolios and vbs performances par average runtime in seconds and number of instances solved timeout seconds. pure solvers portfolios vbsagw agw gnov sat march pico kcnfs sat knn sat sat elty enstein sat zilla par avg time solved solved . . . . . . . . . . between solvers based on data from such instances is pointless. along with the estimated best k the training phase passes along this reduced set of training instances their runtimes for each solver and their features to the main solver selection phase. we emphasize that the training phase does not learn any sophis ticated model e.g. a runtime prediction model rather it simply memorizes the training performances of all solvers and learns only the value of k. despite the simplicity of this approach compared for example to the de scription of satzilla it is highly efficient and outperforms satzilla r the gold medal winning solver in the random category of sat competition . in table we compare simple knn algorithm selection with satzilla r using the random category instances from sat competitions as the training set and the such instances from sat competition as the test set. both portfolios are based on the following local search solvers agwsat agwsat gnovelty kcnfs march dl picosat . and satenstein all in the versions that are identical to the ones that were used when satzilla r entered the competition. to make the comparison as fair as possible knn uses only the core instance features that satzilla is based on and is trained for parscore. for both training and testing we use a time limit of seconds. table shows that satzilla boosts performance of individual solvers dramatically. the pure knn approach pushes the perfor mance level substantially further. it solves more instances and closes about one third of the gap between satzilla r and the virtual best solver vbs which solves instances. improving nearestneighborbased solver selection we now discuss two mutually orthogonal techniques to further improve the per formance of the algorithm selector outlined in section . the exact runtimes in table are lower than the ones reported in due to faster machines dual intel xeon . ghz quadcore nehalem processors with gb of ddr memory. the relative drop in the performance of kcnfs we believe is also due to this hardware difference. vbs refers to the oracle that always selects the solver that is fastest on the given instance. its performance is the best one can hope to achieve with algorithm selection. distancebased weighting. a natural extension of knn is to scale the scores of the k neighbors of an instance based on the euclidean distance to it. intuitively speaking inspired by omahony et al. we assign larger weights to instances that are closer to the test instance assuming that closer instances more accurately reflect the properties of the instance at hand. hence in lines and of algo rithm when computing the par score for solver selection for an instance f we scale a solver ss penalized runtime i.e. actual runtime or tmax on a neighbor f by distff totaldist where totaldist corresponds to the sum of all distances from f to instances in the neighborhood under consideration. clusteringbased adaptive neighborhood size. rather than learning a single value for k we adapt the size of the neighborhood based on the properties of the given test instance. to this end we partition the instance feature space by clustering the training instances using gmeans clustering . an instance is considered to belong to the cluster it is nearest to breaking ties arbitrarily. algorithm can be easily adapted to learn one k for each cluster. given a test instance we first determine the cluster to which it belongs and then use the value of k learned for this cluster during training. we note that our clustering is used to select only the size of the neighborhood based on instance features not to limit the neighborhood itself neighboring instances from other clusters can still be used when determining the best solver based on par score. . experimental setup and evaluation we now describe the benchmark used for portfolio evaluation in the rest of this paper. note that such a benchmark involves not only training and testing instances but also the base solvers used for building portfolios. the challenging benchmark setting we consider mixes incomplete and complete sat solvers as well as industrial crafted and random instances. after describing these we will assess the impact of weighting clustering and their combination on pure knn. note that the reported runtimes include all overhead incurred by our portfolios. benchmark solvers. we consider the following stateoftheart complete and incomplete sat solvers . clasp . cryptominisat . glucose . lineling . lysat i . lysat c . marchhi . marchnn . minisat .. . mxc . precosat . adaptgwsat . adaptgwsat . gnovelty . gnoveltyh . hybridgm . kcnfssat . picosat . saps . tnm and . satenstein . we in fact use six different parametriza tions of satenstein resulting in a total of base solvers. in addition we prepro cess all industrial and crafted instances with satelite version . with default option pre and let the following solvers run on both original and preprocessed version of each instance . clasp . cryptominisat . glucose . lineling . lysat c . lysat i . marchhi . marchnn . minisat . mxc and . precosat. our portfolio is thus composed of solvers. preprocessing usually does not improve performance on random instances. table . average performance comparison of basic knn weighting clustering and the combination of both using the knn portfolio. numbers in braces show in how many of the trainingtest splits does incorporating weighting and clustering outperform basic knn column . basic knn weighting clustering weight.clust. solved unsolved solved . . . . avg runtime par score benchmark instances. we selected instances from all sat competitions and races during and whereby we discarded all instances that cannot be solved by any of the aforementioned solvers within the competition time limit of seconds i.e. the vbs can solve of all instances. now we need to partition these instances into disjoint sets of training and testing instances. in research papers we often find that only one trainingtest split of the instances is considered. moreover commonly this split is computed at random thereby increasing the likelihood that the training set is quite repre sentative of the test set. we propose to adopt some best practices from machine learning and to consider multiple splits as well as a more challenging partitioning into training and test sets. our objective for the latter is to generate splits where entire benchmark families are completely missing in the training set while for other families some instances are present in both the training and in the test partition. to asses which instances are related we use the the first three charac ters in the prefix of an instance name and assume that instances starting with the same three characters belong to the same benchmark family. we select at random about of benchmark families and include them fully in the test partition this typically resulted in roughly of all instances being in the test partition. next we randomly add more instances to the test partition until it has about of all instances resulting in a split. the such partitions used in our experimentation are available online for future reference. results. table summarizes the performance gain from using weighting cluster ing and the combination of the two. we show the average performance across the trainingtest splits mentioned above in terms of number of instances solvednot solved average runtime and par score. depending on the perfor mance measure the combination of weighting and clustering is able to improve performance of basic knn on anywhere from to out of the splits shown in braces in the rightmost column. the gain is modest but serves as a good incremental step for the rest of this paper. for completeness we remark that these modest gains also translate to the benchmark discussed in table where the combination of weighting and clus httpwww.cs.toronto.edu horstcptrainingtestsplits.zip tering solves more instances than basic knn and more than satzilla r. we will return to this benchmark towards the end of this paper. building solver schedules to further increase the robustness of our approach we consider computing sched ules that define a sequence of solvers to try along with individual time limits given an instance. the general idea was previously introduced by streeter and in cphydra . in fact streeter uses the idea of scheduling to gener ate algorithm portfolios. while he suggested using schedules that can suspend solvers and let them continue later on in exactly the same state they were sus pended in we will focus on solver schedules without preemption i.e. each solver will appear in the schedule at most once. this setting was also used in cphydra which computes a schedule of cp solvers based on k nearest neighbors. we note that a solver schedule can never outperform the vbs. in fact a schedule is no better than the vbs with a reduced captime of the longest run ning solver in the schedule. therefore trivial schedules that split the available time evenly between all solvers have inherently limited performance. the reason why we may be interested in solver schedules nevertheless is to hedge our bets we often observe that instances that cannot be solved by one solver even in a very long time can in fact be solved by another very quickly. consequently by allocating a reasonably small amount of time to other solvers we can provide a safety net in case our solver selection happens to be unfortunate. . static schedules the simplest approach is to compute a static schedule of solvers. for example we could compute a schedule that solves the most training instances within the allowed time cf. . we propose to do slightly more namely to compute a schedule that first solves most training instances and that second requires the lowest amount of time among all schedules that are able to solve the same amount of training instances. we can formulate this problem as an integer program ip more precisely as a resource constrained set covering problem rcscp where the goal is to select a number of solverruntime pairs that together cover i.e. solve as many training instances as possible solver scheduling ip min c x i yi x st txst s.t. yi x st ivst xst i x st txst c yi xst i s t binary variables xst correspond to sets of instances that can be solved by solver s within a time t. these sets have cost t and a resource consumption coefficient t. to make it possible that all training instances can be covered even when they remain unsolved we introduce additional binary variables yi. these correspond to the set that contains only item i they have cost c and time resource consumption coefficient . the constraints in this model enforce that we cover all training instances the additional resource constraint that we do not exceed the overall captime c. the objective is to minimize the total cost. due to the high costs for variables yi which will be if and only if instance i cannot be solved by the schedule schedules that solve most instances are favored and among those the fastest schedule is chosen as the cost of xst is t. . a column generation approach the main problem with the above formulation is the sheer number of variables. for our most uptodate benchmark with solvers and more than training instances solving the above problem is impractical even when we choose the timeouts t smartly such that from timeout t to the next timeout t at least one more instance can be solved by the respective solver vst vst. in our experiments we found that the actual time to solve these ips may at times still be tolerable but the memory consumption was often prohibitively high. we therefore propose to solve the above problem approximately using col umn generation aka dantzigwolfe decomposition a wellknown technique for handling linear programs lps with a lot of variables . we discuss it briefly in the general setting. consider the lp min ctx s.t. ax b x in the presence of too many variables it is often not practical to solve the large system directly. the core observation underlying column generation is that only a few variables i.e. columns will be nonzero in any optimal lp solution at most as many as there are constraints. therefore if we knew which variables are important we could consider a much smaller system a x b where a contains only a few columns of a. when we choose only some columns in the beginning lp duality theory tells us which columns that we have left out so far are of interest for the optimization of the global lp. namely only columns with negative reduced costs which are defined based on the optimal duals of the system a x b can help the objective to decrease further. column generation proceeds by considering in turn a master problem the reduced system a x b and a subproblem where we select a new column to be added to the master based on its current optimal dual solution. this process is iterated until there is no more column with a negative reduced cost. at this point we know that an optimal solution to has been found even though most columns have never been added to the master problem when using standard lp solvers to solve the master problem and obtain its optimal duals all that is left is solving the subproblem. to develop a subproblem algorithm subproblem column generation begin minredcosts forall solvers s do t forall i do j i t t j t times j redcosts t t if redcosts minredcosts then solver s timeout t minredcosts redcost if minredcosts then return xsolvertimeout else return none end generator we need to understand how exactly the reduced costs are computed. assume we have a dual value i for each constraint in a. then the reduced cost of a column . . . zt is defined as c c i ii where c is the cost of column . equipped with this knowledge we can apply column generation to solve the continuous relaxation of the solver scheduling ip. to this end we begin the process by adding at the start all columns corresponding to variables y to our reduced system a. next we repeatedly generate and solve a subproblem whose goal is to suggest a solverruntime pair that is likely to increase the objective value of the continuous master problem the most. hence each column we add regards an xst variable specifically the one with minimal reduced cost. to find such an xst first for all solvers s we compute a permutation of the instances such that the time that s needs to solve instance si is less than or equal that the solver needs to solve instance si for appropriate i. see algorithm . obviously we only need to do this once for each solver and not each time we want to generate a new column. now let us denote with i the optimal dual value for the restriction to cover instance i . moreover denote with the dual value of the resource constraint since that constraint enforces a lowerorequal restriction is guaranteed to be nonpositive. finally for each solver s we iterate over i and compute the term t ki sk which in each iteration we can obviously derive from the previous value for t . let t denote the time that solver s needs to solve instance i. then the reduced costs of the column that corresponds to variable xst are t tt . we choose the column with the most negative reduced costs and add it to the master problem. if there is no more column with negative reduced costs we stop. we would like to point out two things. first note that what we have actually done is to pretend that all columns were present in the matrix and computed the reduced costs for all of them. this is not usually the case in column generation approaches where most columns are usually found to have larger reduced costs implicitly rather than explicitly. second note that the solution returned from this process will in general not be integer but contain fractional values. therefore the solution obtained cannot be interpreted as a solver schedule directly. this situation can be overcome in two ways. the first is to start branching and to generate more columns which may still be needed by the optimal integer solution even though they were superfluous for the optimal fractional solution. this process is known in the literature as branchandprice. what we propose and that is in fact the reason why we solved the original problem by means of column generation in the first place is to stick to the columns that were added during the column generation process and to solve the remaining system as an ip. obviously this is just a heuristic that may return suboptimal schedules for the training set. however we found that this process is very fast and nevertheless provides high quality solutions see empirical results in section .. even when the performance on the training set is at times slightly worse than optimal the performance on the test set often turned out as good or sometimes even better than that of the optimal training schedule a case where the optimal schedule overfits the training data. the last aspect that we need to address is the case where the final sched ule does not utilize the entire available time. recall that we even deliberately minimize the time needed to solve as many instances as possible. obviously at runtime it would be a waste of resources not to utilize the entire time that is at our disposal. in this case we scale each solvers time in the schedule equally so that the total time of the resulting schedule will be exactly the captime c. . dynamic schedules omahony et al. found that static schedules work only moderately well. therefore they introduced the idea of computing dynamic schedules at run time for a given instance cphydra considers the ten nearest neighbors in case of ties up to fifty and computes a schedule that solves as many of these instances as possible in the given time limit. accordingly the constraints in the solver scheduling ip are limited to the few instances in the neighborhood which allows cphydra to use a bruteforce approach to compute dynamic schedules at runtime. this is reported to work well thanks to the small neighborhood size and the fact that cphydra only has three constituent solvers. our column generation approach yielding potentially suboptimal but usu ally high quality solutions works fast enough to handle even solvers and over instances within seconds. this allows us to embed the idea of dynamic schedules in the previously developed nearestneighbor approach which selects optimal neighborhood sizes by random subsampling validation which requires us to solve hundreds of thousands of these ips. both clusterguided adaptive neighborhood size and weighting discussed ear lier can be incorporated into solver schedules as well. for the latter we suggest a slightly different approach than cphydra. specifically when given an instance table . average performance of semistatic schedules compared with no schedules and with static schedules based only on the available solvers. numbers in braces show in how many of the trainingtest splits does semistatic scheduling with weighting and clustering outperform the same approach without scheduling column . no sched. static sched. semistatic schedules wtgclu wtgclu basic knn weighting clustering wtgclu solved unsolved solved . . . . . . avg runtime par score f we adapt the objective function in the solver scheduling ip by multiplying the costs for the variables yi which were originally c with distffitotaldist . this favors schedules that solve more training instances that are closer to f . we thus obtain four variations of dynamic schedules. we also used a setting inspired by the cphydra approach size neighborhood size and weighting scheme as in . we refer to this approach as sathydra. in our experiments with dynamic schedules as well as sathydra we found the gain over and above knn solver selection with weights and clustering the rightmost column in ta ble was marginal. sathydra and dynamic schedule without weights and clustering for example each solved only more instances. due to limited space we omit detailed experimental numbers and instead move on to scheduling strate gies that turned out to be more effective. . semistatic solver schedules observe that the four algorithm selection portfolios that we developed in sec tion can themselves be considered solvers. we can add the portfolio itself to our set of constituent solvers and compute a static schedule for this augmented collection of solvers. we quote static here because the resulting schedule is of course still instancespecific. after all the algorithm selector portfolio chooses one of the constituent solvers based on the test instances features. we refer to the result of this process as semistatic solver schedules. depending on which of our four portfolios from section we use we obtain four semistatic schedules. we report their performance table . we observe that semistatic scheduling improves the overall performance in anywhere from to of the trainingtest splits considered depending on the performance measure used compare with column in the table for the best results without scheduling. all semistatic schedules here solve at least more instances within the time limit. again the combination of weighting and clustering achieves the best performance and it narrows the gap to vbs in percentage of instances solved to nearly . for further comparison in the column we show the per formance of a static schedule that was trained on the entire training set and is the same for all test instances. we can confirm the earlier finding that static table . comparison of column generation and the solution to the optimal ip. schedule by solved unsolved solved avg runtime s par score optimal ip . . . . . column generation . . . . . solver schedules are indeed inferior to dynamic schedules and find that they are considerably outperformed by semistatic solver schedules. quality of results generated by column generation. table illustrates the per formance of our column generation approach. we show a comparison of the resulting performance achieved by the optimal schedule. in order to compute the optimal solution to the ip we used cplex on a machine with sufficient memory and a second resolution to fit the problem into the available memory. as we can observe the column generation is able to determine a high quality schedule that results in a performance that nearly matches the one of the coarsegrained optimal schedule according to displayed measures. . fixedsplit selection schedules based on this success we consider a parametrized way of computing solver sched ules. as discussed earlier the motivation for using solver schedules is to increase robustness and hedge against an unfortunate selection of a longrunning solver. at the same time the best achievable performance of a portfolio is that of the vbs with a captime of the longest individual run. in both dynamic and semi static schedules the runtime of the longest running solvers was determined by the column generation approach working solely on training instances. this procedure inherently runs the risk of overfitting the training set. consequently we consider splitting the time between an algorithm selection portfolio and the constituent solvers based on a parameter. for example we could allocate of the available time for the solver selected by the portfolio. for the remaining of the time we run a static solver schedule. we refer to these schedules as selection schedules. note that choosing a fixed amount of time for the schedule of constituent solvers is likely to be suboptimal for the training set but offers the possibility of improving test performance. table captures the corresponding results. we observe that using this re stricted application of scheduling is able to outperform our best approach so far semistatic scheduling shown again in the first column which is outperformed consistently in out of trainingtest splits. we are able to solve nearly instances on average which is more than we were able to solve before and the gap to the virtual best solver is narrowed down to .. recall that we consider a highly diverse set of benchmark instances from the random crafted and industrial categories. moreover we do not work with plain random splits but splits where complete families of instances in the test set are not represented in the training set at all. table . average performance comparison of basic knn weighting clustering and the combination of both using the knn portfolio with a fixedsplit static sched ule. numbers in braces show in how many of the trainingtest splits does fixedsplit scheduling with weighting and clustering outperform the same approach with semi static scheduling column . semistatic fixedsplit schedules wtgclu basic knn weighting clustering wtgclu solved unsolved solved . . . . . avg runtime par score compared to the plain knn approach of malitsky et al. that we started with column of table the fixedsplit selection schedules close roughly one third of the gap to the vbs. the performance gain as measured by welchs t test is significant in most of the trainingtest splits. for example the pvalue for the ttest of an instance being solved or not by the two approaches has a median value of .. similarly the median pvalue across the splits for the penalized runtime is . indicating the improvements are statistically significant. summary and discussion we considered the problem of algorithm selection and scheduling so as to maxi mize performance when given a hard time limit within which we need to provide a solution. we considered two improvements for simple nearestneighbor solver selection weighting and adaptive neighborhood sizes based on clustering. then we developed a lightweight optimization algorithm to compute nearoptimal schedules for a given set of training instances. this allowed us to provide an ex tensive comparison of pure algorithm selection static solver schedules dynamic solver schedules and semistatic solver schedules which are essentially static schedules combined with an algorithm selector. while quantifying the performance of the various scheduling strategies we found out that dynamic schedules are only able to achieve rather minor im provements and that semistatic schedules work the best among these options. finally we compared two alternatives use the optimization component or use a fixed percentage of the allotted time when deciding how much time to allocate to the solver suggested by the algorithm selector. in either case we used a static schedule for the remaining time. this latter parametrization allowed us to avoid overfitting the training data and overall resulted in the best performance. we tested this approach on a highly diverse benchmark set with random crafted and industrial sat instances where we even deliberately removed entire families of instances from the training set. fixedsplit selection schedules demonstrated a convincing performance and solved on average over of the instances that the virtual best solver is able to solve. s o lv e r s e le ct io n f re q u e n cy selected solver significantly worse than vbs selected solver within of vbs or solves under sec fig. . frequency of solver selection by fixedsplit schedule. as an insight into the selection strategy of our fixedsplit selection schedule figure shows the fraction of test instances across all trainingtest splits on which any given solver was chosen and resulted in a successful run. the special bar labeled unsolved shows how often the portfolio made a choice that resulted in failing to solve an instance which here equals the gap to the vbs. note that out of the possible choices our portfolio chose only solvers in successful runs. further the black portion of the bars indicates how often was the selected solver nearly the best possible choice defined as the solver taking within of vbs time or solving the instance within seconds. the predominant black regions with the exception of clasp indicate that our portfolio often selected solvers with performance close to that of the vbs. table . comparison of major portfolios for the satrand benchmark test instances timeout seconds. values in braces denote pvalue of welchs ttest for the considered solver improving upon satzilla r as the baseline. satzilla r sathydra knn vbs solved . . unsolved solved . . . . . avg runtime . . par score . . as a final remark in table we close the loop and consider again the first benchmark set from section which compared portfolios for sat competitions random category instances based on the same solvers as the goldmedal winning satzilla r. overall we go up from . of vbs for satzilla r to . of vbs instances solved for our fixedsplit solver schedules. in other words fixedsplit selection schedule closes over of the performance gap between satzilla r and the vbs. the pvalues of welchs ttest being below . shown within braces indicate that the performance achieved by our fixed split selection schedule is statistically significantly better than satzilla r. references . g. audemard and l. simon. glucose a solver that predicts learnt clauses quality sat competition . . a. balint m. henn o. gableske hybridgm. solver description. sat competition . . a. biere. picosat version . solver description. sat competition . . a. biere. preicosatsc sat competition . . a. biere. lingeling sat race . . d. r. bregman. the sat solver mxc version . sat competition . . g.b. dantzig and p. wolfe. the decomposition algorithm for linear programs. econometrica . . g. dequen and o. dubois. kcnfs. solver description. sat competition . . m. gebser b. kaufmann and t. schaub. solution enumeration for projected boolean search problems cpaior . . p.c. gilmore and r.e. gomory. a linear programming approach to the cutting stock problem. operations research . . c.p. gomes and b. selman. algorithm portfolios. artificial intelligence . . y. hamadi s. jabbour and l. sais. lysat solver description sat competition . . g. hamerly and c. elkan. learning the k in kmeans. nips . . m. heule and h. van marren. march hi solver description sat competition . . m. heule and h. van marren. march nn httpwww.st.ewi.tudelft.nlsatdownload.php. . m. heule j. zwieten m. dufour h. maaren. march eq implementing additional reasoning into an efficient lookahead sat solver. theory and applications of satisfiability testing . . b. huberman r. lukose and t. hogg. an economics approach to hard computational prob lems. science . . f. hutter d.a.d. tompkins and h.h. hoos. scaling and probabilistic smoothing efficient dynamic local search for sat. cp . . a.r. khudabukhsh l. xu h.h. hoos k. leytonbrown. satenstein automatically building local search sat solvers from components. ijcai . . m.g. lagoudakis m.l. littman. learning to select branching rules in the dpll procedure for satisfiability. sat . . k. leytonbrown e. nudelman g. andrew j. mcfadden y. shoham. a portfolio approach to algorithm selection. ijcai . . c.m. li and w. wei. combining adaptive noise and promising decreasing variables in local search for sat. solver description. sat competition . . y. malitsky and a. sabharwal and h. samulowitz and m. sellmann. nonmodelbased algo rithm portfolios for sat. sat to be published . . m. nikolic f. maric p. janici. instance based selection of policies for sat solvers. theory and applications of satisfiability testing . . e. omahony e. hebrard a. holland c. nugent b. osullivan. using casebased reasoning in an algorithm portfolio for constraint solving. irish conference on artificial intelligence and cognitive science . . d.n. pham and c. gretton. gnovelty. solver description. sat competition . . d.n. pham and c. gretton. gnovelty v.. solver description. sat competition . . j. r. rice the algorithm selection problem. advances in computers . . sat competition. httpwww.satcomptition.org. . b. silverthorn and r. miikkulainen. latent class models for algorithm portfolio methods. aaai . . k.a. smithmiles. crossdisciplinary perspectives on metalearning for algorithm selection. acm comput. surv. . . m. soos cryptominisat ... solver description. sat race . . n. sorensson and n. een. minisat .. httpminisat.se . . m. streeter and d. golovin and s. f. smith. combining multiple heuristics online. aaai . . w. wei and c.m. li. switching between two adaptive noise mechanisms in local search for sat. solver description. sat competition . . w. wei c.m. li h. zhang. combining adaptive noise and promising decreasing variables in local search for sat. solver description. sat competition . . w. wei c.m. li h. zhang. deterministic and random selection of variables in local search for sat. solver description. sat competition . . l. xu h. h. hoos k. leytonbrown. hydra automatically configuring algorithms for portfoliobased selection. aaai . . l. xu f. hutter h.h. hoos k. leytonbrown. satzilla an automatic algorithm port folio for sat. solver description. sat competition . . l. xu f. hutter h.h. hoos k. leytonbrown. satzilla portfoliobased algorithm selection for sat. jair . . l. xu f. hutter h.h. hoos k. leytonbrown. satzilla the design and analysis of an algorithm portfolio for sat. cp .  an algorithm selection benchmark of the container premarshalling problem kevin tierney and yuri malitsky decision support operations research lab university of paderborn germany tierneydsor.de ibm t.j watson research center usa ymalitsus.ibm.com abstract. we present an algorithm selection benchmark based on op timal search algorithms for solving the container premarshalling prob lem cpmp an nphard problem from the field of container terminal optimization. novel features are introduced and then systematically ex panded through the recently proposed approach of latent feature analy sis. the cpmp benchmark is interesting as it involves a homogeneous set of parameterized algorithms that nonetheless result in a diverse range of performances. we present computational results using a stateofthe art portfolio technique thus providing a baseline for the benchmark. introduction the container premarshalling problem cpmp is a wellknown nphard prob lem in the container terminals and stacking literature first introduced in . the cpmp deals with the sorting of containers in a set of stacks called a bay of intermodal containers based on their exit times from the stacks such that containers that must leave the stacks first are placed on top of containers that must leave later. this prevents misoverlaid containers from blocking the timely exit of other containers. the goal of the cpmp is to find the minimal number of container movements necessary to ensure that all of the stacks are sorted by the exit time of each container without exceeding the maximum height of each stack. solving the cpmp assists container terminals in reducing delays and increasing the efficiency of their operations. a recent approach for solving the cpmp to optimality presents two state oftheart approaches based on a and ida. we use parameterized versions of these approaches to form a benchmark for algorithm selection. we introduce novel features to describe cpmp instances and show how the approach of latent feature analysis lfa can assist domain experts in developing useful features for algorithm selection approaches. finally we augment the existing cpmp instances with extra instances from a new instance generator. . an extended version of this paper is available at httpsbitbucket.orgeusorpb cpmpasdownloadsaslpmextended.pdf. kevin tierney and yuri malitsky a tier no. stack no. b c d fig. an example solution to the cpmp with misoverlays highlighted. re produced from . the container premarshalling problem given an initial layout of a bay with a fixed number of stacks and tiers stack height the goal of the cpmp is to find the minimal number of container move ments or rehandles necessary to eliminate all misoverlays in the bay. every container is assigned a group that indicates when it must leave the bay. a mis overlaid container is defined as a container with a group that is higher than the group of any container underneath it or a container above a misoverlaid container. consider the simple example of figure which shows a bay composed of three stacks of containers in which containers can be stacked at most four tiers high. each container is represented by a box with its corresponding group. this is not an ideal layout as the containers with groups and will need to be relocated in order to retrieve the containers with higher groups and . that is containers with groups and are misoverlaid. consider a container movement f t defining the relocation of the container on top of the stack f to the top position of the stack t. the containers in the initial layout of figure can reach the final layout d with three relocation moves reaching layout b reaching layout c and reaching layout d where no misoverlays occur. premarshalling is important both in terms of operational and tactical goals at a container terminal. in particular effective premarshalling of containers can help reduce delays moving containers from the terminal yard onto vessels as well as from the yard onto trucks or trains. we refer to for more information and a discussion of related work. we note that multiple containers may have the same group but in order to make containers easily identifiable in this example we have assigned a different group to each container. an algorithm selection benchmark for the cpmp latent feature analysis lfa given a set of solvers for a problem and a set of instance algorithm selection is the study of finding the best performing solver for each instance. there are a variety of approaches that can be used to make this decision including machine learning techniques as well as scheduling algorithms. for an overview of this area we refer the reader to a recent survey . although there are many algorithm selection approaches they are not the only important component in a selection approach. the quality of features in differentiating instances is critical to the success or failure of any algorithm selection strategy. features are normally created based on the knowledge of domain experts. in the authors theorize how latent features gathered from a matrix decom position can systematically help domain experts augment a set of features with more effective ones. specifically shows that latent features can be determined using an existing set of structural features. features that assist algorithm selec tion techniques in making correct predictions can then be identified thus guiding a domain expert towards the features that work best on his or her problem. the idea proposed by uses singular value decomposition svd to find the latent features that best describe the changes in the actual performance of solvers on instances. svd is a method for identifying and ordering the dimensions along which data points exhibit the most variation which is mathematically represented by the following equation m uv t where m is the m n matrix of solver performance. in our case we consider an m where there are m instances each described by the performance of n solvers. this means that the mn orthonormal columns of u can be interpreted as a latent feature that describes that instance. the columns of the v t matrix refer to each solver with each row presenting how active or important a particular feature is for that solver. if for a given instance it were possible to predict the latent features using this decomposition we could multiply the feature vectors by the existing and v t matrices to get back the performance of each solver. while this is of course impossible in practice we can use an existing set of structural features to pre dict these latent features. by then studying these predictions we can identify exactly which latent features are currently difficult to predict accurately and even identify which latent feature we should focus on getting right to maximize the quality of the resulting prediction. it is assumed that if we are unable to accurately predict a latent feature using our existing features then our feature set is missing something critical about the underlying structure of an instance. by computing the correct value for this latent feature and sorting all training instances based on it we assume that there must be something different for the instances where the latent feature value is large and those instances where the value is small. it is then up to a domain expert to try to analyze this difference and propose a new expanded set of features for the algorithm selection approach to take advantage of. kevin tierney and yuri malitsky . number of stacks . number of tiers . tiersstacks ratio . container density . empty stack percentage . percent of all slots stacks that are misoverlaid . bortfeldt forster lower bound . minmaxmeanstdev container group counts . minmaxmeanstdev group of top nonmisoverlaid container in each stack . container density in stacks through stacks . tierweighted groups . largest group l distance from top left average . pct. contiguous empty space including one empty stack . misoverlaid stack containers percentage . lowgroup containers near stack tops percentage fig. features for the cpmp. insts. source training set total bf cv testing set total expo. gen. bflike cvlike fig. instances used. algorithm selection benchmark we now describe our benchmark in detail. four optimal parameterizations of the a and ida approaches in form the basis of the benchmark. due to space limits we refer interested readers to for the algorithm and heuristic de tails. an interesting aspect of the premarshalling benchmark in relation to other benchmarks such as those based on sat csp qbf etc. is that the portfolio of algorithms is not particularly diverse very similar algorithm parameterizations but performance variations are nonetheless significant. we create a set of training and test instances out of existing premarshalling instances from bf and from cv as well as instances we generated. we filter out instances where all algorithms timeoutmemout or are too easy. an overview is provided in figure . our bfgenerated instances are not exactly the same as in because their instance generation is not completely described. the features used in our dataset are given in figure split into three cat egories. features through were designed before performing latent feature analysis. features through were created based on our first iteration of la tent feature analysis and features and using our second iteration. all of the features can be computed quickly. our feature generation code and instance generator is available at httpsbitbucket.orgeusorpbcpmpas. we note that other features are certainly possible such as probing features. original features are created in the standard way for algorithm selection benchmarks based on domain knowledge. the first features address the prob lem size and density of containers. feature counts the number of misoverlaid containers a naive lower bound to the problem whereas feature counts how many stacks contain misoverlaid containers. feature provides the lower bound from analyzing indirect container movements in addition to the misoverlays this benchmark is available in the algorithm selection library www.aslib.net un der the name premarshallingastar an algorithm selection benchmark for the cpmp solver avg. par solved bss . original features . lfa iteration features . lfa iteration features . vbs . . table performance of cshc trained on the three feature sets. present in feature . features through offer information on how many con tainers belong to each group. features through attempt to uncover the structure of the groups of the top nonmisoverlaid container on each stack. lfa features are constructed based on the suggestions of the latent features. feature is the density of containers on the left side of the instance. we note that this feature is likely overtuned to the algorithms in our benchmark. feature measures whether containers with high group values are on high or low tiers by multiplying the tier of a container by its group summing these values together and dividing by the maximum this value could take namely if the highest group container was in each slot. feature measures the l manhattan distance from the top left of a problem to each container in the latest exit time averaging these distances if there are multiple containers in the latest exit group. the final feature from iteration computes the percentage of empty space in the instance in which an area of contiguous empty space includes at least one empty stack. features and come from lfa iteration . feature counts how many stacks with more than two containers are misoverlaid and feature counts low max group valued containers on the top of stacks. computational results we evaluate our features using the costsensitive hierarchical clustering cshc approach from . table provides the performances of a cshc based portfolio when trained on the three datasets versus the best single solver bss and the virtual best solver vbs which is a portfolio that always picks the correct solver. cshc using just the initial arbitrary features already performs significantly better than the bss indicating even the original features have descriptive value. when a cshc portfolio is trained on the first iteration of features the per formance improves not only in the number of instances solved but also on the average time taken to solve each instance. this shows that by utilizing the latent feature analysis a researcher is able to develop a richer set of features to describe the instances. furthermore the process can be repeated as is evidenced by the performance of cshc on the second iteration of features. note that the over all performance is again improved not only in the number of instances solved all runtime data was generated on an amd opteron he processor running at . ghz with a one hour timeout. kevin tierney and yuri malitsky but the time taken to solve them on average. thus multiple iterations of the latent feature analysis process can lead to even better features although there are clearly diminishing returns. conclusion we presented an algorithm selection benchmark for the container premarshalling problem a wellknown problem from the container terminals literature. our benchmark includes novel features and instances. we further showed that latent feature analysis can help in augmenting problem features. we hope that this benchmark will help further algorithm selection research on realworld problems. for future work the latent feature analysis process could be more formalized. a number of open questions remain such as what criteria to use to gauge the per formance of a new feature during a single iteration of the latent feature analysis process. further challenges are to determine the number of iterations to perform and what kind of performancemanhour tradeoff exists for each iteration past the first. references . bortfeldt a. forster f. a tree search procedure for the container premarshalling problem. european journal of operational research . carlo h. vis i. roodbergen k. storage yard operations in container terminals literature overview trends and research directions. european journal of opera tional research . caserta m. vo s. a corridor methodbased algorithm for the premarshalling problem. in giacobini m. et al. ed. applications of evolutionary computing. lecture notes in computer science vol. pp. . springer . expositoizquierdo c. melianbatista b. morenovega m. premarshalling problem heuristic solution method and instances generator. expert systems with applications . kotthoff l. algorithm selection for combinatorial search problems a survey. ai magazine . lee y. hsu n. an optimization model for the container premarshalling problem. computers operations research . lehnfeld j. knust s. loading unloading and premarshalling of stacks in storage areas survey and classification. european journal of operational research . malitsky y. osullivan b. latent features for algorithm selection. symposium on combinatorial search . malitsky y. sabharwal a. samulowitz h. sellmann m. algorithm portfolios based on costsensitive hierarchical clustering. ijcai . stahlbock r. vo s. operations research at container terminals a literature update. or spectrum . tierney k. pacino d. vo s. solving the premarshalling problem to optimality with a and ida. tech. rep. wp dsor lab university of paderborn  boosting sequential solver portfolios knowledge sharing and accuracy prediction yuri malitsky ashish sabharwal horst samulowitz and meinolf sellmann cork constraint computation centre university college cork ireland ynmcs.brown.edu ibm watson research center yorktown heights ny usa ashish.sabharwalsamulowitzmeinolfus.ibm.com abstract. sequential algorithm portfolios for satisfiability testing sat such as satzilla and s have enjoyed much success in the last decade. by leveraging the differing strengths of individual sat solvers port folios employing older solvers have often fared as well or better than newly designed ones in several categories of the annual sat compe titions and races. we propose two simple yet powerful techniques to further boost the performance of sequential portfolios namely a generic way of knowledge sharing suitable for sequential sat solver schedules which is commonly employed in parallel sat solvers and a metalevel guardian classifier for judging whether to switch the main solver sug gested by the portfolio with a recourse action solver. with these addi tions we show that the performance of the sequential portfolio solver s which dominated other sequential categories but was ranked th in the application category of the sat competition can be boosted significantly bringing it just one instance short of matching the perfor mance of the winning application track solver while still outperforming all other solvers submitted to the crafted and random categories. introduction significant advances in solution techniques for propositional satisfiability test ing or sat in the past two decades have resulted in wide adoption of the sat technology for solving problems from a variety of fields such as design automa tion hardware and software verification cryptography electronic commerce ai planning and bioinformatics. this has also resulted in a wide array of challeng ing problem instances that continually keep pushing the design of better and faster sat solvers to the next level. the annual sat competitions and sat races have played a key role in this advancement posing as a challenge a set of socalled application category previously known as the industrial category instances along with equally but differently challenging crafted and random instances. given the large diversity in the characteristics of problems as well as spe cific instances one would like to solve by translation to sat it is no surprise that different sat solvers some of which were designed with a specific set of application domains in mind work better on different kinds of instances. algo rithm portfolios cf. attempt to leverage this diversity by employing several individual solvers and at runtime dynamically selecting what appears to be the most promising solver or a schedule of solvers for the given instance. this has allowed sequential sat portfolios such as satzilla and s to perform very well in the annual sat competitions and races. most of the stateoftheart sequential algorithm portfolios are based on two main components a a schedule of short running solvers to be run first in sequence for some small amount of time usual some fixed percentage of the total available time such as and b a long running solver to be executed for the remainder of the time which is selected by one or the other machine learning technique e.g. logistic regression nearest neighbor or decision forest cf. . if one of the short running solvers succeeds in solving the instance then the portfolio terminates successfully. however all work performed by each short running solver in this execution sequence is completely wasted unless it manages to fully solve the instance. if none of the short running solvers in the schedule succeeds all faith is put in the one long running solver. given this typical sequential portfolio setup it is natural to consider an extension that attempts to utilize information gained by short running solvers even if they all fail to solve the instance. further one may also consider an automated way to carefully revisit the choice of the long running solver whose improper selection may substantially harm the overall portfolio performance. we propose two relatively simple yet powerful techniques towards this end namely learnt clause forwarding and accuracy prediction. we remark that one limitation of current algorithm portfolios is that their performance can never be better than that of the oracle or virtual best solver which for each given instance magically selects an individual solver that will perform best on it. by sharing knowledge we allow portfolio solvers to in prin ciple go beyond vbs performance. specifically a distinguishing strength of our proposed clause forwarding scheme is that it enables the portfolio solver to po tentially succeed in solving an instance that no constituent sat solver can. learnt clause forwarding focuses on avoiding waste of effort by the short running solvers in the schedule. we propose to share or forward the knowl edge gained by the first k solvers in the form of a selection of short learned clauses which are passed on to the k st solver. conflictdirected clause learn ing cdcl is a very powerful technique in sat solving often regarded as the single most important element that allows these solvers to tackle reallife prob lems with millions of variables and constraints. forwarding learnt clauses is a cheap but promising way to share knowledge between solvers and is commonly employed in parallel sat solving. we demonstrate that sharing learnt clauses can improve performance in sequential sat solver portfolios as well. accuracy prediction and recourse aims to use metalevel learning to correct errors made by the portfolio solver when selecting the primary or long running solver. typically effective schedules allocate a fairly large fraction of the available runtime to one solver as not doing so would limit the bestcase performance of the portfolio to that of an oracle portfolio with a relatively short timeout. this of course poses a risk as a substantial amount of time is wasted if the portfolio selects the wrong primary solver. we present a scheme to gen erate large amounts of training data from existing solver performance data in order to create a machine learning model that aims to predict the accuracy of the portfolios primary solver selector. we call this metalevel classifier as a guardian classifier. we also use this training data to determine the most promising re course action i.e. which solver should replace the suggested primary solver. these techniques are general and may be applied to various portfolio algo rithms. however unlike the development of portfolio solvers that do not share information experimentation in our setting is much more cumbersome and time consuming. it involves modifying individual sat solvers and running the de signed portfolio solver on each test instance in real time rather than simply reading off performance numbers from a precomputed runtime matrix. we here demonstrate the effectiveness of our techniques using one base port folio solver namely s which had shown very good performance in sat com petition in the crafted and random categories but was ranked th in the application category. note also that the instances and solvers participating in the competition were designed with a second time limit in mind compared to instances and solvers in the challenge where the time limit was only seconds. our focus is on utilizing algorithm portfolios and our tech niques for solving hard instances. our results with a time limit roughly equiv alent to seconds of the competition machines show that applying these techniques can boost the performance of s on the competition instances to a point where it is only one instance short of matching the performance of the winning solver glucose . on the application track instances. moreover the resulting solver sfp continues to dominate all other solvers from the competition in the crafted and random categories in which s had excelled. we note that our portfolio solver built using these techniques called iss or industrial sat solver was declared the best interacting multiengine sat solver in the sat challenge a category that specifically compared port folios that explore various ways of sharing information among multiple sat engines. background we briefly review some essential concepts in constraint satisfaction sat and portfolio research. definition . given a boolean variable x true false we call x and x speak not x literals over x. given literals l . . . lk over boolean variables x . . . xn we call a la a clause over variables x . . . xn. given clauses c . . . cm over variables x . . . xn we call a ca a formula in conjunctive normal form cnf. definition . given boolean variables x . . . xn a valuation is an assign ment of values true or false to each variable x . . . xn true false. a literal x evaluates to true under iff x true otherwise it evaluates to false. a literal x evaluates to true under iff x false. a clause c evaluates to true under iff at least one of its literals evaluates to true. a formula evaluates to true under iff all its clauses evaluate to true. definition . the boolean satisfiability or sat problem is to determine whether for any given formula f in cnf there exists a valuation such that f evaluates to true. the sat problem has played a prominent role in theoretical computer science where it was the first to be proven to be nphard . at the same time it has driven research in combinatorial problem solving for decades. moreover the sat problem has great practical relevance in a variety of areas in particular in cryptography and in verification. . sat solvers while algorithmic approaches for sat have been developed as early as the be ginning of ai research a boost in sat solving performance has been achieved since the midnineties. problems with a couple of hundred boolean variables frequently posed a challenge back then. today many problems with hundreds of thousands of variables can be solved as a matter of course. while there exist very different algorithmic approaches to solving sat problems the performance of most systematic sat solvers i.e. those that can prove unsatisfiability is frequently attributed to three ingredients . randomized search decisions and systematically restarting search when it exceeds some dynamic fail limit . very fast inference engines which only consider clauses which may actually allow us to infer a new boolean variable for a variable and . conflict analysis and clause learning. the last point regards the idea of inferring new clauses during search that are redundant to the given formula but encode often in a succinct way the reason why a certain partial truth assignment cannot be extended to any solution. these redundant constraints strengthen our inference algorithm when a different partial valuation cannot be extended to a full valuation that satisfies the given formula for a similar reason. one of the ideas that we pursue in this paper is to inform a solver about the clauses learnt by another solver that was invoked previously to try and solve the same cnf formula. this technique is standard in parallel sat solving but surprisingly it has not been considered for solver portfolios before. . solver portfolios another important contribution was the inception of algorithm portfolios . based on the observation that solvers have complementary strengths and thus exhibit incomparable behavior on different problem instances the ideas of running multiple solvers in parallel or to select one solver based on the features of a given instance were introduced. portfolio research has led to a wealth of different approaches and an amazing boost in solver performance in the past decade . solver selection the challenge when devising a solver portfolio is to develop a learning algorithm that for a given set of training instances builds a dynamic mechanism that selects a good solver for any given sat instance. to this end we need a way to characterize a given sat instance which is achieved by computing socalled features. these could be e.g. the number of clauses or variables statistics over the number of negated over positive variables per clause or the clause over variable ratio. features can also include dynamic properties of the given instance obtained by running a solver for a very short period of time as a probe and collecting statistics. as the goal of this paper is to devise tech niques to improve existing portfolios a full understanding of instance features is unnecessary. we refer the reader to xu et al. for a comprehensive study of features suitable for sat. solver scheduling recent versions of satzilla and s no longer just choose one among the portfolios constituent solvers. while still selecting one long run ning primary solver they first schedule a sequence of several other solvers for a shorter amount of time. in particular s our base solver for experimentation employs a semistatic schedule of solvers given a test instance f and an total time limit t . it runs a static schedule independent of f based solely on prior knowledge from training data for an internal time limit t with t of t in which several different solvers with different short time limits are used. this is followed by a long running solver scheduled for time t t based on the features of f computed at runtime. we will refer to these two components of ss scheduling strategy as the preschedule and the primary solver. in this paper we tackle precisely these two aspects how can we improve the interplay between the shortrunning solvers in the preschedule while also passing knowledge on to the primary solver and how can we improve the selection of the longrunning primary solver itself. sharing knowledge among solvers a motivating factor behind the use of a preschedule used in sequential portfo lios is diversity. by employing very different search strategies one increases the likelihood of covering instances that may be challenging for some solvers and very easy for others. diversity has also been an important factor in the design of parallel sat solvers such as manysat and plingeling . when design ing these parallel solvers it has been observed that the overall performance can be improved by carefully sharing a limited amount of knowledge between the search efforts led by different threads. this knowledge sharing must be care fully done as it must balance usefulness of the information against the effort of communicating and incorporating it. one effective strategy has been to share information in the form of very short learned clauses often just unit clauses i.e. clauses with only one literal e.g. the winning parallel solver at the sat competition. . knowledge sharing among clauselearning systematic solvers in contrast current sequential portfolios while also relying on diversity through the use of a preschedule do not exploit any kind of knowledge sharing. if the first k solvers in the preschedule fail to solve the instance the time they spent is wasted. we propose to avoid this waste by employing the same technique that is used in parallel sat namely by forwarding a subset of the clauses learned by one solver in the preschedule to all solvers that follow it. in our implementation clause forwarding is parameterized by two positive integer parameters l and m . each clause forwarding solver outputs all learned clauses containing up to l literals. out of this list the m shortest ones or fewer if not enough such clauses are generated are forwarded to the next solver in the schedule which then treats these clauses as part of the input formula. while we solely base our choice on what clauses to forward on their lengths one could also consider more sophisticated measures e.g. . note that unlike clause sharing in todays parallel sat solvers in the sequential case clause forwarding incurs a relatively low communication overhead. nonetheless it needs to be balanced out with the potential benefits. we implemented clause forwarding in three conflict directed clause learning cdcl solvers henceforth referred to as the clause forwarding solvers. . impact of knowledge sharing on other solvers in addition to cdcl solvers preschedules typically also employ two other kinds of solvers incomplete local search solvers and lookahead based com plete solvers. the former usually perform very well on random and some crafted instances and the latter usually excel in the crafted category and sometimes on unsatisfiable random instances. since these solvers are not designed to generate or use conflict directed learned clauses it is not clear a priori whether such clauses which are redundant with respect to the underlying sat theory would help these two kinds of solvers as well. in our experiments we found it best to run these solvers before our clause forwarding solvers are used. the exceptions to this rule were two solvers march hi and mxcsat which showed a mixed impact of incorporating forwarded learned clauses. we thus chose to run them both before the forwarding solvers as in the base portfolio s table . gap closed to the virtual best solver vbs by using clause forwarding. average closed over s vbs gap . . . . and also after forwarding. our overall preschedule was composed of the origi nal one used by s in the sat competition scaled appropriately to take the difference in machine speeds into account enhanced with clause forwarding solvers and reordered to have nonforwarding cdcl solvers appear after the forwarding ones. we note that changing the preschedule itself did not signifi cantly alter the performance of s. e.g. in the application category as we will later see in table the performances of s with the original and the updated preschedules were very similar. . formula simplification one other consideration that has a significant impact in practice is whether to simplify the cnf formula before handing it to the next solver in the schedule after up to m forwarded clauses have been added to it. with some exper imentation we found that minimal simplification of the formula after adding forwarded clauses performed using satelite in our case was the most re warding. we thus used this clause forwarding setup for the experiments reported in this paper. . practical impact of clause forwarding we will demonstrate in the experiments section that clause forwarding allows s to close a significant part of the gap in performance when compared to the best solvers for application instances of the competition along with more in formation on the choice of trainingtest splits we consider and the experimental setup we use. we here provide an additional preview of the impact of clause for warding when using the latest sat solver available prior to the challenge. for this evaluation we consider three traintest splits of instances the first split uses the competition instances as test instances and every instance avail able before for training the second and third split are defined similarly but for the race and competition respectively. results are presented in table . here we consider the gap in performance be tween the portfolio without clause forwarding and the best possible noknowledge sharing portfolio vbs which uses an oracle to invoke the fastest solver for each given instance. in the table we show how much of that gap is closed by using clause forwarding. of course the portfolio that uses knowledge sharing between solvers is no longer limited in performance by the oracle portfolio as remarked earlier. however using the oracle portfolio gives us a good baseline to compare with. as we see clause forwarding significantly helps on all three competition splits clause. on average using this technique we are able to close over of the gap between the pure portfolio and the oracle portfolio. accuracy prediction and recourse studying the results of the sat competition one can observe that the best sequential portfolio s only solved out of instances in the application category. however when analyzing the performance of the solvers the s portfolio is composed of one can also see that the virtual best solver vbs based on those solvers can actually solve more than application instances. hence the suggestions made by the portfolio are clearly wrong in more than of all cases. the objective of this section is to lower this performance gap. in the following we first try to determine when the suggestion of a portfolio results in a loss in performance and second what to do when we believe the portfolios choice is wrong. . accuracy prediction one way to potentially improve performance would be to improve the portfolio selector itself e.g. by multiclass learning. nonetheless most classifiers often cannot represent exactly the concept class they are used for. one standard way out in machine learning is to conduct classification in multiple stages which is what we considere here. basic classifiers providing a confidence or trust value can function as their own guardian. in ensemble learning more complex recourse classifiers are considered. our goal here is to design such an approach in the specific context of machine learning methods for sat solver selection. we propose a twostage approach where we augment the existing sat port folio classifier by accompanying it with a guardian classifier which aims to predict when the first classifier errs and a second selector classifier that se lects an alternative solver whenever the guardian finds that the first selector is probably not right. to train a guardian and a replacement selector classifier we first need to capture some characteristics that correlate with the quality of the decision of the portfolio. to that end we propose to create a set of features and label the portfolios suggestion as good or bad l goodbad. a key question is how should these two labels be defined. inspired by the sat competition context a good decision will be defined as one where an instance can be solved within the given time limit and a bad one is when it cannot be. we also tried labels that identify top performer e.g. not more than x slower than the best solver for various x but obtained much worse results. the issue here is that it is more ambitious than necessary to predict which solver is best or close to best. instead we need to be able to distinguish solvers that are good enough from those that fail. that is rather than aiming for speed we optimize for solver robustness. the definition of a feature vector f to use for a guardian classifier is unfor tunately far less straightforward. we of course first tried the original features used by s but that did not result in an overall improvement in performance. as is typically done in machine learning we experimented with a few additions and variations and settled on the following list of employed features f distance to closest cluster center f k used for test instance ff minmaxaveragemedianvariance of distance to closest cluster center f solver id selected by k nn f solver type incomplete or complete f average distance to solved instances by top solvers f vbs time on k neighborhood of test instance f number of instances solved by top ranked solvers ff par scoreinstances solved by top ranked solvers ff test instance features table . description of features used by guardian classifier. solver rank is based on average par score on neighborhood. we selected features composed of the first features of the test instance the euclidean based distance measures of training instances in the neighborhood to the test instance and runtime measures of the five best solvers on a restricted neighborhood see table . for details. these features are inspired by the k nearestneighbor classifier that s employs. for other approaches like the voting mechanism in one can also craft features e.g. number of votes for a solver. consequently for the guardian we need to learn a classifier function f l. to this end we require training data. the s portfolio is based on data t that is composed of features of and runtimes on sat instances appearing in earlier competitions. we can split t into a training set ttrain and test set ttest. now we can run the portfolio restricting its knowledge base to ttrain and test its performance on ttest. for each test instance i ttest we can compute the corresponding feature vector fi and obtain the label l. hence the number of training instances we obtain for the classifier is i. obviously one can split t differently over and over by random subsampling and each time one creates new training data to train the guardian classifier. the question arises whether different splits will not merely regenerate exist ing knowledge. this depends on the features chosen but here the feature vector will actually have a high probability to be different for each single split since in each split the neighborhood of a test instance will be different. a thought exper iment that makes this more apparent is the following assume that for a single instance i we sort all other instances according to the distance to i neighbor hood of i. assume further we select training instances from the neighborhood of i with probability k until we have selected k instances where k is the desired neighborhood size. when k it is obviously very unlikely for an instance to have exactly the same neighbors. in order to determine an appropriate amount of training data we first ran domly split the data set t in a training split ttrain and test split ttest before generating the data for the classifier. we then perform the aforementioned split ting to generate training data for the classifier on ttrain and test it on the data generated by running knn with data ttrain on the test set ttest . we use different random splits of type ttrain and ttest and try to determine the best number of splits for generating training data for the classifier. while normally one could essentially look at the plain accuracy of the clas sifier and select the number of splits that result in the highest accuracy we propose to employ another measure based on the following reasoning. the clas sifiers confusion matrix looks in our context like this denoting the solver that was selected by the portfolio on instance i with s a s solves i and classifier predicts that it can b s solves i but classifier predicts that it cannot c s cant solve i but classifier predicts that it can d s cant solve i and classifier predicts that it cannot instances that fall in category a reflect a good choice by the portfolio our original selector and while correctly detected there is also nothing for us to gain. in case c we cannot exploit the wrong choice of the portfolio since the guardian classifier does not detect it. however we will also not degenerate the performance of the portfolio. case b and d are the interesting cases. in b we collect the falsepositives where the classifier predicts that the portfolios choice was wrong while it was not. consequently it could be the case that we degrade the performance of the original portfolio selector by altering its decision. all instances falling in category d represent the correctly labeled decisions of the primary selector that should be overturned. in d lies the potential of our method all instances that fall in this category cannot be solved by solver s that the porimary selector chose and the guardian classifier correctly detected it. since cases a and c are somewhat irrelevant to any potential recourse action we focus on keeping the ratio bd as small as possible in order to favorably balance potential losses and wins. based on this quality measure we determined that roughly splits achieve the most favorable trade off on our data. . recourse when the guardian classifier triggers we need to select an alternative solver. for this purpose we need to devise a second recourse classifier. while we clearly do not want to select the same solver that was suggested by the original portfolio selector the choices for possible recourse actions is vast and their benefits hardly apparent. we introduce the following recourse strategy since we want to replace the suggested solver s we assume s is not suit able for the given test instance i. based on this conditional probability we can also infer that the instances solved by s in the neighborhood of size k of i can be removed from its neighborhood. now it can be the case that the entire neighborhood of i can be solved by s and therefore we extend the size of the neighborhood by . if on this extended neighborhood s cannot solve all in stances we choose the solver with the lowest parscore on the instances in the extended neighborhood not solved by s. otherwise we choose the solver with the second best ranking by the original portfolio selector. in the context of s this is the solver that has the second lowest parscore on the neighborhood of the test instance. designing a good recourse strategy poses a challenge. as we will see later in section . our proposed recourse strategy resulted in solving instances on the sat competition application benchmark compared to the that s solved. we tried a few other simpler strategies as well which did not fare as well. we briefly mention them here first we used the solver that has the second best ranking in terms of the original classifier. for s this means choosing the solver with the second lowest parscore on the neighborhood of the test instance. this showed only a marginal improvement solving instances. we then tried to leverage diversity by mixingandmatching the two recourse strate gies mentioned above giving each exactly half the remaining time. this resulted in overall performance to drop below s without accuracy prediction. finally we computed offline a static replacement map that for each solver s specifies one fixed solver fs that works the best across all training data whenever s is selected by the original classifier but does not solve the instance. this static featureindependent strategy also resulted in degrading performance. for the rest of this paper we will not consider these alternative replacement strategies. empirical evaluation in order to evaluate the impact of our two proposed techniques on an existing portfolio solver we applied them to s the best performing sequential port folio solver at the sat competition. we refer to the resulting enhanced portfolio solver as sf when clause forwarding is used as sp when accuracy prediction and recourse classifiers are used and as sfp when both new tech niques are applied. we compare their performance to the original s which was the winner in the crafted and random categories of the main sequential track of the sat competition. as remarked earlier our techniques are by no means limited to s and may be applied to more recent portfolios. however these techniques are likely to pay off more on harder instances and thus we focus here on the competition in which both instance selection and solver design was done with a seconds time limit in mind. for evaluation we use the competition split i.e. we use the same ap plication crafted and random category instances as the ones the source code of s can be obtained from httpwww.satcompetition.org used in the main phase of the competition. the enhanced variants of s rely only on the pre training data that comes with the original s. we note that we did conduct experiments using random splits after mixing all instances but there the performance of the original knn classifier of s is typically almost perfect leaving little to no room for improvement. competition splits exhibit a completely different and perhaps arguably more realistic behavior as the sub optimal performance of s in the application category shows. we thus focused on splits that were neither random nor handcrafted by us and experimented on competition splits to evaluate the techniques. all experiments were conducted on . ghz amd opteron machines with core cpus and gb memory running scientific linux release .. we used a time limit of sec which roughly matched the sec timeout that was used on the competition machines. as performance measures we consider the number of instances solved average runtime and par score. par stands for penalized average runtime where instances that time out are penalized with times the timeout. . implementation details on clause forwarding we implemented learned clause forwarding in three cdcl sat solvers that were used by s in the competition cryptominisat .. glucose . and minisat .. . the preschedule was modified to prolong the time these three clauselearning solvers are run as discussed earlier. with clause forwarding disabled s with this modified preschedule resulted in roughly the same performance on our testbed as s with the original preschedule used in the competition henceforth referred to as sc. in other words any performance differences we observe can be attributed to clause forwarding and accuracy pre diction and recourse not to the change in the preschedule itself. for clause forwarding we used parameter values l and m i.e. each of the three solvers may share up to clauses of size up to for the next solver to be run. the maximum amount of clauses shared is therefore . we note that these parameters are by no means optimized. amongst other variations we tried sharing an unlimited number of small clauses but this unsurprisingly degraded performance. we expect that these parameters can be tuned better and that the selection of what clauses to forward can be improved e.g. based on the lbd measure introduced in . nevertheless the above choices worked well enough to demonstrate the benefits of clause sharing which is the main purpose of this experimentation. . implementation details on accuracy prediction to predict how good the primary solver suggested by s is likely to be we experimented with several classifiers available in the weka .. data mining and machine learning java library . the results presented here are for the rep tree classifier which is a fast decision tree learner that uses information gain and httpwww.cs.waikato.ac.nzmlweka variance reduction for building the tree and applies reducederror pruning. using training data based on splitting pre instances the ones s is based on times as described earlier we trained a reptree and obtained the following confusion matrix for instances of the sat competition application test data a b c d hence the best possible outcome for a recourse action would be to solve the previously unsolved instances of all the application instances under c and to still be able to solve the instances under b. while in the best case we could gain instances and lose none it is obviously not clear whether one would achieve any gain at all or even solve at least the instances that originally used to be solved. fortunately with our recourse strategy we witness a significant gain in overall performance. we integrated the classifier in s in the following way when s suggests the primary solver if indicated by our guardian reptree model we intercept its decision and alter it as proposed by our recourse strategy. . results on sat competition data since our base portfolio solver s already works best on random on crafted instances considered the objective is to close the large gap between the best sequential portfolios and the best individual solvers in the application track while not degrading the performance of the portfolio on crafted and random categories. to this end let us first note that adding the methods proposed in this paper have no significant impact on s performance on random and crafted instances. on random instances knowledge sharing hardly takes place since cdcl based complete solvers are barely able to learn any short clauses on these instances. for crafted instances a limited amount of clause forwarding does happen but much less so than in application instances. in figure we show how many instances in out test set share how many clauses. on the left we see that on crafted instances we mostly share a modest amount of clauses between solvers if any. the plot on the right shows the situation for application instances. here it is usually the case that the solvers share the fully allowed clauses. interestingly the clause sharing in crafted instance causes a slight decrease in performance but this is outweighed by the positive impact of our prediction and recourse classifiers which actually improve the performance of the solver presented here over s on crafted instances. in summary the solver presented here works as well as s on random instances and insignificantly better than s on crafted instances. note that the numbers do not add up to since with the classifier we only consider instances that have not been solved yet by the prescheduler and can be solved by at least one of the solvers in our portfolio. histogram crafted instances number of forwarded clauses n um be r of in st an ce s histogram application instances number of forwarded clauses n um be r of in st an ce s fig. . histogram showing how often n clauses are forwarded. left crafted instances. right application instances. table . performance comparison of sc from the competition and its four new variants s sf sp and sfp on application. application sc s sf sp sfp solved unsolved solved . . . . . avg runtime par score it remains to test if the methods proposed here can boost s performance to a point where it is competitive on application instances as well. the first column of table shows the performance of s in the version avail able from the competition website sc. the number of solved instances for sc differs slightly from the competition results due to hardware and experi mental differences. comparing sc with s where we changed the preschedule to allow more clauses to be learned we observe that the difference is very small. sc solves just instance more than s letting us conclude that the subsequently reported performance gains are not due to differences in the pre schedule itself. both sf s with clause forwarding and sp s with prediction and recourse are able to improve on s in a significant fashion sf is able to solve more instances and sp solves more. note that in the application category it is usually the case that the winning solver only solves a couple of more instances than the secondranked solver. indeed the difference between the th ranked s in the competition and the winner was only instances. that is to say prediction and recourse closes of the gap to the winning solver and clause forwarding even over . the combination of clause forwarding and prediction and recourse in sfp is able to solve instances. this is just one instance shy of the best sequen tial solver glucose . at the sat competition which we reran on our hardware using the same experimental settings. note that s uses only pre solvers. furthermore we found that the average runtime of sfp is close to glucose . as well which also indicates that in contexts where objectives other than the number of solved instances are of interest sfp is very competitive. conclusion we presented two novel generic techniques for boosting the performance of sat portfolios. the first approach shares the knowledge discovered by sat solvers that run in sequence while the second improves solver selection accuracy by detecting when a selection is likely to be inferior and proposing a more promis ing recourse selection. applying these generic techniques to the sat portfolio s resulted in significantly better performance on application instances while not reducing performance on crafted and random categories making the result ing solver sfp excel on all categories in our evaluation using the sat competition data and solvers. references g. audemard and l. simon. predicting learnt clauses quality in modern sat solvers. in st ijcai pp. pasadena ca july . a. biere. plingeling solver description . sat race. s. a. cook. the complexity of theoremproving procedures. in stoc pp. . acm . c. p. gomes and b. selman. algorithm portfolios. ai j. . m. hall e. frank g. holmes b. pfahringer p. reutemann and i. h. witten. the weka data mining software an update. sigkdd explorations . y. hamadi and l. sais. manysat a parallel sat solver. jsat . j.r.rice. the algorithm selection problem. advances in computers . s. kadioglu y. malitsky a. sabharwal h. samulowitz and m. sellmann. al gorithm selection and scheduling. in th cp vol. of lncs pp. perugia italy sept. . k. leytonbrown e. nudelman g. andrew j. mcfadden and y. shoham. a portfolio approach to algorithm selection. in ijcai pp. . y. malitsky a. sabharwal h. samulowitz and m. sellmann. nonmodelbased algorithm portfolios for sat. in th sat vol. of lncs pp. ann arbor mi june . k. a. smithmiles. crossdisciplinary perspectives on metalearning for algorithm selection. acm comput. surv. . m. soos. cryptominisat .. . httpwww.msoos.orgcryptominisat. n. sorensson and n. een. satelite . . httpminisat.se. n. sorensson and n. een. minisat .. . httpminisat.se. l. xu f. hutter h. h. hoos and k. leytonbrown. satzilla portfoliobased algorithm selection for sat. jair . l. xu f. hutter h. h. hoos and k. leytonbrown. evaluating component solver contributions to portfoliobased algorithm selectors. in sat .  ar x iv . v cs .a i ju l dash dynamic approach for switching heuristics giovanni di liberto serdar kadioglu kevin leo yuri malitsky cork constraint computation centre university college cork ireland dilibertdei.unipd.it y.malitskyumail.ucc.ie oracle united states serdrkgmail.com faculty of it monash university australia kevin.leomonash.edu abstract. complete tree search is a highly effective method for tack ling mip problems and over the years a plethora of branching heuristics have been introduced to further refine the technique for varying prob lems. recently portfolio algorithms have taken the process a step further trying to predict the best heuristic for each instance at hand. however the motivation behind algorithm selection can be taken further still and used to dynamically choose the most appropriate algorithm for each en countered subproblem. in this paper we identify a feature space that captures both the evolution of the problem in the branching tree and the similarity among subproblems of instances from the same mip mod els. we show how to exploit these features to decide the best time to switch the branching heuristic and then show how such a system can be trained efficiently. experiments on a highly heterogeneous collection of mip instances show significant gains over the pure algorithm selection approach that for a given instance uses only a single heuristic throughout the search. mixed integer programming mip is a powerful problem representation that is ubiquitous in the modern world. the problem is represented as the maximiza tion of an objective function while maintaining the specified linear inequalities and restricting some variables to only take integer values while others are allowed to take on any real value. maximize ctx subject to ax b l x u xj integer j d where d ..n through this straight forward formulation it is possible to define a wide variety of problems ranging from scheduling to production planning to network design to auctions and many others. in practice these problems are typically approached using branch and bound or branch and cut techniques . here the main idea is to perform deterministic httparxiv.orgabs.v and inductive reasoning to lower the domains of the variables. when this is no longer possible a variable is selected and assigned a value based on some guiding heuristic. once such a decision is made the search proceeds to function deterministically. if or when it is later found that the decision led to an infeasible or suboptimal solution the search backtracks returning to the parent node to try an alternate assignment. the key behind the success or failure of this complete search approach is the order in which the variables are selected and the order the values are assigned to them. choosing certain variables can significantly reduce the domains of all other variables allowing the deterministic analysis to quickly find a contradiction or determine that no improving solution can exist in the subproblem. alternatively choosing the wrong variables can lead to exponentially longer run times. due to the critical importance of the selection of the branching variable and value there have been a number of heuristics presented . several of them are based on simple rules eg. mostleast infeasible branching base their deci sions on the variables fractionality. other heuristics like pseudocost branching can adapt over time while others like strong branching test which of the frac tional candidates gives the best progress before actually committing to any of them. finally there are also hybrid techniques eg. reliability branching that put together the positive aspects of strong branching and pseudocost branch ing. a good overview of these and other heuristics can be found in . the efficiency of the search however can be much improved if we could use the correct heuristic at the correct time in the search. work with portfolios has already shown that there is often no single solver or approach that works opti mally on every instance . we also know that throughout the branching process as certain variables get assigned and the domains of others are changed the underlying structure of the subproblems changes. in this paper we show how to identify changes in the problem structure and therefore how to make a decision of when it is the best time to switch the employed guiding heuristic. while a similar approach was recently introduced in this work expands the research from the set partitioning problem with problem dependent heuris tics to the much more general problem of mip. we also provide a detailed analysis of how the problem structure changes over time and clearly demon strate the effectiveness of the proposed approach on real benchmarks that are of interest to the community. dynamic switching the objective motivating this work is to create a solver that dynamically adjusts its search strategy selecting the most appropriate heuristic for the subproblem at hand. in a highlevel overview we want the solver to perform a standard branch and bound procedure but before choosing the next branching variable and value it will analyze the structure of the current subproblem using a set of representative features. using this structural information the solver would be able to predict that a specific heuristic is likely better than the alternatives and algorithm dash branch callback procedure branchcallbacksubproblem parent centers heuristics if depth maxdepth and depth interval then x featurescomputationsubproblem for all center c in cs do distancei euclideandistancex centers end for cluster argmindistance heuristic heuristicscluster else heuristic parent.heuristic end if executebranchingsubproblemheuristic end procedure employ it to make the next decision. we refer to such a strategy as a dynamic approach for switching heuristics dash. the specifics of dash are described in algorithm . modeled after the isac approach dash assumes that instances that have similar features share the same structure and so will yield to the same algorithm. we will therefore employ clustering to identify these groups of instances. dash is provided the current subproblem the heuristic employed by the parent node the centers of the known clusters and the list of available heuristics. because determining the feature can be computationally expensive and because switching heuristics at lower depths of the search tree has a smaller impact on the quality of the search dash only chooses to switch the guiding heuristic up to a certain depth and only at predetermined intervals choosing the parents heuristic in all other cases. when a decision does need to be made the approach computes the features of the provided subproblem and determines the nearest cluster based on the euclidean distance. in theory any distance metric can be used here but in practice we found that euclidean works well in the general case. in the end dash employs the heuristic that has been determined best for that cluster. as can be inferred from this algorithm the key component that determines the success or failure of dash is the correct assignment of heuristic to cluster. to train this we follow a similar procedure first described in . for each instance in the training set we compute an assortment of subproblems that are observed when using each of our heuristics. this extended problem set allows us to get a better overview of the type of subproblems dash will be encountering as opposed to just using the original training instances. computing the features of the extended problem set we cluster the instances. for this we employ g means a general clustering approach that automatically determines the best number of clusters for the dataset in question. in particular this clustering approach assumes that a good cluster is one that has a gaussian distribution around the cluster center. starting with all instances being in a single cluster gmeans iteratively calls means to split a cluster in two. if the new clusters are more gaussian than the original the split is accepted and the procedure continues. once all the instances are clustered the clusters with fewer instances than a certain threshold are absorbed by the nearest clusters. once all the subproblems are clustered we have to determine which heuris tic is best in which scenario. however an important caveat to this is that the decision of a using a heuristic for a certain cluster also affects all other deci sions. this is because dash can switch heuristics several times and the types of subproblems observed after applying one heuristic will likely be different then when another one has been applied. therefore we employ the parameter tuner gga to simultaneously assign heuristics to all clusters using only the orig inal instances for training. experimental setup in order to set the stage for dash three things are necessary. first we must have a descriptive feature set that can correctly distinguish between different classes of instances but also do this with minimal overhead. second there must be a diverse set of heuristics each of which performs well on different kinds of instances. finally there must be a heterogeneous domain with a large number of benchmark instances. we touch on all three of these components in this section. we implement our feature computation and heuristics through extending the stateoftheart mip solver cplex version . . here we only modify the built in branching strategy by implementing a branch callback function based on algorithm . because all the tested approaches require this branch callback to be enabled the comparability of the results is guaranteed. finally in order to obtain reliable results we run each cplex execution in the single core version. the experiments were run on dual intel xeon e quadcore processors .ghz computers with gb of ddr fbdimm mhz memory. . feature space the features have to capture as many aspects of the problems as possible without becoming too expensive to compute. to do this we gather statistics about the problem definition of the remaining subproblem a process similar to the one employed in . specifically we compute percentage of variables in the subproblem percentage of variables in the objective function of the subproblem percentage of equality and inequality constraints statistics min max avg std of how many variables are in each constraint statistics of the number of constraints in which each variable is used depth in the branch and bound tree. wherever a feature has to do with the problem variables we separately com pute the same feature for each type of variable type eg. continuous integer and binary. therefore the resulting set is composed of features. note that cplex switches off certain heuristics as soon as branch callbacks even empty ones are being used so that the entire search behavior could be different. . branching heuristics in order to realize and test our solving approach we implemented a portfolio of six branching heuristics. most fractional rounding mf one of the simplest mip branching tech niques is to select the variable that has a relaxed lp solution whose fractional part is most fractional and to round it first. the driving reasoning behind this is to make decisions on variables that deterministic analysis is least certain about. therefore this heuristic strives to find infeasible solutions as quickly as possible. less fractional rounding lf alternatively to mf this technique selects the the variable that has a relaxed lp solution whose fractional part is closest to an integer value and to round it first. this is done to gently nudge the deter ministic reasoning in whatever direction it is currently pursuing with a smallest chance of making a mistake. less fractional and highest objective rounding lfho this heuris tic is based on the same motivation behind the less fractional branching. for each subproblem we branch on the variable for which the pair pfr obj is min imized where fr is the fractionality and obj is the objective value. this means that if we branch on a variable k in n the following propriety is guaranteed i n frk fri or objk obji most fractional and highest objective rounding mfho we use a modification of the previous approach but this time we focus on the most fractional variables. for each subproblem we branch on the variable for which the pair pfr obj is maximized. in this case the guaranteed property is i n frk fri or objk obji pseudocost branching weigthed score pw this heuristic is based on the pseudocosts numerical values that estimates the variation in objective value for rounding up or rounding down called respectively uppseudocost and down pseudocost. the pseudocosts of a variable can be combined in a score function . that returns a numeric value. this result is used to guide the branching for which we choose the variable that maximize the score. further details can be found in . scoreq q minq q maxq q . pseudocost branching product score p this approach is based on the same idea as pw. the difference lies in the score function that is now the product of the two pseudocosts. cluster cluster cluster cluster cluster table instance distribution percentage in the clusterization at the root node. the problem types are airland fc genassignment lotsizing mik miplib nexp pmedcap region region scp sscflp . dataset in order to obtain a solver that works well for a generic mip problem we collected instances from many different datasets miplib fc lotsizing mik nexp region and pmedcapv airland genassignment scp ss cflp were originally downloaded from . from an initial dataset of about instances we filtered those for which all our solvers timed out in seconds. we then removed the easy instances solved entirely during the cplex presolving or in less than one second by each solver. we finally obtained a dataset of instances with the desired properties. we randomly selected for the training set and for the testing set. if we cluster our training data the distribution of instances per cluster can be seen in table . each row is normalized to sum unto . thus for cluster of the instances are from the airland dataset. from this table we first observe that there are not enough clusters to perfectly separate the different datasets into unique clusters. this however is not what we would want to see. this is because we are more interested in capturing similarities between instances not splitting benchmarks. and we observe that the region and region instances are grouped together. we also see that cluster logically groups the lotsizing and the sscflp instances together. finally we see that the instances from the miplib those instances that are supposed to be an overview of all problem types are spread across all clusters. this clustering therefore demonstrates that we both have a diverse set of instances and that our features are representative enough to automatically notice interesting groupings. numerical results with the described methodology the main question that needs to be addressed is whether switching heuristics can indeed be beneficial to the performance of the solver. to test this for each of the instances in our test set we ran each of the implemented heuristics without allowing any switching. we then also ran two versions of a solver that switched between heuristics uniformly at random. the first solver switched between all heuristics while the second switched only among the top four best heuristics. the results are summarized in table . solver avg par solved bss . rand . rand . vbs . vbs rand table solving times on the testing set. . . . . . . . pc p c dash solving time pca depth solver lfho mfho lf mf pw p a multipath evolution . . . . . . . pc p c dash solving time pca depth solver lfho mfho lf mf pw p b singlepath evolution fig. position of a subproblem in the feature space based on depth. what we observe is that neither of the random switching heuristics perform very well by themselves. however based on the performance of the virtual best solver that employs these new solvers the performance can be further improved beyond what is possible when always sticking to the same heuristic. the question therefore now becomes if we can get improved performance just by switching between heuristics randomly can we do even better if we do so intelligently to answer this question we must first set a few parameters of our solver. particularly till what depth should we allow our solver to switch heuristics and at what interval for this we cluster the extended dataset that includes both the original training instances and the possible observed subproblems. there are a total of clusters formed. projecting the feature space into two dimensions using principal component analysis pca we present figure . here the cluster boundaries are represented by the solid lines and the best heuristic for each cluster is represented by a unique symbol at its center. on these figures we also show the typical way in which features change as the problem is solved with a particular heuristic. the nodes are colored based on the depth of the tree with a showing all the observed subproblems and b that of a single branch. vbs is an oracle solver that for every instance always uses the strategy that results in the shortest runtime solver avg par solved bss . isac . isac filt . dash . dash . dashfilt . vbs . vbs dash . table solving times on the testing set. what this figure shows is that the features change gradually. this means that there is no need checking the features at every decision node. we therefore choose to check the subproblem features at every rd node. similarly the figure and those like it show that using a depth of is reasonable as in most cases the nodes dont span across more than two clusters. we use gga to tune the parameters of dash computing the best heuristic for each cluster. we then present the results in table where we compare it to a vanilla isac approach that for a given instance chooses the single best heuristic and then does not allow any switching. what we observe is that dash is able to perform much better than its more rigid counterpart. however we do allow for the possibility that switching heuristics might not be the best strategy for every instance. we therefore also introduce dash which first clusters the original instances using isac and then allows each cluster to independently decide if it wants to use dynamic heuristic switching. taking a lesson from which shows that often the features are not equally important we tried to achieve better overall performance including a feature se lection operation. in this paper we utilize the information gain filtering technique often used in decision trees. in particular this method is based on the calcula tion of entropy of the data as a whole and for each class. we apply the feature filtering to isac and dash referring to them respectively as isac filt and dashfilt having an improvement in both cases. in particular the resulting solver dashfilt performs considerably better than everything else. we finally show the performance of a virtual best solver if allowed to use dash. and what we observe is that even though the current implementation cannot overtake vbs future refinements to the portfolio techniques will be able to achieve performances much better than techniques that rely purely on sticking to a single heuristic. conclusion in this paper we introduce a dynamic approach for switching heuristics dash. using mip as the running example we show how to automatically determine when a subproblem observed during a branch and bound search is significantly different from what has been observed before and therefore warrants a change of tactics used while solving it. employing a diverse set of instances we demon strate that significant performance improvements are possible if a solver does not stick to using a single guiding heuristic. references . kadioglu s. malitsky y. sabharwal a. samulowitz h. sellmann m. algo rithm selection and scheduling. cp . wolsey laurence a. p.y. production planning by mixed integer programming. springer berlin . a. balakrishnan t.m. mirchandani p. network design. in annotated bibli ographies in combinatorial optimization. wiley new york . zurel e. nisan n. an efficient approximate allocation algorithm for combinato rial auctions. . mitten l. branchandbound methods general formulation and properties. op erations research . padberg m. rinaldi g. a branchandcut algorithm for the resolution of large scale traveling salesman problems. siam review . achterberg t. koch t. martin a. branching rules revisited. operations re search letters . linderoth j.t. savelsbergh m.w.p. a computational study of search strategies for mixed integer programming. informs journal on computing . achterberg t. constraint integer programming. phd thesis technische univer sitat berlin . kadioglu s. malitsky y. sellmann m. tierney k. isac instancespecific algo rithm configuration. ecai . omahony e. hebrard e. holland a. nugent c. osullivan b. using case based reasoning in an algorithm portfolio for constraint solving. aics . xu l. hutter f. shen j. hoos h.h. leytonbrown k. satzilla improved algorithm selection based on costsensitive classification models sat com petition. . kadioglu s. malitsky y. sellmann m. nonmodelbased search guidance for set partitioning problems. aaai . hamerly g. elkan c. learning the k in kmeans. nips . ansotegui c. sellmann m. tierney k. a genderbased genetic algorithm for the automatic configuration of algorithms. cp . ibm ibm cplex v.. httpwww.software.ibm.comwebappdownloadpreconfig.jspid . koch t. achterberg t. andersen e. bastert o. berthold t. bixby r.e. danna e. gamrath g. gleixner a.m. heinz s. lodi a. mittelmann h. ralphs t. salvagnin d. steffy d.e. wolter k. miplib mixed inte ger programming library version . mathematical programming computation . atamturk a. flow pack facets of the single node fixedcharge flow polytope. operations research letters . atamturk a. oz j.c.m. a study of the lotsizing polytope. mathematical pro gramming . atamturk a. on the facets of the mixedinteger knapsack polyhedron. mathe matical programming . atamturk a. nemhauser g.l. savelsbergh m.w.p. valid inequalities for problems with additive variable upper bounds. mathematical programming httpwww.software.ibm.comwebappdownload preconfig.jspidaa.rstactscmp . leytonbrown k. pearson m. shoham y. towards a universal test suite for combinatorial auction algorithms. acm conference on electronic commerce ec . saxena a. mip benchmark instances. httpwww.andrew.cmu.eduuseranureetsmpsinstances.htm . abdi h. williams l.j. principal component analysis . kroer c. malitsky y. feature filtering for instancespecific algorithm configura tion. ictai httpwww.andrew.cmu.edu useranureetsmpsinstances.htm dash dynamic approach for switching heuristics giovanni di liberto serdar kadioglu kevin leo yuri malitsky  deep learning for algorithm portfolios deep learning for algorithm portfolios andrea loreggia university of padova ibm research ny u.s.a. andrea.loreggiagmail.com yuri malitsky ibm research ny u.s.a. yuri.malitskygmail.com horst samulowitz ibm research ny u.s.a. samulowitzus.ibm.com vijay saraswat ibm research ny u.s.a. vijaysaraswat.org abstract it is well established that in many scenarios there is no single solver that will provide optimal performance across a wide range of problem instances. taking ad vantage of this observation research into algorithm se lection is designed to help identify the best approach for each problem at hand. this segregation is usually based on carefully constructed features designed to quickly present the overall structure of the instance as a constant size numeric vector. based on these features a plethora of machine learning techniques can be utilized to pre dict the appropriate solver to execute leading to sig nificant improvements over relying solely on any one solver. however being manually constructed the cre ation of good features is an arduous task requiring a great deal of knowledge of the problem domain of in terest. to alleviate this costly yet crucial step this paper presents an automated methodology for producing an informative set of features utilizing a deep neural net work. we show that the presented approach completely automates the algorithm selection pipeline and is able to achieve significantly better performance than a sin gle best solver across multiple problem domains. introduction over the last decade it has become an accepted fact that there is often no single approach that will dominate across a wide range of problem instances. techniques like algo rithm configuration hutter hoos and leytonbrown ansotegui sellmann and tierney fitzgerald malit sky and osullivan can certainly improve the aver age performance of any parameterized solver for a particular dataset such improvements are typically achieved by sacri ficing some performance on a subset of instances. therefore if instead there are a number of highly specialized solvers available techniques like algorithm selection are designed to automatically determine the most appropriate approach for any newly presented instance rice . when applied to competitions in domains like sat le berre roussel and simon maxsat argelich et al. and csp van dongen et al. it is commonly observed that a portfo lio properly utilizing solvers from one or even two years ago copyright c association for the advancement of artificial intelligence www.aaai.org. all rights reserved. can readily dominate over any new single solver. developing new solvers is imperative and the only way to drive research forward but it is clear that rather than being general pur pose jackofalltrades programs these new solvers need to be highly specialized for only a small subset of problems. it is then the job of algorithm selection techniques to identify when each of the solvers should be used. yet while there is now a plethora of competing algorithm selection approaches kotthoff all of them are fun damentally dependent on the quality of a set of structural features they use to distinguish amongst the instances. if the features are too noisy or uninformative no selection tech nique will be able to make intelligent decisions. over the years each domain has defined and refined its own set of features yet at their core they are mostly a collection of ev erything that was considered useful in the past. it is typically the job of filtering techniques to identify the most represen tative feature set for each selection technique for a particular set of solvers. in recent years there have been a few attempts to augment this shotgun generation of features through the use of a systematic analysis of the latent features based on solver performances malitsky and osullivan but in practice such approaches take a considerable amount of ex pertise to generate. the focus of this paper is therefore to eliminate the hu man element from the feature generation process by using a deep learning approach. the paper notes that for a ma jority of domains the specifics of any problem instance is typically expressed as a text document. for example a sat problem is typically represented in the dimacs cnf for mat trick et al. where after a header each line in the file describes the literals in each clause. similarly csp problems can be represented in the xcsp format roussel and lecoutre while mips can be represented in lp or mps formats ilog . regardless of the format the problem instances are represented in a text file. this paper therefore presents a way to automatically convert any such text file into a grayscale square image which can in turn be used to train a deep neural network to predict the best solver for the instance. the proposed technique was applied across multiple datasets in sat and csp domains. we first benchmark the datasets by showing the performance of a stateoftheart al gorithm selection strategy cshc malitsky et al. uti lizing the established set of features. we subsequently eval uate the performance of the deep neural network to predict the best solver as well as the quality of the output vector of such a network to act as a new feature vector for an existing selection strategy. both of the new automated techniques are considerably better than the best single solver in any of the benchmarks and while not quite at the level of the stateof theart portfolio on features studied for over a decade our fully automated approach is shown to be competitive. to the best of our knowledge this is the first tentative us age of deep learning in this area. very recently introduced approaches share the idea of extracting knowledge from raw data without employing any crafted information. for exam ple a novel approach shows how a deep neural network can learn the semantic of simple arithmetic operations such as addition and subtraction by simply training the network us ing images. the input consists of two images of numbers while the output corresponds to the results of the selected arithmetic operation hoshen and peleg . the concept of number and arithmetic operator is left to be learnt by the neural network. this work can be thought of as a com puter vision task of frame prediction and can be grouped with other approaches in the same area such as vinyals et al. . of a particular interest is another recent method that applies a temporal convolutional networks to text under standing where the applied technique has no knowledge of words phrases or sentences nor any knowhow about syntax or semantic structure zhang and lecun . yet results evidence surprising achievement even starting with only the simple character level input. these techniques however are specialized to their per spective domains whereas the approach presented here aims to work across multiple problem representations each with a unique grammar and internal organization of data. in short a sat file looks nothing like an xcsp file. furthermore even the problems themselves can vary dramatically in terms of their size while most machine learning approaches rely on constant sized feature vectors. this problem can be ab stracted away in text mining by relying on word counts or other relations but in our domain we might care about more than how frequently variable xi appears. therefore the work in this paper is a proposed first step in developing a method ology of representing a diverse class of problem domains in a finite representation. algorithm selection algorithm selection is the study of choosing the most appro priate solver for the problem at hand based on a descriptive set of features that describe the instance. in the sat community was rocked by the introduction of a new solver that completely dominated their annual competition xu et al. . satzilla was a a simple portfolio that relied on ridge regression to predict the expected logruntime of its constituent solvers executing the one with the lowest ex pected runtime. yet this straightforward application dramat ically altered how we approach solver development. in the subsequent years increasingly better portfolios came to the scene. isac kadioglu et al. utilized clustering to dif ferentiate the instances cphydra omahony et al. and s kadioglu et al. then used scheduling to find the best sequence of solvers to evaluate. in satzilla came out with a new method utilizing a random forest re claiming first place prizes in the sat community le berre roussel and simon . now the range of techniques is growing at a rapid rate and for an up to date list of related research we refer the reader to the constantly updated sur vey kotthoff . currently based on the latest results in the sat and maxsat competitions a stateoftheart portfolio is cshc malitsky et al. which is the technique we em ploy in this paper. an acronym for costsensitive hierarchi cal clustering cshc bases its decision by training a forest of random trees. each tree in the forest is trained with a split ting criteria that ensures that it divides the training data such that the instances in each child node maximally agree on the solver they prefer. the partitioning stops when the amount of data in a child node becomes too small. to ensure varia tion of the forest each tree is trained on a random subset of of the data and a random subset of considered features. therefore whenever a new instance is presented each tree votes for the best solver based on its data. from text to images to selection there are many file specifications utilized each one specifi cally formulated to most conveniently represent a particular problem. satisfiability instances are typically represented in cnf format c sat example p cnf in this representation a line beginning with a c is consid ered a comment while p signals the problem type number of variables and number of clauses. the subsequent lines list the literals belonging to each clause with a minus sign de picting a negation. in the example above the mathematical sat problem represented is x x x x x. the cnf format efficiently captures all the necessary de tails about the sat problem but at the same time is unable to capture the requirements of a full constraint satisfaction problem. for something like that the xcsp format is a bet ter fit which uses xml to first specify the domains then the variables and then the relations between the variables. the example below defines a problem with variables a and a that can take a value or and must each must be different. domains nbdomains domain named nbvalues..domain domains variables nbvariables variable namea domaind variable namea domaind variables constraint namec arity scopea a referenceglobalalldifferent certainly the sat problem could be represented as a csp. albeit not always practical one could even encode any np complete problem into any other np complete problem in polynomial time. but for our approach we want to be able to take potentially any problem definition and encode into something usable by a machine learning approach like a deep neural network. therefore this section shows how to take the above presented formats and convert them to gray scale images with a predefined dimension n. we subse quently show how these images can be used to train and test a deep neural network using foldcross validation. in our specification the output of the network represents a scoring of all solvers on the provided input instances which can either be used directly as a selection approach or as new features to be used by existing selection strategies. image generation converting text documents into images of a fixed size is a well studied topic for identification and prediction but is not readily applicable to our scenario. for one note that the vo cabulary and grammar is vastly different between the cnf and xcsp formats preventing us from taking any advantage of known structures. we particularly avoid such structures because we want our approach to be as general as possible. we similarly cannot take advantage of the typical approach of counting the frequency of words since in most cases we do not care how frequently words appear together but the relations of which words they appear next to. finally prob lems can be of widely different sizes with sat instances ranging from hundreds to millions of clauses and csp in stance consisting of just dozens of lines. ultimately the core issue is that we need a way to represent the employed conversion process works as follows for each available instance the plain text file is read character by character and replaced with its corresponding ascii code. each such code is stored in a vector of lengthn wheren is the number of total characters in the input file. after reading the entire file the vector is reshaped using the new dimen sion n . we can now draw a square gray scale image for each instance using the ascii code value for a shade of gray. for example the following snippet of a cnf file is represented as the following vector of ascii codes .... note that all characters are mapped including spaces and line break symbols. since ascii codes range between and they can be mapped conveniently to gray scale. q while this initial image representation is lossfree since there exists a onetoone mapping from the original text to the image and vice versa we now rescale the image to a predefined size e.g. x using standard image scaling operators. hence the process produces a set of images which are all of the same size. this is one key point of this work we strongly believe that instances expose structure and self similarity ansotegui et al. patterns can easily been figure image extracted from sat problem instance. figure image extracted from csp problem instance. visualized using images and that these properties can be maintained once we rescale the images. the images can be useful to visualize and to analyze these structures regardless of the considered domain. while scaling the images incurs a high loss in information it seems to be the case that the retained structure is sufficient to address decision problems such as algorithm selection. figures and show an image extracted from a sat and csp instance patterns are easily visible in each image. neural network in machine learning a neural network is a structure espe cially used for classification or regression tasks when the high dimensionality and nonlinearity of the data make these tasks hard to accomplish. in the realm of visual data the standard is to employ convolutional neural networks cnn. cnns are directly inspired by the hierarchy of the cells in vi sual neuroscience hubel and wiesel . the same struc ture roughly resembles the one in the visual cortex felle man and essen . nowadays it represents the stateof theart in image classification area krizhevsky sutskever and hinton and in many others such as speech recog nition sainath et al. and face recognition taigman et al. . convolutional neural networks are specifically de signed to deal with multidimensions input such as images. figure deep convolutional neural network structure. the modular approach of the deep network and the use of convolutional layers allow the early stage of the network to search for junctions of features while the use of pooling lay ers try to merge them semantically. the great success of con volutional neural networks on image related tasks inspired our approach in the context of algorithm portfolios could an image representation of textual information leverage the capabilities of cnns to perform algorithm selection while the last section described how we converted textual repre sentations to images we describe the employed cnn model. our deep cnn network starts with three convolutional layers each one followed by a maxpool layer and a dropout layer. at the very end of the network there are two fully connected layers with a single dropout layer in the middle. the output layer is composed of m nodes where m is the number of solvers. dropout layers help to prevent overfit ting srivastava et al. and make the neural network performances more stable in combination with other tech niques such as adjusting momentum and learning rate dur ing the training phase. the network uses the stochastic gradient descent sgd algorithm to speedup the backpropagation and during the training phase it is updated using nesterov momen tum sutskever et al. . the minibatch size is set to learning rate is initially set to . and momentum is initially set to .. both are adjusted during the training phase with step size of . for learning rate and . for momentum. the nonlinearity used is the rectify function x max x while the output layer uses the sigmoid function x ex. figure represents the structure of the convolutional neu ral network implemented and used in all experiments. the figure also reports the number of filters used and their di mensions as well as the dimension for each convolutional layers and the probabilities used by the dropout layers and maxpool layers. before training the neural network the data is preprocessed in the following way for each feature we subtract the mean and normalize each feature to have a standard deviation equal to . this preprocessing step has been shown to be beneficial for efficiency and performances of neural networks lecun et al. . the plain classification task i.e. training the neural net work to predict which is the best solver to use for a given instance has resulted in poor performances. consequently we moved to a slightly easier binary regression task which corresponds to train the neural network to predict whether a solver can solve the given instance or not. the objective loss function is the binary cross entropy l t logp t log p where t is the ground truth value and p is the predicted value. discussion as desired the proposed approach is oblivious to any domain specific properties since its parsing problem instances character by character and does not rely on any given predefined structure. in general the process exhibits very little bias except for the step that scales the initial image to its miniature sized version. our method relies on the fact that the employed scaling function e.g. the default image scaling algorithm retains the structure that is needed to perform algorithm selection. ideally one would want to learn this reduction function so that the needed structure is retained without depending on a somewhat arbitrary trans formation. in future work one could consider employing for example an lstm hochreiter and schmidhuber to learn the appropriate transformation function as well. experiments we implemented the neural network described in section using python . and lasagne .. lasagne is a framework based on theano . bastien et al. bergstra et al. which allows development of cnns at a rather ab stract and transparent level. in addition the framework al lows exploitation of high performance gpus. the main idea of lasagne is to compose the neural network using different available layers stacking them on top of each other. for each layer it is possible to alter various parameters based on the layers type. this rather straightforwardly leads to the imple mentation of the deep neural network. we have limited the training of the neural network to epochs since increas ing the number of epochs made the neural network overfit due to the limited amount of data available in those domains. we have also experimented with different image sizes e.g. x and x and while for domains with the highest dataset cshc bss cnn new feat. industrial . . . . . . . . random . . . . . . . . crafted . . . . . . . . csp . . . . . . . . table percentage of solved instances. number of instances e.g. random larger images resulted in better performances we decided to choose x which seemed to have the best tradeoff between number of input parameters and performance. we empirically evaluated this novel approach using dif ferent data sets coming from the satisfiability sat and constraint programming domains csp. the sat datasets are publicly available on the sat competition website and are usually divided into the following three subdomains industrial random and crafted. we have the performances of solvers for each of about instances for indus trial more than instances of random and about of crafted. we also use the performances of solvers for each of the almost instances in the csp domain. the csp instances come from the csp competition csp and include nontrivial instances from problem classes such as timetabling frequency assignment jobshop openshop quasigroup costas array golomb ruler latin square all interval series balanced incomplete block design and many others. this set includes both small and large arity constraints and all of the global constraints used during the csp solver competitions alldifferent element weighted sum and cumulative. for each dataset we performed the prediction task using a fold cross validation approach. hence we first split a dataset into a training and test set. the train set is then split further into train and validation splits using a ratio of . the neural network was trained using the im ages corresponding to the instances of a given dataset. the trained neural network was then used to predict which solver could finish a given test instance within the timelimit. the neural network outputs for each solver a value between and where indicates that the solver cannot finish the given instance and means the opposite. for evaluation we select the solver whose output obtains the highest value. this strategy will be referred to as cnn. alternatively in stead of relying solely on the neural network to make the correct decision on which solver to use it is also possible to interpret the output layer as a new feature vector. a special ized approach for algorithm selection e.g. xu et al. malitsky et al. can then be used to try to refine the selection process. we refer to this approach as new feat in the results that follow. the results obtained by our methods are compared with the ones obtained using regular manually crafted features with cshc malitsky et al. which represents a state oftheart approach in the area of algorithm portfolios. we use our own implementation of the classifier. similar to a majority class in a plain classification task the baseline in this setting is the following after executing dataset cshc bss cnn new feat. vbs industrial random crafted csp table average running time in seconds for various selec tion strategies. all solvers on the train dataset and computing the average running time elapsed by each one one chooses as prediction the algorithm that behaves on average the best. this selec tion strategy we label the best single solver bss. tables and summarize our empirical results. while our approach is not able to achieve stateoftheart perfor mance it does give better performances than the baseline on all considered domains. note that this is without relying on features crafted by expert humans. in particular table shows the percentage of solved in stances of a given domain using one of the beforementioned methods. the presented deviations are based on the statis tics after performing fold cross validation. we believe that the performance could be boosted further if more prob lem instances would be available for training. while the performance on random and csp can be clearly distinigu ished from the baseline the difference in performance on the benchmarks with a smaller set of available instances in dustrial and crafted is not as pronounced. we also consider how our approach performs in terms of average run time per instance. to this end we compared the prediction from the neural network not only in terms of num ber of solved instances but also in terms of average runtime used by the prediction to solve the instance. table reports these results. as before the first column reports the results obtained with cshc. the very last column vbs corre sponds to the oracle performance which corresponds to the average runtime that one would achieve if for each instance one would always select the fastest solver. once again this new approach is performing better then the bss in all of the scenarios and the gap to the stateoftheart is within reason able limits for a fully automated approach. table reports the number of misclassifications that the neural network incurred on sat instances. as already said the very last layer of the neural network is a vector of val ues in where in position i means that the ith solver cannot solve the given instance otherwise. for any test in stance and for any solver we know the actual ability of the solver e.g. solve or not solve. so given a test instance we counted how many output values of the neural network are dataset misclassification industrial . random . crafted . table average number of misclassified solvers per in stance by the convolutional neural network. wrong by changing to all the values less than equal to . and changing to the others comparing the outcomes with reality. on the industrial and random benchmarks the neural network makes very few errors where less than out of solvers are predicted incorrectly per instance. the number of misclassifications grows to about for the crafted dataset. but note that as long as the cnn selects a solver that can solve the instance the resulting portfolio will still yield good performance. in addition existing algorithm selection tech niques can further learn patterns on top of the predictions made by the neural network to automatically correct mis takes and improve overall predictions. given the presented results and considering the complex ity of our approach there are a number of alternate hypoth esis to which the rise in performance can be attributed to. for one it is possible to imagine that if the majority of the solvers in the portfolio are reasonable than simply randomly guessing the best solver could achieve comparable behavior. table shows that this is not the case in the examples we presented. the random agent always performs worse than the best single solver and its executions is far from the ones of the neural network suggesting that the neural network is able to extract some structure from the data that enables it to select the correct solver. an alternate explanation for the performance can be that our neural network is not really learning to differentiate be tween instances but between the generators used to cre ate them. this is possible since certain random generators can put larger clauses towards the top of the instance or der clauses lexicographically or subconsciously incorporate some other kind of order. it is well know in the literature that neural networks are notorious at picking up on these external patterns producing misleading results. to test this hypothesis we tried repeating the experiments with the sat instances but this time randomly shuffling the clauses of the instances and ordering the variables in each clause lexico graphically. however the performance of the resulting port folios was still comparable to the performance presented in table . an interesting question is if combining the manually crafted and automatically generated features results in fur ther improvements in terms of performance. to that end we simply append the original and new features and use the composed feature vector to train cshc. however the achieved performance based on the combined feature vector does not result in any improvements over just using the orig inal features. we have not yet explored if techniques like fea ture selection applied to the combined feature vector would boost performance. overall the results seem to strongly suggest that the in dataset bss random industrial . . . . random . . . . crafted . . . . table average percentage of solved instances for an agent that chose randomly. troduced approach of converting textual representations of problem instances into gray scale images does capture some structure of the underlying instance. this structure can then be picked up and exploited with the right tools. the result means that we can completely remove human expertise from the picture of algorithm selection allowing the tools to be readily applied to new fields. conclusion algorithm selection has drastically changed the practice of research of algorithms. the significant amount of re search has shown that instead of creating jackofalltrades methodologies we should instead focus on highly special ized techniques selecting the best strategy for the problem at hand. however in order to get selection techniques to work to their full potential a substantial amount of human exper tise is required to create the features necessary to differen tiate between instances. in this work we take the first step to fully automating this process in hopes to make algorithm selection an easier tool for researchers to use offthe shelf. we introduced the usage of deep learning techniques in automated algorithm portfolios by training a neural network using images extracted from problem instances. we show how this new approach gives solid performances on different domains even though we have not used any domain knowl edge. avoiding feature generation and the usage of domain knowledge makes this new approach very appealing on a va riety of different domains. while the presented approach continuously out performed any single solver it still naturally lags behind carefully con figured and specialized approaches. nonetheless there are many subsequent lines of research to pursue. the effects of introducing some domain knowledge to filter out repeti tive irrelevant words. alternatively another way of encoding words rather than character by character could be possible. furthermore learning how to compress the initial image in order to retain the necessary structure could not only im prove performance further but would also remove the bias induced by the employed scaling function. finally could one leverage existing knowledge representations such as the one available from imagenet deng et al. to improve performance all these directions are now being considered for future work. references ansotegui c. bonet m. l. girldezcru j. and levy j. . the fractal dimension of sat formulas. in demri s. kapur d. and weidenbach c. eds. ijcar volume of lecture notes in computer science . springer. ansotegui c. sellmann m. and tierney k. . a gender based genetic algorithm for the automatic configuration of algo rithms. in proceedings of the th international conference on principles and practice of constraint programming cp . berlin heidelberg springerverlag. argelich j. li c. m. manya f. and planes j. . maxsat evaluation . bastien f. lamblin p. pascanu r. bergstra j. goodfellow i. j. bergeron a. bouchard n. wardefarley d. and bengio y. . theano new features and speed improvements. corr abs.. bergstra j. breuleux o. bastien f. lamblin p. pascanu r. desjardins g. turian j. wardefarley d. and bengio y. . theano a cpu and gpu math compiler in python. in van der walt s. and millman j. eds. proceedings of the th python in science conference . . csp solver competition benchmarks. httpwww. cril.univartois.fr lecoutrebenchmarks. html. deng j. dong w. socher r. li l.j. li k. and feifei l. . imagenet a largescale hierarchical image database. in cvpr. felleman d. j. and essen d. c. v. . distributed hierarchical processing in the primate cerebral cortex. cerebral cortex . fitzgerald t. malitsky y. and osullivan b. . reactr re altime algorithm configuration through tournament rankings. in proceedings of the twentyfourth international joint conference on artificial intelligence ijcai. hochreiter s. and schmidhuber j. . long shortterm mem ory. in neural computation . hoshen y. and peleg s. . visual learning of arithmetic op erations. corr abs.. hubel d. and wiesel t. . receptive fields binocular interac tion and functional architecture in the cats visual cortex. journal of physiology . hutter f. hoos h. and leytonbrown k. . sequential modelbased optimization for general algorithm configuration. in learning and intelligent optimization th international confer ence lion rome italy january . selected papers . ilog. . cplex . file formats. kadioglu s. malitsky y. sellmann m. and tierney k. . isac instancespecific algorithm configuration. in coelho h. studer r. and wooldridge m. eds. ecai volume of fron tiers in artificial intelligence and applications . ios press. kadioglu s. malitsky y. sabharwal a. samulowitz h. and sellmann m. . algorithm selection and scheduling. in in ternational conference on constraint programming volume . kotthoff l. . algorithm selection for combinatorial search problems a survey. ai magazine . krizhevsky a. sutskever i. and hinton g. e. . imagenet classification with deep convolutional neural networks. in bartlett p. l. pereira f. c. n. burges c. j. c. bottou l. and weinberger k. q. eds. nips . le berre d. roussel o. and simon l. . sat compe tition. lecun y. bottou l. orr g. and muller k. . efficient backprop. in orr g. and k. m. eds. neural networks tricks of the trade. springer. malitsky y. and osullivan b. . latent features for algorithm selection. in the seventh annual symposium on combinatorial search socs. malitsky y. sabharwal a. samulowitz h. and sellmann m. . algorithm portfolios based on costsensitive hierarchical clustering. in ijcai proceedings of the rd international joint conference on artificial intelligence beijing china august . omahony e. hebrard e. holland a. nugent c. and osullivan b. . using casebased reasoning in an algorithm portfolio for constraint solving. in proceedings of the th irish conference on artificial intelligence and cognitive science. rice j. . the algorithm selection problem. advances in computers . roussel o. and lecoutre c. . xml representation of con straint networks format xcsp .. corr abs.. sainath t. n. rahman mohamed a. kingsbury b. and ramab hadran b. . deep convolutional neural networks for lvcsr. in icassp . ieee. srivastava n. hinton g. e. krizhevsky a. sutskever i. and salakhutdinov r. . dropout a simple way to prevent neural networks from overfitting. journal of machine learning research . sutskever i. martens j. dahl g. e. and hinton g. e. . on the importance of initialization and momentum in deep learn ing. in icml volume of jmlr proceedings . jmlr.org. taigman y. yang m. ranzato m. and wolf l. . deepface closing the gap to humanlevel performance in face verification. in conference on computer vision and pattern recognition cvpr. trick m. chvatal v. cook b. johnson d. mcgeoch c. and tarjan b. . the second dimacs implementation challenge. van dongen m. lecoutre c. manya f. and planes j. . csp evaluation . vinyals o. toshev a. bengio s. and erhan d. . show and tell a neural image caption generator. cite arxiv.. xu l. hutter f. hoos h. and leytonbrown k. . satzilla an automatic algorithm portfolio for sat. solver de scription. sat competition. xu l. hutter f. shen j. hoos h. and leytonbrown k. . satzilla improved algorithm selection based on cost sensitive classification models. solver description sat challenge . zhang x. and lecun y. . text understanding from scratch. corr abs..  feature filtering for instancespecific algorithm configuration christian kroer ituniversity of copenhagen copenhagen denmark ckroitu.dk yuri malitsky dept. of computer science brown university providence ri. usa ynmcs.brown.edu abstractinstancespecific algorithm configuration isac is a novel general technique for automatically generating and tuning algorithm portfolios. the approach has been very successful in practice but up to now it has been committed to using all the features it was provided. however traditional fea ture filtering techniques are not applicable requiring multiple computationally expensive tuning steps during the evaluation stage. to this end we show three new evaluation functions that use precomputed runtimes of a collection of untuned solvers to quickly evaluate subsets of features. one of our proposed func tions even shows how to generate such an effective collection of solvers when only one highly parameterized solver is available. using these new functions we show that the number of features used by isac can be reduced to less than a quarter of the original number while often providing significant performance gains. we present numerical results on both sat and cp domains. keywordsfeature selection algorithm configuration sat cp i. introduction it has long been observed in the constraint programming cp and satisfiability sat communities that certain prob lems are easier to solve with one algorithm but performance then suffers on other problems. this observation has led to the pursuit of designing approaches that can adapt to the problem at hand. one such approach can be characterized as algorithm configuration which adjusts the parameters defining the behavior of a solver to best suit the provided training set . alternatively algorithm portfolios take advantage of the variety of solvers already developed and the inherent variance in their performance. when given a new instance this approach predicts the performance quality of each solver to then use the one with the best expected outcome . instancespecific algorithm configuration isac is a recent example of an approach which creates its own portfolio of solvers. the approach first clusters the training instances and then tunes a separate solver for each cluster. when a new instance is provided it is assigned to a cluster and then solved with the tuned solver. the tuned solver can be a single parameterized solver like satenstein saps or even cplex . the solver can also be a portfolio where the tuner not only chooses the best solver for a cluster but also the parameters for that solver. isac was recently shown to be highly effective in the sat domain. a novel solver tuned using the isac methodology won gold medals in the sat competition . isacs application has also been shown for other domains like set covering scp and mixed integer mip prob lems . one of the major drawbacks of isac is its dependence on the feature vector it uses to differentiate the problem instances. the success of the entire clustering hinges on the ability of these features to correctly group instances that are likely to behave similarly under the same solver. however there is a disconnect between the clustering ob jective and the performance objective isac tries to improve. so far research of isac was fortunate that the computed clusters yielded significant performance gains over the other approaches. to tackle this problem we propose an approach that builds off our initial assumption that instances with similar features will behave comparably under the same parameter settings. we therefore design three new evaluation functions that can be computed without retuning solvers for each iteration. these functions are first evaluated on four standard sat benchmarks and then confirmed in the cp domain. in the remainder of the paper we give an overview of related work in section ii and a description of isac in section iii. section iv presents the new evaluation strate gies we propose. section v introduces the feature filtering algorithms that we employ. section vi presents the numeric results on sat and cp domains and section vii concludes with a discussion of our contributions and results. ii. related work algorithm selection is a well studied approach in machine learning . in this scenario a large number of algorithms are run on a collection of training instances and the resulting data would be used to train a model mapping from algorithm problem pairs to the expected performance. the problem is represented as a vector of feature values. when a new instance is provided its feature vector and expected time for each solver is computed. the solver with the shortest expected runtime is then used to solve the instance. in the optimization domain these portfolios were first introduced in by carla gomes and bart selman . portfolio algorithms have become increasingly popular es pecially after the success of satzilla . this sat solver dominated the sat competitions in and in the randomly generated problem category. the solver is based on basic machine learning techniques sparse multino mial logistic regression to predict the log runtime of a solver and feedforward selection to filter superfluous features. in the case of satzilla feature filtering can be applied directly because a new prediction model can be trained quickly. cphydra is another portfolio algorithm that domi nated the recent cp competitions creating a schedule of solvers in its portfolio that best utilizes the allotted time. in this approach a knearestneighbor approach is employed to gather the data used by a constraint program to create the schedule. this approach assumes that all the features are equally important and performs no feature filtering. this could possibly be due to the time requirements of evaluating the quality of a subset of features. evaluation will need a large number of constraint programs to be computed to create a new schedule for each training instance before overall performance is computed. for situations where there is only a limited supply of solvers available a hybrid methodology has been proposed that aims to create a custom set of solvers for the algorithm portfolio. one such method is hydra which iteratively tunes a new version of a highly parameterized solver saten stein adding it to the existing portfolio only if it helps to improve performance on a training set of instances. this methodology is based on satzilla once the solver is added the resulting portfolio is tuned just like satzilla. in argosmart the authors parametrize and tune the argosat solver using a partition of the sat competition instances from the random category into training and testing set. using a supervised clustering ap proach the authors build families of instances based on the directory structure in which the sat competition has placed these instances. the authors enumerate all possible parametrization of argosat in total and find the best parametrization for each family. for a test instance argosmart then computes the core satzilla features that do not involve runtime measurements and assigns the instance to one of the instance families based on majority knearestneighbor classification utilizing a noneuclidean distance metric. the best parametrization for that family is then used to tackle the given instance. yet here also the authors assume that all features are equally important. in these and other approaches in the optimization com munity algorithm portfolios have been shown to drastically improve performance over using a single solver. however only a few of the approaches use feature filtering. this is partly because applying any standard feature selection methods would require very computationally expensive eval isaclearna t f f s t normalizef k c s cluster t f for all i . . . k do pi ggaasi end for return k pc s t isacruna x k p c d s t f featuresx fi fisi ti i i minif ci return ax pi algorithm instancespecific algorithm configuration uation functions. in this paper we show how isac can be augmented using feature filtering with three newly proposed evaluation functions. iii. isac isac is a newly proposed approach for tuning algorithms for the problem instances they will be used for. the approach works as follows see algorithm . in the learning phase isac is provided with a parameterized solver a a list of training instances t and their corresponding feature vectors f . first the gathered features are normalized so that every feature ranges from and the scaling and translation values for each feature s t is memorized. this normalization helps keep all the features at the same order of magnitude and thereby keeps the larger values from being given more weight than the lower values. then the instances are clustered based on the normalized feature vectors. clustering is advantageous for several rea sons. first training parameters on a collection of instances generally provides more robust parameters than one could obtain when tuning on individual instances. that is tuning on a collection of instances helps prevent overtuning and al lows parameters to generalize to similar instances. secondly the found parameters are prestabilized meaning they are shown to work well together. to avoid specifying the desired number of clusters before hand the gmeans algorithm is used. robust parameter sets are obtained by not allowing clusters to contain fewer than a manually chosen threshold a value which depends on the size of the data set. beginning with the smallest cluster the corresponding instances are redistributed to the nearest clusters where proximity is measured by the euclidean distance of each instance to the clusters center. the final result of the clustering is a number of k clusters si and a list of cluster centers ci. then for each cluster of instances si favorable parameters pi are computed using the instance oblivious tuning algorithm gga . e distc r i n v for i i do ni normalizeri end for for c c do v v c ii ji ni nj end for return v algorithm evaluation functions used to measure the quality of a clustering of instances. gga races a large number of parameter settings on subsets of the training instances with the best performing parameters getting the chance to crossover and continue to subsequent generations. starting with a small number of instances the subset of instances used for tuning grows with each iteration as the bad parameter settings get weeded out of consideration. in the final generation of gga when all training instances are used the best parameter setting has been shown to work very well on these and similar instances. when running algorithm a on an input instance x isac first computes the features of the input and normalize them using the previously stored scaling and translation values for each feature. then the instance is assigned to the nearest cluster. finally isac runs a on x using the parameters for this cluster. iv. cluster evaluation the effect of the filtering algorithms such as the ones discussed in section v strongly depends on the quality of the evaluation function. in order to evaluate a set of features using standard techniques the training instances would be clustered and a new solver tuned for each cluster. the quality of the feature would then be defined as the performance of the portfolio solver on some validation set of instances. however because of the long time needed to tune the algorithms evaluation based on this kind of performance is impractical. to sidestep this issue we instead focus on the primary assumption behind isac that a solver will have consistent performance on instances that are clustered together. based on this assumption we introduce three possible evaluation functions that utilize a collection of untuned solvers to determine the quality of a cluster of instances. the first evaluation criteria is presented in algorithm as e dist. given the clustering of the instances c the runtime of each untuned solver on each instance r and the list of instances i this algorithm tries to match the relative quality of solver runtimes on instances in the same cluster. thus the algorithm tries to make sure that the same solver works best on all instances in the cluster and the same solver provides the worst performance. because the runtimes e timec r v for c c do v v minsrruntimes c end for return v algorithm evaluation functions used to measure the quality of a clustering of instances. can vary significantly between instances these times are normalized for each instance to be from to with being the fastest runtime and the longest. these normalized runtimes n can then be used to judge how similar two instances are and a good cluster is one where the average euclidean distances between each instance within the cluster is minimized. the evaluation of the overall clustering v is then the summation of the quality of each cluster weighted by the number of instances in that cluster c. here we do not consider the distances between clusters because it is not necessarily the case that different clusters require different solvers. only the uniformity of the instances within a cluster determine the success of a clustering. an alternative evaluation function measures the quality of the clustering directly by computing the performance of a portfolio algorithm based on the available solvers. e time in algorithm creates a portfolio of untuned solvers and chooses which solver to assign to each cluster. the algorithm finds the best performing solver in r on the instances of each cluster. the clustering can then be evaluated by summing the score for each cluster when using the best solver. this evaluation approach benefits from being similar to how isac will be evaluated in practice without having to tune each solver for the cluster. for algorithm we use the exact runtimes of the best solvers to evaluate a clustering. we also experimented with an algorithm where we again select the best solver for each cluster but the evaluation is done using a penalized version of the runtimes called par. each instance not solved gets penalized with a runtime that is ten times the cutoff time. using penalized runtimes makes the algorithms focus on minimizing the number of instances not solved. however we found that using the regular nonpenalized runtimes provided better performance both in terms of the average runtimes achieved and number of instances solved. using the performance of a portfolio for evaluating the clustering can yield very good results if the solvers in the available portfolio are numerous and have a lot of variance in their performance. this is true in the case of a well studied problem like sat but is not necessarily the case in all problem domains. to circumvent this issue we extend the evaluation criteria to generate the desired portfolio. given a single highly parameterized solver it is possible feedforwardselectionf i r f f f s s while s s do f s for f f do v evaluateclusterf f i r if v s then f f s v end if end for if s s then f f f s s end if end while return f algorithm feed forward feature selection to tune this solver using gga. in our case however we do not need the best performing parameter set but many parameter sets that behave reasonably well and with a lot of variance. these parameter sets can be initialized randomly but the resulting solvers are likely to perform very poorly. in a case were every solver times out it is impossible to determine which solver is best. but if we use the solvers from an intermediate generation of gga we will find that the very bad parameter sets have already been killed off by the tuner and all that remains are parameters that work well on different subsets of our training data. using these parameter settings we create a large portfolio of solvers that we can use for the direct evaluation of a clustering. this evaluation approach works as algorithm using the best solver on each cluster to compute the performance score of a clustering the difference being that the runtimes of the generated solvers are used in place of the regular solvers. v. filtering algorithms it is well established that the success of a machine learning algorithm depends on the quality of its features. too few features might not be enough to properly differentiate between two or more classes of instances. alternatively too many features often results in some of the features being noisy and even damaging. furthermore as the feature space increases more data is needed in order to make accurate predictions. in this paper we work with the three standard feature selection algorithms feedforward selection back ward selection and a hybrid of the two. all three of these algorithms can use any of the evaluation functions discussed in section iv. feedforward selection algorithm starts with an empty set f and tries to add each of the available features f . using each of these new subsets of features the training set i is clustered and evaluated. the feature f whose addition to the current set yields the best performance s is added to the set and the process is repeated until no more features can be added without the evaluation score deteriorating. alternatively backwardselection starts with the full fea ture set and removes features one at a time in a manner that is analogous with how feed forward selection adds them. the algorithm terminates when such a removal leads to a decrease in performance according to the evaluation function. a natural extension of the above two algorithms is a hybrid approach of the two. as in backward selection the algorithm begins with the full feature set and removes features one at a time while the solution doesnt deteriorate. the algorithm however also checks if adding any of the removed features improves the solution. if this is the case the feature is added back into the set. this helps when the beneficial effects are being obfuscated by many noisy features. once the troublesome features are removed the benefits are observed and the mistake of removing the feature is rectified. vi. numerical results for our experiments we first focus on the sat do main a well studied problem that has numerous solvers benchmarks and well defined features. sat is also a do main where isac has been shown to yield stateoftheart performance. showing the performance gains on sat we then continue by switching to the cp domain. the timed experiments used dual intel xeon . ghz quad core nehalem processors with gb of ddr memory ghz. a. benchmarks for sat we choose to focus on local search solvers predominately due to the existence of satenstein a highly parameterized solver. satenstein is used to explore our evaluation function that uses gga to create a portfolio of solvers. evaluation is based on the hand and rand datasets that span a variety of problem types and were developed to test local search based solvers. because local search can never prove that a solution doesnt exist these four datasets only have satisfiable instances. these two data sets were chosen because they have been shown to be hard for portfolio algorithms in . the hand and rand datasets are respectively com posed of handcrafted and randomly generated instances. the hand dataset has training and testing in stances. the rand dataset has training and test ing instances. for features we use the well established features introduced in . it is important to note that these features have been used for sat portfolios for over a decade so they have been thoroughly vetted as being important. hand bs all e dist e time e time gga rsaps features forward backward hybrid forward backward hybrid forward backward hybrid par avg par std runtime avg . . . . . . . . . . . runtime std . . . . . . . . . . features clusters solved solved . . . . . . . . . . . rand bs all e dist e time e time gga gnovelty features forward backward hybrid forward backward hybrid forward backward hybrid par avg . . . . . . . . . . par std runtime avg . . . . . . . . . . . runtime std . . . . . . . . . . . features clusters solved solved . . . . . . . . . table i results on the sat benchmarks comparing the best performing individual solver bs the original isac using all features all features and all the combinations of evaluation functions and filtering algorithms. in these experiments we used an assortment of success ful local search solvers paws rsaps saps agwsat agwsat agwsatp gnovelty gwsat ranov vw and anov . we also use six additional fixed parameterizations of satenstein known as fact cmbc rfix hgen swgcp and qcp. for evaluation a second timeout was used. for the cp benchmarks we employ instances from cpai . we removed a small number of instances for which the cphydra feature computation code did not work. the remaining instances were split into train instances and test instances. our portfolio con sisted of a subset of the solvers that competed in the original competition abscon v ac abscon v esac bpsolver casper zao casper zito choco dwdeg choco impwdeg cphydra k cphydra k mdgnoprobe mdgprobe minion tailor mistraloption . mistralprime . satj csp sugar v.minisat sugar v.picosat. for the runtimes we used the runtimes from the competition results which had a second timeout . b. e dist approach table i shows the results of the best performing solver over all the instances in each of the four benchmarks. the table then presents the performance of an algorithm portfolio of the local search solvers tuned with isac using the complete set of features. in this scenario once the training instances are clustered the best performing solver in the portfolio is assigned to each cluster. the decision of the solver to use is based on the best average runtime. in all cases the average runtime was improved. the change is especially significant for rand. furthermore as is expected in all cases isac usually doesnt use the solver that is found best over all instances which suggests that certain solvers are better at solving certain instances while sacrificing performance on other types. for the hand benchmark however the time gain is minimal and one fewer instance is solved. judging by the performance of isac with feature filtering this poor performance is due to a poor clustering as a result of some confounding and noisy features. table i then shows the results from running isac after the features are filtered using the euclidean distance evaluation criteria e dist. it is interesting to note that for both of the datasets it is possible to maintain the same level of perfor mance with significantly fewer features. this is especially true for feedforward selection that uses less than a quarter of the features. however we also observe that there is no significant improvement of the overall performance of the resulting portfolio. c. e time approach once we use a more accurate evaluation criteria e time we observe that isacs performance can be boosted as seen in table i. here feedforward selection is again the best performing approach and we observe improvements in both of our datasets although we also observe an increase in the number of used features. this seems to support the assumption that not all of the established features are necessary and are in fact damaging to the clustering in isac. it is interesting to note that the features found by forward selection do not overlap much for four benchmarks with only two features appearing in both sets. the first is the maximum ratio of positive to negative literals per clause. the second is the number of unary literals in the problem. also of note is that feedforward selection only chooses cp cphydra all forward backward k features parscore std. deviation avg. runtime . . . . std. deviation . . . . features clusters solved solved . . . . table ii results on the cp benchmark comparing the best performing solver cphydra isac using all features and the forward and backward filtering algorithms using the e time evaluation function. the local search probing features twice for rand. in back ward selection however almost all these probing features are used in both benchmarks. these features are stochastic with potentially a lot of variance between computations. they are also very computationally expensive especially for larger instances. fortunately according to a comparison of the forward and backward selection algorithms these features are not needed and do not improve performance. the number of clusters does not change drastically when switching from the euclidean distance e dist to the time performance evaluation functions e time. this suggests that simply increasing or decreasing the number of clusters is not enough to improve performance. this also validates our clustering approach showing that if a cluster contains similar instances then it is possible to tune a very high quality solver for those instances. in these experiments there is little difference in the fea tures found by backward selection and our hybrid approach. this is not very surprising since the clustering is based on a linear relation between the features and it is unlikely that a removed feature would become beneficial once another feature is removed. these results on sat are encouraging and as can be seen in table ii they also extend to the cp domain. when comparing all the single solvers cphydra significantly out performs the other solvers. the closest competitor is mistral prime . which solves instances. as shown in the table applying isac to tune a portfolio algorithm leads to marginal improvements in the average runtime. however once feature filtering is applied the performance of the tuned portfolio improves significantly. using backward filtering creates a performance gap to cphydra equal to the one separating cphydra from its closest competitor. d. e time gga approach running the feature filtering algorithms using the runtimes of the gga generated solvers yields the times presented under e time gga in table i. in this case the forward selection algorithm worsens performance on rand but otherwise remains competitive with the original isac with less than a of the features. backward selection does not worsen performance on any datasets and even gives the best performance of all the approaches on the rand dataset while removing a significant number of features in all cases. in all cases we see that using feedforward selection greatly reduces the number of needed features while usually improving the overall performance of the resulting tuned solver. backward selection on the other hand seems like a more conservative approach removing fewer features but it also offers consistent improvements over all datasets. this suggests that there are some features that should not be used for clustering and all filtering algorithms remove these. but there are also some dependancies between the features and including these features is important to improve the quality of the clusters used by isac. while feedforward selection generally outperformed backward selection on all datasets when using the portfolios of solvers we see that when using the gga generated solvers backward selection clearly outperforms forward selection. when using a portfolio of solvers we have access to more variety in solvers as opposed to using a set of parameterized versions of the same solver. this seems to indicate that feedforward selection has a higher need for diversity in solvers as it struggles with picking out the most important features starting from none whereas backward selection is able to successfully remove a large set of noisy features and provide performance gains using the less diverse gga solvers. vii. conclusions instancespecific algorithm configuration isac is a newly proposed methodology that has been very successful in tuning a wide range of solvers for sat mip and sc. however the approach was very dependent on the quality of features it was provided. it is well known in the machine learning community that a poorly selected set of features can have a strong detrimental effect on performance. standard approaches for feature filtering were not applicable due the computational cost associated with the evaluation of a clustering using a subset of features. in this paper we showed three modifications to the evaluation function that remove the expensive portion of the approach. applying feature filtering to isac we observe performance gains in both sat and cp domains while reducing the size of the feature sets. these performance gains are important in the case of sat since the features are already a subset of a larger set of features which has been carefully studied for the last ten years. yet even in this case we show that proper feature filtering does not worsen the performance but has a chance to greatly improve it. this observation is confirmed in the cp domain where the features have not been as carefully vetted. just applying isac on all features didnt improve the number of solved instances. but once feature filtering was applied the performance also improved. references b. adensodiaz and m. laguna. finetuning of algorithms using fractional experimental design and local search. operations research . c. ansoteguigil m. sellmann k. tierney. a gender based genetic algorithm for the automatic configuration of algorithms. cp . argosat. httpargo.matf.bg.ac.rssoftware. cpai competition. httpwww.cril.univartois.frcpai. c.p. gomes and b. selman. algorithm portfolios. artificial intelligence . g. hamerly and c. elkan. learning the k in kmeans. nips . h.h. hoos. adaptive novelty novelty with adaptive noise. aaai . f. hutter d. tompkins h.h. hoos. rsaps reactive scaling and probabilistic smoothing. cp . f. hutter h.h. hoos k. leytonbrown t. stuetzle. paramils an automatic algorithm configuration frame work. jair . ibm. ibm cplex reference manual and user manual. v. ibm . s. kadioglu y. malitsky m. sellmann k. tierney. isac instancespecific algorithm configuration. ecai . a.r. khudabukhsh l. xu h.h. hoos k. leytonbrown. satenstein automatically building local search sat solvers from components. ijcai . k. leytonbrown e. nudelman g. andrew j. mcfadden y. shoham. a portfolio approach to algorithm selection. ijcai . c.m. li and w.q. huang. gwsat gradientbased greedy walksat. sat . m. nikolic f. maric p. janici. instance based selection of policies for sat solvers. theory and applications of satisfiability testing . e. omahony e. hebrard a. holland c. nugent b. osullivan. using casebased reasoning in an algorithm portfolio for constraint solving. irish conference on artifi cial intelligence and cognitive science . d.n. pham and anbulagan. ranov. solver description. sat competition . d.n. pham and c. gretton. gnovelty. solver description. sat competition . s. prestwich. vw variable weighting scheme. sat . j.r. rice. the algorithm selection problem. advances in computers . sat competition. httpwww.satcomptition.org. sat competition . httpwww.cril.univartois.frsat. k.a. smithmiles. crossdisciplinary perspectives on meta learning for algorithm selection. acm comput. surv. . b. silverthorn and r. miikkulainen. latent class models for algorithm portfolio methods. aaai . j. thornton d.n. pham s. bain v. ferreira. additive versus multiplicative clause weighting for sat. pricai . d.a.d tompkins f. hutter h.h. hoos. saps. solver descrip tion. sat competition . r. vilata and y. drissi. a perspective view and survey of metalearning. artificial intelligence rev. . w. wei c.m. li h. zhang. combining adaptive noise and promising decreasing variables in local search for sat. solver description. sat competition . w. wei c.m. li h. zhang. deterministic and random selec tion of variables in local search for sat. solver description. sat competition . w. wei c.m. li h. zhang. adaptgwsatp. solver description. sat competition . l. xu h. h. hoos k. leytonbrown. hydra automatically configuring algorithms for portfoliobased selection. aaai . l. xu f. hutter h.h. hoos k. leytonbrown. satzilla an automatic algorithm portfolio for sat. solver descrip tion. sat competition . l. xu f. hutter h.h. hoos k. leytonbrown. satzilla portfoliobased algorithm selection for sat. jair .  features for exploiting blackbox optimization problem structure tinus abell yuri malitsky and kevin tierney it university of copenhagen copenhagen denmark tmabkevtitu.dk cork constraint computation centre cork ireland y.malitskyc.ucc.ie abstract. blackbox optimization bbo problems arise in numerous scientific and engineering applications and are characterized by compu tationally intensive objective functions which severely limit the number of evaluations that can be performed. we present a robust set of features that analyze the fitness landscape of bbo problems and show how an algorithm portfolio approach can exploit these general problem indepen dent features and outperform the utilization of any single minimization search strategy. we test our methodology on data from the gecco workshop on bbo benchmarking which contains stateofthe art solvers run on wellestablished functions. introduction this paper tackles the challenge of crafting a set of features that can capture the structure of blackbox optimization bbo problem fitness landscapes for use in portfolio algorithms. bbo problems involve the minimization of an objective function fx . . . xn subject to the constraints li xi ui over the variables xi r i n. these types of problems are found throughout the scientific and engineering fields but are difficult to solve due to their oftentimes expen sive objective functions. this complexity can arise when the objective involves difficult to compute expressions or that are too complicated to be defined by a simple mathematical expression. even though bbo algorithms do not guaran tee the discovery of the optimal solution they are an effective tool for finding approximate solutions. however different bbo algorithms vary greatly in per formance across a set of problems. thus deciding which solver to apply to a particular problem is a difficult task. portfolio algorithms like instance specific algorithm configuration isac which uses a clustering approach to identify groups of similar instances provide a way to automatically choose a solver for a particular instance using offline learning. however a set of features that consolidate the relevant attributes of a bbo instance into a vector is required to use such methods on bbo problems. the only way to generate these features for bbo problems is by evaluating yuri malitsky is partially supported by the eu fet grant icon project . kevin tierney is supported by the danish council for strategic research as part of the enerplan project. tinus abell yuri malitsky and kevin tierney expensive queries to the black box which contrasts with most nonblackbox problems e.g. sat or the set covering problem where many features can be quickly inferred from the problem definition itself. in this paper we propose a novel set of features that are fast to compute and are descriptive enough of the instance structure to allow a portfolio algorithm like isac to accurately cluster and tune for the benchmark. these features are based on wellknown fitness landscape measures and are learned through sampling the black box. these features allow anybody to take advantage of the recent advances in the isac framework in order to more efficiently solve their bbo problems. this paper is a short version of . related work there has been extensive research studying the structure of bbo problems and copious measures have been proposed for determining the hard ness of local search problems by sampling their fitness landscape such as the search space diameter optimal solution densitydistribution fitnessdistance correlation fdc the correlation length epistasis measures information analysis modality and neutrality measures and fitness distance analysis . difficulty measures for bbo problems in particular were studied by which concluded that in the worst case building predictive dif ficulty measures for bbo problems is not possible to do in polynomial time. most recently watson introduced several cost models for combinatorial land scapes in order to try to understand why certain algorithms perform well on certain landscapes . in the authors identify six lowlevel feature classes to classify bbo problems into groups. in algorithm selection for bbo problems is considered with a focus on minimizing the cost of incorrect algorithm selections unlike our approach which minimizes a score based on the penalized expected runtime. our approach also differs from online methods and reactive techniques that attempt to guide algorithms based on information from previously explored states because isac performs all of its work offline. bbo dataset solver portfolio we evaluate the effectiveness and robustness of our features on a dataset from the gecco workshop on blackbox optimization benchmarking bbob . the dataset contains the number of evaluations required to find a particular objective value within some precision on one of continuous noisefree opti mization functions from in different dimension settings for solvers. the solvers are all run on the data times each time with a different target value set as the artificial global optimum. note that the bbob documentation refers to each of these target values as an instance. to avoid confusion with the in stances that isac uses to train and test on we will only refer to bbob targets. removing instances from the dataset for which no solver was able to find a solution the dataset consists of instances. we use the solvers of the bbob dataset with full solution data for all instances. this portfolio consists of a diverse set of continuous optimizers in cluding covariance matrix adaptation cma variants differential evolution our results do not contradict this as we are not predicting the hardness of instances. features for exploiting blackbox optimization problem structure problem definition features . solver accuracy . number of dimensions hill climbing features . average distance between optima average std. dev. . distance between best optima and other optima average std. dev. . percent of optima that are the best optimum random point features . distance to local optimum average std. dev. . fitnessdistance correlation fdc table bbo problem features. de variants an ant colony optimization aco algorithm a genetic algorithm ga and a particle swarm optimization pso algorithm. features computing features for bbo problems is difficult because evaluating the objec tive function of a bbo problem is expensive and there is scarce information about a problem instance in its definition other than the number of dimensions and the desired solver accuracy. in the absence of any structure in the problem definition we have to sample the fitness landscape. however such sampling is expensive and on our dataset performing more than objective evaluations removes all benefits of using a portfolio approach. we therefore introduce a set of features that are based on wellstudied aspects of search landscapes in the literature . our features are drawn from three information sources the problem definition hill climbs and random points. table summarizes our features. the problem definition features contain the desired accuracy of the continuous variables feature and the number of dimensions that the problem has feature which together describe the size of the problem. the hill climbing features are based off of a number of hill climbs that are initiated from random points and continued until a local optimum or a fixed number of evaluations is reached. we then calculate the average and standard deviation of the distance between optima features and which describes the density of optima in the landscape. using the best optimum found we then compute the average and standard deviation of the distance between the optima and the best optimum features and using the nearest to each non best optimum for these features if multiple optima qualify as the best. feature describes what percentage of the optima are equal to the best optimum giving a picture of how spread out the optima are throughout the landscape. the random point features and contain the average and standard deviation of the distance of each random point to the nearest optimum which describes the distribution of local optima around the landscape. feature computes the fitnessdistance correlation a measure of how effectively the fitness value at a particular point can guide the search to a global optimum . in feature we compute an approximation to the fdc. numerical results in this section we describe the results of using our features in full and in various combinations to train a portfolio solver using the isac method on the bbob full details about the algorithms are available in . tinus abell yuri malitsky and kevin tierney dataset. we measure the performance of each solver using a penalized score that takes into account the relative performance of each solver on an instance. we do not directly use the expected running time ert value because the amount of evaluations can vary greatly between instances and too much focus would be placed on instances where a large number of evaluations is required. the penalized score of solver s on an instance i is given by scores i pert s i besti worsti besti where pert s i is the penalized ert defined by pert s i ert s i if ert s i worsti otherwise besti refers to the lowest ert score on instance i and worsti refers to the highest noninfinity ert score on the instance. the penalized ert therefore returns ten times the worst ert on an instance for solvers that were unable to find the global optimum. we are forced to use a penalized measure because if a solver cannot solve a particular instance it becomes impossible to calculate its performance over the entire dataset. . isac results table shows the results of training and testing isac on the bbob dataset. for each entry in the table we run a fold cross validation using features from each of the bbob target values. the scores of each of the crossvalidation folds are accumulated for each instance and the entries in the table are the average and standard deviation across all instances in the dataset. we compare our results against the best single solver bss on the dataset mvde which is the best performing solver across all instances. we train using several subsets of our features only feature f only feature f and only features and f. we then train using all features all and only landscape features lsf i.e. features through . all and lsf include the evaluations necessary to compute the features whereas all other entries do not include the feature computation in the results. we used several different settings of the number of hill climbs and maximum hill climb length based on our feature robustness experiments in hill climbs of maximum length hill climbs of maximum length and hill climbs of maximum length . the closer a score is to the score of the virtual best solver the better the performance of an approach. based on results for f f and f the easy to compute bbo features alone are only able to give isac some information about the dataset and that a landscape analysis is justified. on the other hand f outperforms bss. in fact f performs equally well to all and lsf for cluster with hill climbs of length and for hill climbs of length . in addition f significantly outperforms all on cluster size where it is clear that it overfits the training data. this is a clear indication that hill climbs of length or hill climbs of length do not provide enough information to train isac to be competitive with simply using the number of dimensions of a problem. features for exploiting blackbox optimization problem structure test train test train test train bss . . . . . . . . . . . . f . . . . . . . . . f . . . . . . . . . . . . f . . . . . . . . . . . . all . . . . . . . . . . . . all . . . . . . . . . . . . lsf . . . . . . . . . . . . lsf . . . . . . . . . . . . bss . . . . . . . . . . . . f . . . . . . . . . f . . . . . . . . . . . . f . . . . . . . . . . . . all . . . . . . . . . . . . all . . . . . . . . . . . . lsf . . . . . . . . . . . . lsf . . . . . . . . . . . . table the average and standard deviation of the scores across all instances for various minimum cluster sizes numbers of hill climbs and hill climb lengths for the best single solver and isac using various features. the fact that lsf is able to match the performance of f on hill climbs of length for both cluster size and an important accomplishment. with so little information learned about the landscape the fact that isac can learn such an effective model indicates that our features are indeed effective. once we move up to hill climbs of length lsf significantly out performs f and even outperforms all which suffers from overfitting. in fact lsf is able to cut the total score to under a fourth of bsss score and to one half of fs score indicating that the fitness landscape can indeed be used for a portfolio. in addition lsf has a lower standard deviation than bss. lsfs score on the training set of . and . on the test set are surprisingly close to the virtual best solver which has a score of zero indicating that isac is able to exploit the landscape features to nearly always choose the best or second best solver for each instance. on the downside hill climbs of length requires too many evaluations to be used in a competitive portfolio and all needs times the evaluations of bss. conclusion and future work we introduced a set of features based on accepted and wellstudied properties and measures of fitness landscapes to categorize bbo problems for use in algo rithm portfolios like isac that can greatly improve the ability of practitioners to solve bbo problems. we experimentally validated our features within the isac framework showing that isac is able to exploit problem structure learned during feature computation to choose the fastest solver for an unseen instance. the success of the features we introduced clearly indicates that selecting algo rithms from a portfolio based on the landscape structure is possible. for future work we plan to use these features to analyze what types of landscapes fit best tinus abell yuri malitsky and kevin tierney to which solvers which could influence solver development allowing solvers to more specifically target problems they solve well. references . t. abell y. malitsky and k. tierney. fitness landscape based features for exploit ing blackbox optimization problem structure. technical report tr it university of copenhagen . . a. auger n. hansen v. heidrichmeisner o. mersmann p. posik and m. preuss. gecco workshop on blackbox optimization benchmarking bbob. httpcoco.gforge.inria.frdoku.phpidbbob . . r. battiti and m. brunato. reactive search optimization learning while optimiz ing. handbook of metaheuristics pages . . b. bischl o. mersmann h. trautmann and m. preu. algorithm selection based on exploratory landscape analysis and costsensitive learning. in gecco pages new york ny usa . acm. . j. boyan and a.w. moore. learning evaluation functions to improve optimization by local search. the journal of machine learning research . . c. brooks and e. durfee. using landscape theory to measure learning difficulty for adaptive agents. in alonso e. et. al. editor adaptive agents and multiagent systems volume of lncs pages . springer . . s. finck n. hansen r. ros and a. auger. realparameter blackbox optimization benchmarking presentation of the noisy functions. technical report research center ppe . . j. he c. reeves c. witt and x. yao. a note on problem difficulty measures in blackbox optimization classification realizations and predictability. evolutionary computation december . . h.h. hoos and t. stutzle. stochastic local search foundations applications. morgan kaufmann publishers inc. . . t. jones and s. forrest. fitness distance correlation as a measure of problem difficulty for genetic algorithms. in icga pages . . s. kadioglu y. malitsky m. sellmann and k. tierney. isacinstancespecific algorithm configuration. in ecai volume of faia pages . . v.v. melo. benchmarking the multiview differential evolution on the noiseless bbob function testbed. in gecco pages . acm . . o. mersmann b. bischl h. trautmann m. preu c. weihs and g. rudolph. exploratory landscape analysis. in gecco pages . acm . . p. merz and b. freisleben. fitness landscapes memetic algorithms and greedy operators for graph bipartitioning. evolutionary computation . . b. naudts and l. kallel. a comparison of predictive measures of problem difficulty in evolutionary algorithms. ieee trans. on evo. comp. . . t. smith p. husbands p. layzell and m. oshea. fitness landscapes and evolv ability. evolutionary computation . . p.f. stadler and w. schnabl. the landscape of the traveling salesman problem. physics letters a . . v.k. vassilev t.c. fogarty and j.f. miller. information characteristics and the structure of landscapes. evolutionary computation march . . j. watson. an introduction to fitness landscape analysis and cost models for local search. in m. gendreau and j. potvin editors handbook of metaheuristics volume pages . springer . . e. weinberger. correlated and uncorrelated fitness landscapes and how to tell the difference. biological cybernetics .  isac instancespecific algorithm configuration serdar kadioglu and yuri malitsky and meinolf sellmann and kevin tierney abstract. we present a new method for instancespecific algorithm configuration isac. it is based on the integration of the algorithm configuration system gga and the recently proposed stochastic off line programming paradigm. isac is provided a solver with cate gorical ordinal andor continuous parameters a training benchmark set of input instances for that solver and an algorithm that com putes a feature vector that characterizes any given instance. isac then provides high quality parameter settings for any new input in stance. experiments on a variety of different constrained optimiza tion and constraint satisfaction solvers show that automatic algorithm configuration vastly outperforms manual tuning. moreover we show that instancespecific tuning frequently leads to significant speedups over instanceoblivious configurations. introduction when developing a new heuristic or complete algorithm for a con straint satisfaction or a constrained optimization problem we fre quently face the problem of choice. there may be multiple branch ing heuristics that we can employ different types of inference mech anisms various restart strategies or a multitude of neighborhoods to choose from. furthermore the way in which the choices we make affect one another is not readily known. the task of making these choices is known as algorithm configuration. developers often make many of the algorithmic choices during the prototyping stage. based on a few preliminary manual tests certain algorithmic components are discarded even before all the remaining components have been implemented. however by doing this the de velopers can unknowingly discard algorithmic components that are used in the optimal configuration. in addition the developer of an algorithm has limited knowledge about the instances that a user will typically employ the solver for. that is the very reason why solvers have parameters to enable users to finetune a solver for their spe cific needs. manually tuning such a solver can take a lot of time and effort. be fore even trying the numerous possible parameter settings the user must learn about the inner workings of the solver to understand what each parameter does. furthermore it has even been shown that man ual tuning often leads to highly inferior performance . the field of automatic algorithm configuration which has experi enced a renaissance in the past decade tries to overcome these lim itations of manual parameter tuning. the idea is that the developer implements all alternatives of each algorithm component that can be selected via parameters. then based on a set of representative problem instances an automatic configurator tunes the algorithm by selecting the parameters that yield the best performance. existing techniques select one parameter set that works reason ably well on all instances in the training set. in this work we de this work was supported in part by the national science foundation through the career cornflower project award number . brown university usa email serdarkynmselloktierneycs.brown.edu velop a new type of configurator that provides highquality parameter sets that are based on the specific problem instance that needs to be solved. that is we make algorithm configuration instancespecific. related work . automatic algorithm configuration several approaches exist in the literature for the automatic tuning of algorithms. some of these were created for a specific algorithm or task. for example devises a modular algorithm for solving constraint satisfaction problems csps. using a combination of ex haustive enumeration of all possible configurations and parallel hill climbing the technique automatically configures the system for a given set of training instances. another approach presented in focuses on the configuration of adaptive algorithms employing a se quential parameter optimization approach. other approaches automatically design and build an entire solver to best tackle a set of example training instances. for example uses genetic programming to create an evolutionary algorithm ea. here the chromosome is an ea operation like the selection of par ents mutation or crossover and the task is to find a sequence of the genetic programming operators that is best suited for the speci fied problem. for sat classifies local search ls approaches by means of contextfree grammars. this approach then uses a genetic programming approach to select a good ls algorithm for a given set of instances. there also exist approaches that are applicable to more general al gorithms. for example in order to tune continuous parameters suggests an approach that determines good parameters for individual training instances. this approach first evaluates the extreme param eter configurations and then fits a regression function to map the pa rameterperformance tuple. the minimization of the resulting func tion yields a set of parameters for a given instance. for a small set of possible parameter configurations employs a racing mecha nism. during training all potential algorithms are raced against each other whereby a statistical test eliminates inferior algorithms before the remaining algorithms are run on the next training instance. alter natively the calibra system starts with a factorial design of the parameters. once these initial parameter sets have been run and evaluated an intensifying local search routine starts from a promis ing design whereby the range of the parameters is limited according to the results of the initial factorial design experiments. to date there are only two systems that can configure arbitrary al gorithms with very large numbers of parameters paramils proposed by and gga which was proposed by . paramils conducts an iterated local search whereby a special technique is used to limit the number of training instances that need to be run for each pa rameter set by focusing the test runs on promising parameter sets. as the name suggests gga an abbreviation for genderbased ge netic algorithm conducts a populationbased local search whereby the separation of a competitive and a noncompetitive gender bal ances exploitation and exploration of the parameter space. . algorithm selection algorithm selection is a topic that is closely related to algorithm con figuration. given an instance the objective of the solver is to choose an algorithm that is likely to yield best performance. for example in a sampling technique selects one of several different branch ing variable selection heuristics in a branchandbound approach. proposed to run in parallel or interleaved on a single proces sor multiple stochastic solvers that tackle the same problem. these algorithm portfolios were shown to work much more robustly than any of the individual stochastic solvers. this insight has since then led to the technique of randomization with restarts which is com monly used in all stateoftheart complete sat solvers. on the other hand suggested to use algorithm portfolios in a different way. like before they consider multiple algorithms for the same problem. however in this approach a forecast of the runtime is made for each algorithm for any given input instance based on char acteristic instance features. then the algorithm with the shortest pre dicted runtime is employed. satzilla is a prominent example of this approach for sat. since its initial introduction in satzilla has consistently been ranked highly at the sat competitions . . instancespecific tuning what is interesting about algorithm selection is that it considers the input instance when configuring the solver to pick the correct al gorithm. the limitation of these methods is of course that the given portfolio can only consist of a small set of solvers. compared to al gorithm configuration the number of choices in these portfolios is extremely limited. therefore when an entire family of algorithms as represented by exponentially or even infinitely many parameter settings is given it is no longer possible to learn a prediction model for each different setting. some methods therefore try to integrate the benefits of both ap proaches considering a parameterized solver i.e. an entire family of algorithms in the portfolio and base the selection of parameters instancespecifically according to the features of the input instance. in a selftuning walksat approach is presented that chooses walksat parameters based on the input instance. in another ap proach tackle solvers with continuous and ordinal but not categorical parameters. here bayesian linear regression is used to learn a mapping from features and parameters into a prediction of runtime. based on this mapping for given instance features a pa rameter set which minimizes predicted runtime is searched for. the approach in led to a twofold speedup for the local search sat solver saps. alternatively instead of using regression to map instance fea tures to a parameter configuration introduced the stochastic of fline programming paradigm. this is an iterated three step approach. first the training instances are clustered into distinct sets based on the similarity of their feature vectors. then assuming that instances with alike features behave similarly under the same algorithm local search is used to find good parameters for each cluster of instances. finally the algorithm refines the distance metric in the feature space so that it can find better clusters in future iterations. the entire pro cedure is repeated until no significant improvement or changes are achieved. experiments on the set covering problem showed that the solutions computed by a randomized greedy algorithm can be mas sively improved in this way. the same paper also showed that in this domain regressionbased learning of instancespecific parame ters leads to no improvement over the best instanceoblivious param eters. instancespecific algorithm configuration the objective of this work is to develop a general configurator that can tune any solver and choose solver parameters according to the instance to be solved. based on the limited research that has been conducted on this subject we decided to continue the most success ful approach so far stochastic offline programming which is based on the clustering of instances followed by the computation of high quality parameters for all instances within each cluster. clustering is advantageous for parameter tuning for several rea sons. first training parameters on a collection of instances generally provides more robust parameters than one could obtain when tuning on individual instances. that is tuning on a collection of instances helps prevent overtuning and allows parameters to generalize to sim ilar instances. secondly the found parameters are prestabilized meaning they are proven to work well together. note that this is not the case for the approaches presented in which may provide parameter sets that have never before been run in combination. in order to use clustering a metric in the feature space must be provided. to this end the approach in employs the loss in per formance when using a parameter set computed for one cluster on an instance from another. this works well when improving solution quality of a heuristic for set covering where it is possible to perfectly assess algorithm performance. the situation changes when our ob jective is to minimize runtime. this is because parameter sets that are not well suited for an instance are likely to run for a very long time necessitating the need to introduce a timeout. this then implies that we do not always know the real performance and all we can use is a lower bound on the desired distance between two points in the feature space. this complicates learning a new metric for the feature space. in our experiments for example we found that most instances from one cluster timed out when run with the parameters of another. this not only leads to poor feature metrics but also costs a lot of processing time. consequently for the purpose of tuning the speed of general solvers we suggest a different approach. instead of learning a fea ture metric over several iterations we normalize the features using translation and scaling so that over the set of training instances each feature spans exactly the interval . that is for each feature there exists at least one instance for which this feature has value and at least one instance where the feature value is whereby we discard features which are identical for all training instances. for all other instances the value lies between these two extremes. another issue with the approach from is that it employs k means for clustering. this algorithm first selects k random points in the feature space. it then alternates between two steps until some ter mination criterion is reached. the first step assigns each instance to a cluster according to the shortest distance to one of the k points that were chosen. the next step then updates the k points to the centers of the current clusters. the problem with kmeans clustering is that it requires the user to explicitly specify the number of clusters k. if k is too low this means that we lose some of our potential to tune parameters more precisely for different parts of the instance feature space. on the other hand if there are too many clusters we sacrifice the robustness and generality of the parameter sets that we optimize for these clusters. furthermore for a mixed set of training instances it is unreasonable to assume that the value of k is known. gmeansx k i c s kmeansx k while i k do c s kmeanssi v c c w p vi yi p vixiw if andersondarlingtesty failed then ci c si s k k ck c sk s else i i end if end while return c s k algorithm gmeans clustering algorithm we address this issue by using gmeans a clustering algorithm proposed in which automatically determines the number of clus ters. proposes that a good cluster exhibits a gaussian distribution around the cluster center. the algorithm presented in algorithm first considers all inputs as forming one large cluster. in each itera tion we pick one of the current clusters and try to assess whether it is already sufficiently gaussian. to this end gmeans splits the cluster in two by running means clustering. we can then project all points in the cluster onto the line that runs through the centers of the two subclusters obtaining a onedimensional distribution of points. gmeans now checks whether this distribution is normal using the widely accepted statistical andersondarling test. if the current cluster does not pass the test it is split into the two previously com puted clusters and we continue with the next cluster. we found that the gmeans algorithm works very well for our pur poses. the only problem we encountered was that sometimes clus ters can be very small containing very few instances. to obtain ro bust parameter sets we do not allow clusters that contain fewer than a manually chosen threshold a value which depends on the size of the data set. beginning with the smallest cluster we redistribute the corresponding instances to the nearest clusters where proximity is measured by the euclidean distance of each instance to the clusters center. in summary our approach works as follows see algorithm . in the learning phase we are provided with the parameterized solver a a list of training instances t and their corresponding feature vec tors f . first we normalize the features in the set and memorize the scaling and translation values for each feature s t. then we use the gmeans algorithm to cluster the training in stances based on the normalized feature vectors. the resulting small clusters with too few instances are redistributed to larger clusters as discussed above. the final result of the clustering is a number of k clusters si a list of cluster centers ci and for each cluster a dis tance threshold di which determines when a new instance will be considered as close enough to the cluster center to be solved with the parameters computed for instances in this cluster. then for each cluster of instances si we compute favorable pa rameters pi using the instanceoblivious tuning algorithm gga. af ter this is done we compute parameter set r for all the training in stances. this serves as the recourse for all future instances that are not near any of the clusters. we use gga because it is one of the most competitive and ro bust tuners available able to handle any type of parameter. also compared gga to paramils the only other viable option and isaclearna t f f s t normalizef k c s d cluster t f for all i . . . k do pi ggaa si end for r ggaa t return k p c d s t r isacruna x k p c d s t r f featuresx fi fi tisi i for all j . . . k do if f ci di then return ax pi end if end for return ax r algorithm instancespecific algorithm configuration showed significant gains in performance and robustness for gga over paramils. when running algorithm a on an input instance x we first com pute the features of the input and normalize them using the previ ously stored scaling and translation values for each feature. then we determine whether there is a cluster such that the normalized fea ture vector of the input is close enough to the cluster center. if so we run a on x using the parameters for this cluster. if the input is not near enough to any of our clusters we use the instanceoblivious parameters r that work well for the entire training set. specifically in our experiments an instance was considered too far away if it was more than the average distance plus two standard deviations of the distance of all points in the cluster to its center. this tended to be only of the test instances. numerical results . set covering we begin our empirical evaluation on one of the most studied com binatorial optimization problems the set covering problem scp. in scp given a finite set s . . . m of items a family f s . . . sn s of subsets of s and a cost function c f r the objective is to find a subset c f such that s s sic si and p sic csi is minimized. in the unicost scp the cost of each set is set to one. this problem formulation appears in numerous practical applications such as crew scheduling location of emergency facilities and production planning in var ious industries . solvers we consider three different solvers. the first is the greedy randomized set covering heuristic gs from . gs repeatedly adds sets one at a time until reaching a feasible solution. during the construction of the cover a probability distribution is used to specify the set selection heuristic at each step. the other two solvers are state oftheart local search scp solvers. the dialectic search algorithm hegel was introduced in and the tabu search algorithm ts was introduced in which is restricted to unicost instances. benchmark a highly diverse set of randomly generated set cover ing instances was introduced in . these instances involve up to items and sets. we precompute the optimal values of these instances. when tuning ts we set the cost of each set uni formly to to achieve unicost instances. the final data set comprises training instances and test instances. instance features the generation of a feature vector for each scp was done according to the process outlined in . this process first computes the following normalized cost vector c m vector of bag densities sini...m vector of item costs p ijsi c ij...n vector of item coverings i j simj...n vector of costs over density cisii...m vector of costs over square density cisii...m vector of costs over k log kdensity c i si log si i...m and vector of rootcosts over square density p cisi i...m. the final feature vector is then formed by computing the maxima minima averages and standard deviations of all these vectors. com putation of these feature values on average took only . seconds per instance. numerical results unless otherwise noted experiments were run on dual processor dual core intel xeon . ghz computers with gb of ram. scp solvers hegel and ts were evaluated on quad core dual processor intel xeon . ghz processors with gb of ram. the first objective of our experiments is to compare isac with the stochastic offline programming sop approach from . when deterministically using the single best greedy heuristic gs leaves on average a . . optimality gap on the training testing data . it is possible to shrink this gap using a uniform distribu tion over the six set selection heuristics used in gs. we refer to this default version of the gs as uniform. in a configurator was developed which could compute highquality instanceoblivious gs parameters. we refer to this approach as sopcombined. moreover showed that clustering the training data into sets and then tun ing these sets individually could lead to further improvements. we refer to this approach as sopclustered. we compare these two con figurators with generalpurpose instanceoblivious configuration of gga and instancespecific parameter tuning of isac. solver gs optimality gap closed train test uniform . . . sop combined . . . . sop clustered . . . . gga . . . . isac . . . . table . comparison of isac versus the default and instanceoblivious parameters provided by sop and gga and the instancespecific parameters provided by sop. we present the percent of optimality gap closed stdev. in table we compare the resulting five gs solvers presenting the percentage of optimality gap closed by each solver. comparing the average percent of optimality gap closed we find that isac is as capable of improving over the default approach as sop which was developed particularly for the gs solver. that is isac can effec tively liberate us from having to select the number of clusters while at the same time enjoying wide applicability to other solvers. more over isac works more efficiently than sop since it does not require multiple reclustering steps for learning a metric in the instance fea ture space. table shows a clear distinction between the instancespecific and the instanceoblivious tuning methods. both isac and sop clustered perform significantly better than gga and sopcombined. instancespecific tuning is best realized by isac closing the opti mality gap by more than on average. we next evaluate isac on two stateoftheart local search scp solvers hegel and ts. for both solvers we measure the time to find a set covering solution that is within of optimal. hegel and ts had a timeout of seconds for training and testing. in table we compare the default configuration of the solvers the instanceoblivious configuration obtained by gga and the instance specifically tuned versions provided by isac. to provide a more holistic view of isacs performance we present three performance metrics the arithmetic and geometric means of the runtime in sec onds and the average slow down the arithmetic mean of the ratios of the performance of the competing solver over isac. for these experiments we set the minimum cluster size to in stances. this setting resulted in clusters of roughly equal size. solver avg. run time geo. avg. avg. slow down train test train test train test ts default . . . . . . gga . . . . . . isac . . . . . . hegel default . . . . . . gga . . . . . . isac . . . . . . table . comparison of default instanceoblivious parameters provided by gga and instancespecific parameters provided by sop for hegel and ts. we present the arithmetic and geometric mean runtimes in seconds and the average degradation when comparing each solver to isac. we first observe that the default configuration of both solvers can be improved significantly by automatic parameter tuning. for solver ts we measure an arithmetic mean runtime of . seconds for isacts . seconds for ggats and . seconds for de fault ts. that is instanceoblivious parameters run slower than instancespecific parameters. for hegel we find that the default ver sion runs more than slower than isachegel. it is worth noting that we observe a high variance of the runtimes from one instance to another which is caused by the diversity of our benchmark. to get a better understanding we also compute the aver age slow down of each solver when compared with the correspond ing isac version. for this measure we find that for an average test instance default ts requires more than . times the time of isac ts and ggats needs . times over isacts. for default hegel an average test instance takes . times the time of isachegel while ggahegel only runs slower. this confirms the findings in that hegel runs robustly over different instance classes with one set of parameters. we conclude that even highly sophisticated stateoftheart solvers can greatly benefit from automatic parameter tuning. de pending on the solver instancespecific parameter tuning works as well or significantly better than instanceoblivious tuning. note that this is not selfevident since the instancespecific approach runs the risk of overtuning by considering fewer instances per cluster. in our experiments we do not observe these problems. instead we find that our instancespecific algorithm configurator offers the potential for great performance gains without overfitting the training data. . mixed integer programming we next consider mixed integer programming problems mips. mips involve optimizing a linear objective function while obeying a collection of linear inequalities and variable integrality constraints. mixed integer programming is an area of great importance in opera tions research. solver the fastest and best known mip solver is ibm cplex . for years it has represented the stateoftheart in mip solving. the solver is ideal for our purposes because it is highly parameterized allowing the user to precisely choose the settings they think are best suited for their mip instances. for these experiments we use cplex .. benchmark we assembled a highly diverse benchmark data set composed of problem instances from six different sources. net work flow instances capacitated facility location instances bounded and unbounded mixed knapsack instances and capacitated lot sizing problems all taken from as well as combinatorial auction in stances from . in total there are instances in this set which was split into training and test instances. instance features even though solving mips is an active field to the best of our knowledge no prior research exists on the type of features that can be used to classify a mip instance. we therefore propose to use the information about the objective vector the right hand side rhs vector and the constraint matrix to formulate our feature vector. we first compute the following values number of variables and number of constraints percentage of binary integer or continuous variables percentage of variables all integer or continuous with non zero coefficients in the objective function and percentage of or constraints. we also use the mean min max and standard deviation of the following vectors where z xi xi is restricted to be integer r xi xi is real valued and u z r vector of coefficients of the objective function of all integer or continuous variables cixi x where x u x z x r vector of rhs of the or constraints bj ajx bj where vector of number of variables all integer or continuous per con straint j aij aij xi x where x ux z x r vector of the coefficients of variables all integer or continuous per constraint j p i aijj xi x where x u x z x r and vector of the number of constraints each variable i all integer or continuous belongs to aij aij xi x where x u x z x r. computation of these feature values on average took only . seconds per instance. numerical results experiments were carried out with a timeout of seconds for training and seconds for evaluation on the train ing and testing sets. we set the size of the smallest cluster to be instances. this resulted in clusters where consisted of only one problem type and cluster combined network flow and capacitated lot sizing instances. solver avg. run time geo. avg. avg. slow down train test train test train test cplex default . . . . . . gga . . . . . . isac . . . . . . table . comparison of isac versus the default and the instanceoblivious parameters provided by gga when tuning cplex. we present the arithmetic and geometric mean runtimes as well as the average slowdown per instance. table compares instancespecific isac with instanceoblivious gga and the default settings of cplex. we observe again that the default parameters can be significantly improved by tuning the algo rithm for a representative benchmark. on average isaccplex needs . seconds ggacplex needs . seconds and default cplex re quires . seconds. instanceobliviously tuned cplex is slower and default cplex even more than slower than isaccplex. the improvements achieved by automatic parameter tuning can also be seen when considering the average perinstance slowdowns. according to this measure for a randomly chosen instance in the test set we expect that ggacplex needs more time than required by isaccplex. default cplex even needs more time than isac cplex. we would like to note that due to license restrictions we could only use a very small training set of instances which is very few given the high diversity of the considered benchmark. taking this into ac count and seeing that cplex is a highly sophisticated and extremely welltuned solver the fact that isac boosts performance so signifi cantly is surprising and shows the great potential of instancespecific tuning. . satisfiability our final evaluation of isac is on the propositional satisfiability problem sat the prototypical npcomplete problem that has far reaching effects on many areas of computer science. for sat given a propositional logic formula f in conjunctive normal form the ob jective is to determine whether there exists a satisfying truth assign ment to the variables of f . in recent years there has been tremen dous progress in solving sat problems so that stateoftheart sat solvers can now tackle instances with hundreds of thousands of vari ables and over one million clauses. solvers we test isac on the highly parameterized stochastic local search solver saps . unlike most existing sat solvers saps was originally designed with automatic tuning in mind and therefore all of the parameters influencing the solver are readily accessible to users. furthermore since it was first released the default parame ters of the solver have been drastically improved by general purpose parameter tuners . benchmarks we consider the collection of sat instances de scribed in table . qcp is a collection of quasigroup completion problems swgcp contains smallworld based graph coloring prob lems satrandom has sat instances created using the g gen erator and satstructured are instances that are modeled to mimic real world search problems. data set train test ref. qcp swgcp satrandom satstructured table . data sets used to evaluate isac on sat. instance features we utilize the features proposed by to clas sify each problem instance. we find that while the local search fea tures mentioned in take a considerable amount of time to com pute they are not imperative to finding a good clustering of instances. consequently we exclude them and use only the following problem size features number of clauses c number of variables v and their ratio cv variableclause graph features degree statistics for variable and clause nodes variable graph features node degree statistics balance features ratio of positive to negative literals per clause ratio of positive to negative occurrences of each variable fraction of binary and ternary clauses proximity to horn clauses fraction of horn clauses and statistics on the number of occurrences in a horn clause for each variable unit propagations at depths and on a random path in the dpll search tree and search space size estimate mean depth to contradiction and esti mate of the log of number of nodes. computation of these feature values on average took only . seconds per instance. numerical results experiments were carried out with a timeout of seconds for training and seconds for evaluation on the training and testing sets. we set the size of the smallest cluster to be at least instances. this resulted in clusters each with roughly instances. here not only were all of the types of instances correctly separated into distinct clusters a further partition of instances from the same class was provided. we evaluate the performance of saps using the default parame ters gga and isac and present the results in table . solver avg. run time geo. avg. avg. slow down train test train test train test saps default . . . . . . gga . . . . . . isac . . . . . . table . comparison of the saps solvers with default gga tuned and isacsaps. the arithmetic and geometric mean runtimes in seconds are presented as well as the average slowdown per instance. even though the default parameters of saps have been tuned heavily in the past tuning with gga solves the benchmark over times faster than default saps. instancespecific tuning allows us to gain another factor of . over the instanceoblivious parameters resulting in a total performance improvement of over one order of magnitude. this refutes the conjecture of that saps may not be a good solver for instancespecific parameter tuning. it is worth noting that over of instances in this benchmark can be solved in under seconds. consequently some exception ally hard longrunning instances greatly dilute the average runtime. we therefore present again the average slowdown per instance. for the average sat instance in our test set default saps runs times slower than isac. even if we use gga to tune a parameter set specifically for this benchmark gga is still expected to run almost times slower than isac. conclusion in this paper we presented isac a new automatic algorithm config urator that provides highquality parameter sets based on instance specific information. the proposed approach has two major steps. first employing normalized features the training instances are clus tered using gmeans a clustering algorithm that automatically deter mines the appropriate number of clusters. we assume that instances with similar features are likely to behave similarly when solved by the same solver. therefore the second step uses the instance oblivious algorithm configurator gga to find the best parameters for each cluster. at runtime when a new instance needs to be solved we determine the cluster that is closest to the input instance feature vector and then solve the instance with the parameters for the respec tive cluster. for instances that are very far from all clusters we use instanceoblivious parameters obtained by gga on the entire set of training instances. to our knowledge isac is the first instancespecific configura tion algorithm that can handle a large number and any type of pa rameters continuous ordinal as well as categorical. we evaluated isac on five highperformance solvers for three different problems set covering mixed integer programming and satisfiability. in all cases we found that automatic algorithm configuration could boost the performance of the default solvers including the cutting edge solvers hegel for set covering and cplex for mixed integer program ming. moreover we found that instancespecific tuning never works worse than instanceoblivious configuration. on the contrary isac outperformed the instance oblivious tuner gga in all cases for most solvers quite substantially. references b. adensodiaz and m. laguna. finetuning of algorithms using fractional experimental design and local search. operations research . c. a. gil m. sellmann and k. tierney. a genderbased genetic algorithm for the automatic configuration of algorithms. cp springer lncs pp. . m. birattari t. stuetzle l. paquete and k. varrentrapp a racing algorithm for configuring metaheuristics. gecco . a. caprara m. fischetti p. toth d. vigo and p.l. guida. algorithms for railway crew management. mathematical programming . s.p. coy b.l. golden g.c. runger and e.a. wasil. using experimental de sign to find effective parameter settings for heuristics. journal of heuristics . ibm. ibm cplex reference manual and user manual. v. ibm . m. davis g. logemann d. loveland. a machine program for theorem proving. commun. acm . a. fukunaga automated discovery of local search heuristics for satisfiability testing. evolutionary computation . i. p. gent h. h. hoos p. prosser and t. walsh. morphing combining structure and randomness. aaai . c.p. gomes and b. selman. problem structure in the presence of perturbations. aaai . c.p. gomes b. selman. algorithm portfolios. artificial intelligence . g. hamerly and c. elkan. learning the k in kmeans. nips . e. housos and t. elmoth. automatic optimization of subproblems in schedul ing airline crews. interfaces . k.l. hoffmann and m.w. padberg. solving airline crew scheduling problems by branchandcut. management science . f. hutter and y. hamadi. parameter adjustment based on performance predic tion towards an instanceaware problem solver. technical report msrtr microsoft research cambridge uk . f. hutter y. hamadi h.h. hoos and k. leytonbrown. performance prediction and automated tuning of randomized and parametric algorithms. cp pp. . f. hutter h.h. hoos k. leytonbrown and t. stuetzle. paramils an auto matic algorithm configuration framework. jair volume pages . b. huberman r. lukose and t. hogg. an economics approach to hard com putational problems. science . f. hutter d.a.d. tompkins and h.h. hoos. scaling and probabilistic smooth ing efficient dynamic local search for sat. cp . s. kadioglu and m. sellmann. dialectic search. cp springer lncs . k. leytonbrown m. pearson and y. shoham. towards a universal test suite for combinatorial auction algorithms. proceedings of the acm conference on electronic commerce acmec pp. . l. lobjois and m. lematre. branch and bound algorithm selection by perfor mance prediction. aaai pp. . y. malitsky and m. sellmann. stochastic offline programming. ictai . s. minton. automatically configuring constraint satisfaction programs. con straints . a. saxena. mip benchmark instances. httpwww.andrew.cmu.edu useranureetsmpsinstances.htm. m. motoki and r. uehara. unique solution instance generation for the satisfiability sat problem.in sat pages . n. musliu. local search algorithm for unicost set covering problem. ieaaie . m. oltean. evolving evolutionary algorithms using linear genetic program ming. evolutionary computation . d. j. patterson and h. kautz. autowalksat a selftuning implementation of walksat. electronic notes in discrete mathematics volume pp. . m. preuss and t. bartzbeielstein. sequential parameter optimization applied to selfadaptation for binarycoded evolutionary algorithms. parameter set ting in evolutionary algorithms studies in computational intelligence . sat competition. httpwww.satcomptition.org. a. slater. modelling more realistic sat problems. th australian joint conference on artificial intelligence lnai pp. . c. toregas r. swain c. revelle and l. bergman. the location of emergency service facilities. operational research . f.j. vasko and f.e. wolf. optimal selection of ingot sizes via set covering. operations research . l. xu f. hutter h. h. hoos and k. leytonbrown. satzilla portfoliobased algorithm selection for sat. jair volume pages .  latent features for algorithm selection latent features for algorithm selection yuri malitsky and barry osullivan insight centre for data analytics department of computer science university college cork ireland yuri.malitsky barry.osullivaninsightcentre.org abstract the success and power of algorithm selection tech niques has been empirically demonstrated on numerous occasions most noticeably in the competition settings like those for sat csp maxsat qbf etc. yet while there is now a plethora of competing approaches all of them are dependent on the quality of a set of structural features they use to distinguish amongst the instances. over the years each domain has defined and refined its own set of features yet at their core they are mostly a collection of everything that was considered useful in the past. as an alternative to this shotgun generation of features this paper instead proposes a more systematic approach. specifically the paper shows how latent fea tures gathered from matrix decomposition are enough for a linear model to achieve a level of performance comparable to a perfect oracle portfolio. this informa tion can in turn help guide researchers to the kinds of structural features they should be looking for or even just identifying when such features are missing. introduction over the last decade algorithm selection has led to some very impressive results in a wide variety of fields like satis fiability sat xu et al. b kadioglu et al. con straint satisfaction csp omahony et al. and ma chine learning thornton et al. . the underlying princi ple guiding this research is that there is no single algorithm that performs optimally on every instance. that whenever a solver is developed to perform well in the general case it can only do so by sacrificing quality on some instances. algorithm selection therefore studies each instances struc tural properties through a collection of numeric features to try to predict the algorithm that will perform best on it. the ways in which these decisions are made is constantly grow ing with only a small number referred to in the following sections but the results have been dominating international competitions of sat csp maxsat and others over the last few years. for a general overview of some of the portfolio methodologies we refer the reader to a recent literature re view kotthoff gent and miguel . copyright c association for the advancement of artificial intelligence www.aaai.org. all rights reserved. to get a better handle on this field consider the semian nual sat competition domain where in there were solvers submitted by research groups. if we consider just the top solvers and evaluate each with a sec ond timeout on instances we would observe that the best solver can find a solution to instances or .. ignoring the runnerup which was an alternate version of the winner the next best solver could only solve in stances .. however if we instead introduce an ora cle solver that would always choose the fastest solver for each instance an additional instances could be solved more than the best solver. furthermore if we look at the solvers chosen by this oracle in this particular case we would observe that the best solver is never chosen. this means that while the best solver is good overall it is never the fastest for any given instance. closing this gap between the best single solver and the oracle is the objective of algo rithm selection. yet while the performance of algorithm selection tech niques is continually improving it does so at the cost of transparency of the employed models. the new version of satzilla the winning portfolio in trains a tree to pre dict the winner between every pair of solvers xu et al. b. cshc the winner takes an alternate approach of introducing a new splitting criterion for trees that makes sure that each subtree is more consistent on the preferred solver than its root malitsky et al. . but in order to make the approach competitive many of these trees need to be grouped into a forest. yet other approaches create sched ules of solvers to improve the chances of solving each in stance kadioglu et al. helmert roger and karpas . unfortunately even though all these approaches are highly effective at solving instances once they are trained they are nearly impossible to use to get a better understand ing of the fundamental issues of a particular problem do main. in short we can now answer what we should do when we see a new instance the new question should therefore be why a particular solver is chosen and we should use this in formation to spur the development of a new wave of solvers. the focus of this paper is therefore to present a new port folio approach that can achieve similar performance to lead ing portfolios while also presenting a human interpretable model. specifically this is done with the help of matrix de composition. the paper shows that instead of using the cur proceedings of the seventh annual symposium on combinatorial search socs rent set of features which have been generated sporadically and manually by including anything that may be of use it is necessary to instead turn to latent features generated by studying the matrix detailing the performance of each solver on each instance. it is argued that these latent features cap ture the differences between solvers by observing their ac tual performances. it is then shown that a portfolio trained on these features can significantly outperform the best existing approaches. of course because in the regular setting these features are not actually available the paper shows how the currently existing features can be used to approximate these latent features. this has the additional benefit of revealing the absence of some crucial information if the existing fea tures are unable to accurately predict a latent feature. this information can in turn be used to guide future research in getting a better understanding of what it is exactly that causes one solver to outperform another. the remainder of the paper is broken down into three sec tions. first we present an assortment of algorithm selection techniques and apply them to problems in three separate do mains. the section therefore demonstrates both the effec tiveness and the drawbacks of current techniques. the fol lowing section then presents the concept of latent features and how they are computed using singular value decompo sition. this section goes into detail about the strengths and applications of the new approach. the paper concludes with a general discussion of the further avenues of research that become accessible using the described technique. algorithm selection although the term was first introduced by rice in rice it is only recently that the study of algorithm selection has begun to take off. a survey by kotthoff et.al. touches on many of the recent works kotthoff gent and miguel . unfortunately at this time many of the top approaches are either not open source or are very closely linked to a spe cific dataset or format. this makes an exhaustive compari son with the state of the art impractical. there are currently a number of steps being taken to unify the field such as the configuration and selection of algorithms coseal project coseal but this paper will instead focus on the standard machine learning approaches to algorithm selection. yet as was recently shown hutter et al. one of the most effective approaches is to rely on a random forest to predict the best performing solver. this random for est approach is one of the techniques presented here. this section will first introduce the three problem do mains that will be explored in the paper and the features used to analyze them. the section will then proceed to present the results of standard algorithm selection techniques and the re sults of feature filtering. all presented experiments were run on two x intel xeon e processors . ghz. satisfiability satisfiability testing sat is one of the premier domains where algorithm portfolios have been shown to excel. ever since satzilla xu et al. was first entered in the sat competition a portfolio approach has always placed in each subsequent competition. sat tackles the problem of assigning a truth value to a collection of boolean variables so as to satisfy a conjunction of clauses where each clause is a disjunction of the variables. it is a classical npcomplete problem that is widely studied due to its application to for mal verification scheduling and planning to name just a few. since the set of features used to describe the sat instances has been steadily expanding to now number in total. these features are meant to capture the structural properties of an instance like the number of variables num ber of clauses number of variables per clause frequency with which two variables appear in the same clause or two clauses share a variable. these features also include a number of stochastic features gathered after running a few solvers for a short time. these features include the number of propagations the number of steps to a local optimum and the number of learned clauses. in the interest of space we refer the reader to the official paper xu et al. a for a complete list of these features. yet as expansive as this list is it was generated by including everything that was thought useful resulting in many features that are not usually very informative and are usually removed through feature filter ing kroer and malitsky . the dataset investigated in this paper is comprised of industrial instances gathered from sat competitions dating back to . we focus solely on industrial instances as these are of more general interest in practice. each in stance was evaluated with the top solvers in the competition with a timeout of seconds. these solvers are clasp... jumpy clasp.. trendy ebminisat gluem inisat lingeling lrglshr picosat restartsat circminisat clasp cryptominisat eagleup gnoveltyp march rw mphasesat mphasesatm precosat qutersat sapperlot satj... sattimep sparrow tnm cryptominisat min isatpsm sattime ccasat and glucose . maxsat maxsat is the optimization version of the sat problem aiming to find a variable setting that maximizes the num ber of satisfied clauses. with applications in bioinformat ics timetabling scheduling and many others the problem domain is of particular interest for research as it can rea son about both optimality and feasibility. the particular dataset used in this paper focuses on the regular and partial weighted maxsat wms and wpms respectively prob lem. this means that the problem clauses are split and clas sified as either hard or soft. the objective of a wpms solver is to satisfy all of the hard clauses while also maximizing the cumulative weight of the satisfied soft clauses. wms has a similar objective except that all the soft clauses have the same weight. the investigated dataset is comprised of instances gathered from the maxsat evaluation. each of the instances is identified by a total of features intro duced as part of a recently dominating maxsat portfo lio ansotegui malitsky and sellmann . these are a subset of the deterministic features used to clas sify sat instances. the other features measure the table performance of algorithm selection techniques on sat maxsat and csp datasets. the algorithms are compared using the average runtime avg the timeout penalized runtime par and the number of instances not solved ns. sat maxsat csp avg par ns avg par ns avg par ns bss tree linear svm radial knn . forest . vbs . . percentage of clauses that are soft and the statistics of the distribution of weights avg min max stdev. each of the instances is evaluated with top solvers from for seconds. the solvers are ahmaxsat ak maxsat ls ckmax small ilp maxsatzf msun core scip maxsat shinmaxsat wmaxsatz wmaxsatz wpm wpm wpmshuc b qmaxsat m pwbo. and six parameterizations of wpm found in ansotegui malitsky and sellmann . constraint satisfaction a constraint satisfaction problem csp is a powerful for mulation that can model a large number of practical prob lems such as planning routing and configuration. such problem instances are represented by a set of variables each with their own domain of possible assignments. the actual assignments are determined by adherence to a set of con straints. a typical example of such a constraint is the all different constraint which symbolizes that no two variables take the same value. in this paper we consider the instances used to train the proteus portfolio approach hurley et al. . this is a subset of instances from the csp solver competition containing graph coloring random quasirandom black hole quasigroup completion quasigroup with holes langford towers of hanoi and pigeon hole problems. each instance was solved with a second timeout by the four top solvers mistral gecode choco and abscon. a total of features were used for each instance using mistral hebrard . the set includes static features like statistics about the types of constraints used average and maximum domain size and dynamic statistics recorded by running mistral for seconds average and standard deviation of variable weights number of nodes number of propagations and a few others. numerical evaluation as a baseline for each of the datasets described above we present the performance of the best single solver bss as well as that of the virtual best performance of an oracle virtual best solver vbs. we then evaluate a number of machine learning techniques for algorithm selection each csp solver competition instances httpwww.cril.univartois. frlecoutrebenchmarks.html of which will try to predict the runtime of each solver se lecting the expected best. the evaluations are done using llama kotthoff a freely available r package for fast prototyping of algorithm selection techniques. addi tionally the parameters of each of the machine learning ap proaches were tuned using the r caret kuhn library which uses grid based search and cross validation to find the best parameters for each technique. table presents a comparison of a small subset of algo rithm portfolio techniques on the three datasets. here all of the algorithms are evaluated using fold cross valida tion presenting the average runtime avg the timeout pe nalized average runtime par and the number of not solved instances ns. note that for avg unsolved tasks are counted with the timeout. for par if a solver times out on an instance the time taken is recorded as times the timeout otherwise the regular time is recorded. the ta ble compares the performances of training a tree a linear model a support vector machine with a radial basis kernel a knearest neighbor and a random forest consisting of trees. for these particular datasets the instances where no solver finished within the timeout are removed hence the unsolved instances for vbs. note that in the presented results all approaches perform better than any single solver highlighting once again the power of algorithm selection. note also that among the cho sen approaches the tree and linear models are arguably the most easily interpretable. yet they are not the best perform ing portfolios so it is not clear whether it is safe to make judgments based on their results. after all it is impossible to tell why they get certain instances wrong. it would be much better if the predictions of the preferred solver were more accurate. on the other end of the spectrum knearest neighbor and a random forest are much stronger predictors. forests in par ticular are now commonly referred to in the literature as the best approach for algorithm selection. yet as a price for this improved performance the resulting models are increas ingly opaque. looking at the generated trees it is im possible to say why one solver is better than another for a particular type of instance or the exact changes among features that highlight this difference. the same is true for knearest neighbor. of course it is possible to look at all the neighboring instances that impact a solvers decision but this only highlights the potentially similar instances not table performance of algorithm selection techniques after feature filtering on sat maxsat and csp datasets. the algo rithms are compared using the average runtime avg the timeout penalized runtime par and the number of instances not solved ns. sat maxsat csp avg par ns avg par ns avg par ns bss tree linear svm radial . knn . forest . vbs . . specifically what those differences are. feature filtering in practice it is well accepted that dealing with problems with large feature vectors is often illadvised. one of the reasons for this is that the more numerous the feature set the more instances are needed to differentiate useful data from noise that happens to look beneficial. after all in a collection of thousands randomly generated features and only one hun dred instances there is a high probability that at least one of the features will correspond with the target value. this issue is mitigated through the use of feature filtering. there are numerous techniques designed to isolate and remove unhelpful or even adverse features. in this paper we utilize four common filtering techniques made available by the fselector r library romanski chisquared in formation gain gain ratio and symmetrical uncertainty. be cause all these approaches depend on a classification for each instance we use the name of the best solver for that purpose. chisquared. the chisquared test is a correlationbased filter and makes use of contingency tables. one advantage of this function is that it does not need the discretization of continuous features. it is defined as ij mij mijmij where mij mi.m.jm mij is the number of times instances with target value y yj and feature value x xi appear in a dataset where y are the class and x are features. here m denotes the total number of instances. information gain. information gain is based on information theory and is often used in decision trees and is based on the calculation of entropy of the data as a whole and for each class. for this ranking function continuous features must be discretized in advance. gain ratio. this function is a modified version of the infor mation gain and it takes into account the mutual informa tion for giving equal weight to features with many values and features with few values. it is considered to be a stable evaluation. symmetrical uncertainty. the symmetrical uncertainty is built on top of the mutual information and entropy measures. it is particularly noted for its low bias for multivalued fea tures. each of these filtering techniques assigns a relevance score to each of the features with higher values signifying greater information quality. to select the final subset of fea tures we order the features based on these scores and use the biggest difference drop as a place to cut the important from the remainder. in the interest of space table only shows the perfor mances of the algorithm selection techniques after the most empirically effective approach chisquared filtering. ap plying the filtering technique on the datasets certainly has the desired effect of maintaining or even improving per formance while reducing the feature set. in the case of sat we observe that only of the features are needed. for maxsat the reduction is smaller of while for csp it is of . yet as can be clearly seen from the table while these filtering techniques can focus us on more informative features it does not greatly improve the performance of the transparent models leaving the opaque random forest model the clear winner. latent features a latent variable is by definition something that is not di rectly observable but rather inferred from observations. this is a concept that is highly related to that of hidden variables and is employed in a number of disciplines including eco nomics rutz bucklin and sonnier medicine yang et al. and machine learning ghahramani griffiths and sollich . this section introduces the idea of col lecting latent variables that best describe the changes in the actual performance of solvers on instances. thereby in stead of composing a large set of structural features that might possibly correlate with the performance this paper proposes a top down approach. specifically the paper pro poses that matrix factorization like singular value decompo sition svd can be used for this purpose. singular value decomposition the ideas behind singular value decomposition herald back to the late s when they were independently discov ered by five mathematicians eugenio beltrami camille jor dan james joseph sylvester erhard schmidt and hermann table performance of algorithm selection techniques using the latent features computed after singular value decomposition on sat maxsat and csp datasets. the algorithms are compared using the average runtime avg the timeout penalized runtime par and the number of instances not solved ns. we therefore observe that a linear model using the svd features could potentially perform as well as an oracle. sat maxsat csp avg par ns avg par ns avg par ns bss standard portfolio forest orig . vbs . . tree . linear . . svd based portfolio svm radial . knn . forest . . weyl. in practice the technique is now currently embraced for tasks like image compression prasantha and data mining martin and porter by reducing massive sys tems to manageable problems by eliminating redundant in formation and retaining data critical to the system. at its essence singular value decomposition is a method for identifying and ordering the dimensions along which data points exhibit the most variation which is mathemat ically represented by the following equation m uv t where m is the mn matrix representing the original data. here there are m instances each described by n values. the columns of u are the orthonormal eigenvectors of mmt the columns of v are orthonormal eigenvectors of mtm and is a diagonal matrix containing the square roots of eigenvalues from u or v in descending order. note that if m n then being a diagonal matrix most of the rows in will be zeros. this means that after multi plication only the first n columns of u are needed. so for all intents and purposes for m n u is an m n matrix while both and v t are n n. because u and v are orthonormal intuitively one can in terpret the columns of these matrices as a linear vector in the problem space that captures most of the variance in the orig inal matrix m . the values in then specify how much of the variance each column captures. the lower the value in the less important a particular column is. this is where the concept of compression comes into play when the amount of columns in u and v can be reduced while still capturing most of the variability in the original matrix. from the perspective of data mining the columns of the u matrix and the rows of the v matrix have an additional interpretation. let us assume that our matrix m records the performance of n solvers over m instances. in such a case it is usually safe to assume that m n. so each row of the u matrix still describes each of the original instances in m . but now each column can be interpreted as a latent topic or feature that describes that instance. meanwhile each col umn of the v t matrix refers to each solver while each row presents how active or important a particular topic is for that solver. these latent features in u give us exactly the information necessary to determine the runtime of each solver. this is because once the three matrices are multiplied out we are able to reconstruct the original performance matrix m . so if we are given a new instance i if we are able to identify its latent features we could multiply by the existing and v t matrices to get back the performance of each solver. therefore if we had the latent features for an instance as computed after the singular value decomposition it would be possible to train a linear model to accurately predict the performance of every solver. a linear model where we can see exactly which features influence the performance of each solver. this is exactly what table presents. like before here we perform fold cross validation. for each train ing set we compute matrices u v and and train each model to use the latent features in u to predict the solver performances. for the test instances we use the runtimes p to compute what the values of u should be by computing pv . these latent features are then used by the trained models to predict the best solver. unfortunately these latent features are only available by decomposing the original performance matrix. this is infor mation that we only have available after all the solvers have been run on an instance. information that once computed means we already know which solver should have been run. yet note that the performance of the models is much bet ter than it was using the original features especially for the linear model. this is again a completely unfair comparison but it is not as obvious as it first appears. what we can gather from these results is that the matrix v and are still rele vant even when applied to previously unseen instances. this means that the differences between solvers can in fact be differentiated by a linear model provided it has the correct structural information about the instance. this also means that if we are able to replicate the latent features of a new instance the supporting matrices computed by the decom position will be able to establish the performances. recall also that the values in are based on the eigenval ues of m . this means that the columns associated with the lower valued entries in encapsulate less of the variance in the data than the higher valued entries. figure therefore shows the performance of the linear model as more of the table performance of an algorithm selection technique that predicts the latent features of each instance using a random forest on the sat maxsat and csp datasets. the algorithms are compared using the average runtime avg the timeout penalized runtime par and the number of instances not solved ns. note that even using predicted latent features a linear model of svd predicted can achieve the same level of performance as the more powerful but more opaque random forest. sat maxsat csp avg par ns avg par ns avg par ns bss forest orig . vbs . . svd predicted . figure number of unsolved instances remaining after us ing a linear model trained on the latent features after sin gular value decomposition. the features were removed with those with lowest eigenvalues first. the data is collected on the wpms dataset. this means that even removing over half the latent features a portfolio can be trained that solves all but instances. latent features are removed under the wpms dataset. we just use the wpms dataset for the example because the csp dataset only has solvers and the results for the sat dataset are similar to those presented. note that while all of the la tent features are necessary to recreate the performance of the vbs it is possible to remove over half the latent features and still be able to solve all but instances. estimating latent features although we do not have direct access to the latent features for a previously unseen instance we can still estimate them using the original set of features. note that while it is possible to use the result of v as a means of computing the final times training a linear model on top of the latent features is the better option. true both approaches would be performing a linear transformation of the features but the linear model will also be able to auto matically take into account any small errors in the predic tions of the latent features. therefore the method proposed in this section would use a variety of models to predict each latent feature using the original features. the resulting pre dicted features will then be used to train a set of linear mod els to predict the runtime of each solver. the solver with the best predicted runtime will be evaluated. to predict each of our latent features it is of course pos sible to use any regression based approach available in ma chine learning. from running just the five approaches that we have utilized in the previous sections unsurprisingly we observe that a random forest provides the highest qual ity prediction. the results are presented in table . here svd predicted uses a random forest to predict the values of each latent feature and then trains a secondary linear model over the latent features to predict the runtime of each solver. from the numbers we observe that this portfolio behaves similarly to the original random forest approach that sim ply predicts the runtime of each solver. this is to be expected since the entire procedure as we described so far can be seen as simply adding a single meta layer to the model. after all one of the nice properties of forests is that they are able to capture highly nonlinear relations between the features and target value. all we are doing here is adding several forests that are then linearly combined into a single value. but this procedure does provide one crucial piece of new informa tion. whereas before there was little feedback as to which fea tures were causing the issues we now know that if we have a perfect prediction of the latent features we can dramatically improve the performance of the resulting portfolio. further more we know that we dont even need to focus on all of the latent features equally since figure revealed that we can survive with less than half of them. therefore using singular value decomposition we can now identify the latent features that are hard to predict the ones resulting in the highest error. we can then subsequently use this information to claim that the reason we are unable to predict this value is because the regular features we have available are not properly capturing all of the structural nu ances that are needed to distinguish instances. this obser vation can subsequently be used to split the instances into two groups one where the random forest over predicts and one where it under predicts. this is information that can then help guide researchers to identify new features that do cap ture the needed value to differentiate the two groups. this therefore introduces a more systematic approach to generat ing new features. just from the results in table we know that our current feature vectors are not enough when compared to what is achievable in table . we also see that for the well studied sat and csp instances the performance is better than for maxsat where the feature vectors have only recently been introduced. we can then just aim to observe the next latent feature to focus on. this can be simply done by iterating over each la tent feature and artificially assigning it the correct value while maintaining all the other predictions. whichever fea ture thus corrected results in the most noticeable gains is the one that should be focused on next. whenever two la tent features tie in the possible gains we should also focus on matching the one with the lower index since mathemati cally that is the feature that captures more of the variance. if we go by instance names as a descriptive marker surprisingly following our approach results in a sep aration where both subsets have instances with the same names. so following the latent feature suggested for the maxsat dataset we observe that there is a difference between ped.g.recomb..wcnf and ped.g.recomb..wcnf. for csp we are told that fapp.xml and fapp.xml should be different. this means that the performance of a solver on an instance goes beyond just the way that instance was gener ated. there are still some fundamental structural differences between instances that our current features are not able to identify. this only highlights the need for a systematic way in which to continue to expand our feature sets. conclusions algorithm selection is an extremely powerful tool that can be used to distinguish between variations among problem in stances and using this information to recommend the solver most likely to yield the best performance. over just the last few years this field of research has flourished with numerous new algorithms being introduced that consistently dominate international competitions. yet the entire success and failure of these approaches is fundamentally tied to the quality of the features used to distinguish between instances. despite its obvious importance there has been remarkably little re search devoted to the creation and study of these features. instead researchers prefer to utilize a shotgun approach of including everything they imagine might be remotely useful relying on feature filtering to remove the unhelpful values. while this approach has worked out fine by capturing most of the key structural differences at its core it depends on luck. this paper therefore aims to introduce a more systematic way of coming up with a feature set for a new problem do main. specifically it is suggested that a matrix decomposi tion approach like singular value decomposition svd be used to analyze the performances on a number of solvers on a representative set of instances. this procedure will therefore generate a set of latent features that best separate the data and identify when a particular solver is preferred. through experimental results on three diverse datasets the paper shows that if we had access to these latent features a straightforward linear model can achieve a level of perfor mance identical to that of the perfect oracle portfolio. while in practice we do not have access to these features the paper further shows that we can still predict their val ues using our original features. a portfolio that uses these predicted values is able to achieve the same level of perfor mance as one of the best portfolio approaches if not slightly better. furthermore any latent feature where the prediction error is high helps identify the instances that the researcher must focus on differentiating. this will then lead to a more systematic method of generating features and will later help us understand why certain instances can be solved by certain approaches leading to a new wave of algorithm develop ment. acknowledgments. this publication has emanated from research supported in part by a research grant from science foundation ireland sfi under grant number sfirc. it is also funded in part by the fpfunded fetopen project called inductive constraint programming icon grant agreement . references ansotegui c. malitsky y. and sellmann m. . maxsat by improved instancespecific algorithm configu ration. aaai. coseal. . configuration and selection of algorithms project. httpscode.google.compcoseal. ghahramani z. griffiths t. l. and sollich p. . bayesian nonparametric latent feature models. world meet ing on bayesian statistics. hebrard e. . mistral a constraint satisfaction library. proceedings of the third international csp solver compe tition. helmert m. roger g. and karpas e. . fast down ward stone soup a baseline for building planner portfolios. icaps. hurley b. kotthoff l. malitsky y. and osullivan b. . proteus a hierarchical portfolio of solvers and trans formations. cpaior. hutter f. xu l. hoos h. h. and leytonbrown k. . algorithm runtime prediction methods evaluation. arti ficial intelligence . kadioglu s. malitsky y. sabharwal a. samulowitz h. and sellmann m. . algorithm selection and schedul ing. cp . kotthoff l. gent i. and miguel i. p. . an evalu ation of machine learning in algorithm selection for search problems. ai communications . kotthoff l. . llama leveraging learning to automatically manage algorithms. technical report arxiv.. httparxiv.orgabs.. kroer c. and malitsky y. . feature filtering for instancespecific algorithm configuration. ictai . kuhn m. . classification and regression training. http cran.rproject.orgwebpackagescaretcaret.pdf. malitsky y. sabharwal a. samulowitz h. and sellmann m. . algorithm portfolios based on costsensitive hier archical clustering. ijcai. martin c. and porter m. . the extraordinary svd. mathematical association of america . omahony e. hebrard e. holland a. nugent c. and osullivan b. . using casebased reasoning in an al gorithm portfolio for constraint solving. aics. prasantha h. . image compression using svd. con ference on computational intelligence and multimedia ap plications . rice j. . the algorithm selection problem. advances in computers . romanski p. . fselector. cran r library. rutz o. j. bucklin r. e. and sonnier g. p. . a latent instrumental variables approach to modeling keyword conversion in paid search advertising. journal of marketing research . thornton c. hutter f. hoos h. h. and leytonbrown k. . autoweka combined selection and hyper parameter optimization of classification algorithms. kdd . xu l. hutter f. hoos h. h. and leytonbrown k. . satzilla portfoliobased algorithm selection for sat. jair . xu l. hutter f. hoos h. and leytonbrown k. a. features for sat. httpwww.cs.ubc.calabsbetaprojects satzillareport sat features.pdf. xu l. hutter f. shen j. hoos h. h. and leytonbrown k. b. satzilla improved algorithm selection based on costsensitive classification models. sat compe tition. yang w. yi d. xie y. and tian f. . statistical identification of syndromes feature and structure of disease of western medicine based on general latent structure mode. chinese journal of integrative medicine .  maxsat by improved instancespecific algorithm configuration carlos ansotegui department of computer science university of lleida spain carlosdiei.udl.es yuri malitsky insight centre for data analytics university college cork ireland yuri.malitskyinsightcentre.org meinolf sellmann ibm watson research center yorktown heights ny usa meinolfus.ibm.com abstract our objective is to boost the stateoftheart performance in maxsat solving. to this end we employ the instance specific algorithm configurator isac and improve it with the latest in portfolio technology. experimental results on sat show that this combination marks a significant step forward in our ability to tune algorithms instancespecifically. we then apply the new methodology to a number of maxsat prob lem domains and show that the resulting solvers consistently outperform the best existing solvers on the respective prob lem families. in fact the solvers presented here were inde pendently evaluated at the maxsat evaluation where they won six of the eleven categories. introduction maxsat is the optimization version of the satisfiability sat problem. it can be used effectively to model prob lems in several domains such as scheduling timetabling fpga routing design and circuit debugging software pack age installation bioinformatics probabilistic reasoning etc. from the research perspective maxsat is also of particular interest as it requires the ability to reason about both opti mality and feasibility. depending on the particular problem instance being solved it is more important to emphasize one or the other of these inherent aspects. maxsat technology has significantly progressed in the last years thanks to the development of several new core al gorithms and the very recent revelation that traditional mip solvers like cplex can be extremely well suited for solving some families of partial maxsat instances ansotegui and gabas . given that different solution approaches work well on different families of instances matos et al. used metaalgorithmic techniques developed in cp and sat to devise a solver portfolio for maxsat. surprisingly and in contrast to sat until this idea had not led to the de velopment of a highly efficient maxsat solver that would dominate e.g. the yearly maxsat evaluations argelich et al. . we describe the methodology that led to a maxsat port folio that won six out of eleven categories at the maxsat evaluation. in particular we develop an instance specifically tuned solver for every version of maxsat copyright c association for the advancement of artificial intelligence www.aaai.org. all rights reserved. that outperforms all existing solvers in their respective do mains. we do this for regular maxsat ms partial pms weighted wms and weighted partial wpms maxsat. the method we apply to obtain these solvers is a portfo lio tuning approach isac which generalizes both tuning of individual solvers as well as combining multiple solvers into one solver portfolio. as a sideeffect of our work on maxsat we found a way to improve instancespecific algo rithm configuration isac kadioglu et al. by com bining the original methodology with one of the latest and most efficient algorithm portfolio builders to date. the next section formally introduces our target prob lem maxsat. then we review the current stateoftheart in instancespecific algorithm configuration and algorithm portfolios. we show how both techniques can be combined and empirically demonstrate on sat that our improved method works notably better than the original method and other instancespecific algorithm tuners. we then apply the new technique to maxsat. finally in extensive experiments we show that the developed solvers significantly outperform the current stateoftheart in every maxsat domain. maxsat problem definition let us begin by formally stating the problem a weighted clause is a pair cw where c is a clause and w is a natural number or infinity indicating the penalty for falsifying the clause c. a weighted partial maxsat formula wpms is a multiset of weighted clauses c w . . . cm wm cm . . . cmm where the first m clauses are soft and the last m clauses are hard. here a hard clause is one that must be satisfied while also satisfying the maximum combined weight of soft clauses. a partial maxsat formula pms is a wpms formula where the weights of soft clauses are equal. the set of variables occurring in a formula is noted as var. a total truth assignment for a formula is a func tion i var that can be extended to literals clauses sat formulas. for maxsat formulas is defined as ic w . . . cm wm m i wi ici. the optimal cost of a formula is cost mini i var and an optimal assignment is an assign ment i such that i cost. the weighted partial maxsat problem for a weighted partial maxsat formula is the problem of finding an op timal assignment. solvers among the stateoftheart maxsat solvers we find two main approaches branchandboundbased algo rithms heras larrosa and oliveras argelich and manya darras et al. lin su and li ansotegui and gabas and satbased solvers fu and malik marquessilva and planes ansotegui bonet and levy manquinho marquessilva and planes . from the annual results of the international maxsat evaluation argelich et al. we can see that satbased solvers clearly dominate on industrial and some crafted instances while branchandbound solvers dominate on random and some families of crafted instances. in this setting employing multiple solution techniques is well motivated. consequently in matos et al. a maxsat portfolio was devised and tested in a promising but limited experimental evaluation. in particular matos et al. used satzillas approach to build a portfolio of solvers for maxsat. the proposed portfolio was then ap plied on some pure maxsat instances i.e. formulae where all clauses have weight which implies that there are no hard clauses. the format of these instances is exactly the dimacs cnf format. therefore matos et al. could use existing sat instancefeatures to characterize the given maxsat instances. the particular features used were prob lem size features balance features and local search probe features. the extension to partial and weighted maxsat in stances which would have required the definition of new features was left for future work. to our best knowledge this is the only study of maxsat portfolios. in particular there has been no dominant algorithm portfolio in the an nual maxsat evaluations argelich et al. . we will devise instancespecific solvers for each of the maxsat domains. these will be based on algorithm tuning and algorithm portfolios. therefore in the next two sections we develop the technology that we will then later use to de vise a novel solver for maxsat. metaalgorithms just as we observed for maxsat in the practice of com binatorial search algorithms there is oftentimes no single solver that performs best on every single instance family. rather different algorithms and even different parametri sations of the same solver excel on different instance fam ilies. this is the underlying reason why algorithm port folios have been so successful in sat xu et al. kadioglu et al. cp omahony et al. and qbf pulina and tacchella . namely all these portfo lio builders select and schedule solvers instancespecifically. in the literature we find two metaalgorithmic approaches for making solvers instancespecific. the first are algorithm portfolio builders for given sets of solvers the second are instancespecific tuners for parametrized solvers. in the fol lowing we will review the stateoftheart in both related areas. algorithm portfolios the first approach on algorithm selection that really stood out was satzilla xu et al. . in this approach a regression function is trained to predict the performance of every solver in the given set of solvers based on the features of an instance. when faced with a new instance the solver with the best predicted runtime is run on the given instance. the resulting sat portfolios excelled in the sat competi tions in and in and pushed the stateoftheart in sat solving. meanwhile more performant algorithm portfolio builders have been developed. for a while the trend was towards more highly biased regression and classification models sil verthorn and miikkulainen . then the simple k nearest neighbor knnbased portfolio s balint et al. won in the sat competition. s is notable be cause it was the first portfolio that excelled in different cat egories while using the same solver and training base for all categories random combinatorial and industrial satzilla had won multiple categories earlier but by entering a differ ent portfolio tailored for each instance category. this was achieved by using a lowbias kadioglu et al. call it nonmodel based machine learning approach for se lecting the primary solver used to tackle the given instance. namely s uses a costsensitive knn approach for this pur pose. the latest satzilla xu et al. now also uses a low bias machine learning approach that relies on costsensitive decision forests and voting. for every pair of solvers in its portfolio a forest of binary decision trees is trained to choose what is the better choice for the instance at hand. the decisions of all trees are then aggregated and the solver with the highest score is used to solve the given instance. this portfolio clearly dominated the sat challenge com petition where it performed best on both industrial and combinatorial instances. moreover the satzillaall portfo lio which is identical for all three categories came in second in both categories. in a new portfolio builder was introduced malit sky et al. . this tool named cshc is based on cost sensitive hierarchical clustering of training instances. cshc combines the ability of satzilla to handle large and partly uninformative feature sets difficult for s as the dis tance metric is corrupted with s ability to handle large sets of base solvers difficult for satzilla as it trains a random forest for each pair of solvers. cshc was also shown to outperform both s and satzilla and it won two categories in the sat competition. algorithm tuning portfolio approaches are very powerful in practice but there are many domains that do not have the plethora of high performance solvers. often though there exists at least one solver that is highly parameterized. in such cases it may be possible to configure the parameters of the solver to gain the most benefit on a particular benchmark. the fact that there are often subtle nonlinear interactions between parameters of sophisticated stateoftheart algo rithms makes manual tuning very difficult. consequently a number of automated algorithm configuration and parameter algorithm instancespecific algorithm configuration isaclearna t f f s t normalizef kc s cluster t f for all i . . . k do pi ggaasi return k pc s t isac runa x k p c d s t f featuresx fi fisi ti i i minif ci return ax pi tuning approaches have been proposed over the last decade. these approaches range from gradientfree numerical opti mization audet and orban to gradientbased opti mization coy et al. to iterative improvement tech niques adensodiaz and laguna to iterated local search techniques like paramils hutter et al. and to populationbased local search approaches like the gender based genetic algorithm gga ansotegui sellmann and tierney . in light of the success of these oneconfigurationfits all tuning methods a number of studies explored how to use them to effectively create instancespecific tuners. hy dra xu hoos and leytonbrown for example uses the parameter tuner paramils hutter et al. to it eratively tune the solver and add parameterizations to a satzilla portfolio that optimizes the final performance. the isac method with the objective to boost performance in maxsat we ex ploit an approach called instancespecific algorithm con figuration isac kadioglu et al. that we recap in de tail in this section. isac has been previously shown to out perform the regression based satzilla approach and when coupled with the parameter configurator gga isac outperformed hydra xu hoos and leytonbrown on several standard benchmarks. isac is an example of a lowbias approach. unlike sim ilar approaches such as hydra xu hoos and leyton brown and argosmart nikolic maric and janici isac does not use regressionbased analysis. instead it computes a representative feature vector that character izes the given input instance in order to identify clusters of similar instances. the data is therefore clustered into non overlapping groups and a single solver is selected for each group based on some performance characteristic. given a new instance its features are computed and it is assigned to the nearest cluster. the instance is then solved by the solver assigned to that cluster. more specifically isac works as follows see algo rithm . in the learning phase isac is provided with a pa rameterized solvera a list of training instances t their cor responding feature vectors f and the minimum cluster size . first the gathered features are normalized so that every feature ranges from and the scaling and translation values for each feature s t are memorized. this normal ization helps keep all the features at the same order of mag nitude and thereby keeps the larger range values from being given more weight than the lower ranging values. next the instances are clustered based on the normalized feature vectors. clustering is advantageous for several rea sons. first training parameters on a collection of instances generally provides more robust parameters than one could obtain when tuning on individual instances. that is tuning on a collection of instances helps prevent overtuning and al lows parameters to generalize to similar instances. secondly the parameters found are prestabilized meaning they are shown to work well together. isac uses gmeans hamerly and elkan for clus tering. robust parameter sets are obtained by not allowing clusters to contain fewer than a manually chosen threshold a value which depends on the size of the data set. in our case we restrict clusters to have at least instances. beginning with the smallest cluster the corresponding instances are re distributed to the nearest clusters where proximity is mea sured by the euclidean distance of each instance to the clus ters center. the final result of the clustering is a number of k clusters si and a list of cluster centers ci. then for each cluster of instances si favorable parameters pi are com puted using the instanceoblivious tuning algorithm gga. when running algorithm a on an input instance x isac first computes the features of the input and normalizes them using the previously stored scaling and translation values for each feature. then the instance is assigned to the nearest cluster. finally isac runs a on x using the parameters for this cluster. portfolio tuner note how isac solves a core problem of instancespecific algorithm tuning namely the selection of a parametrization out of a very large and possibly even infinite pool of possible parameter settings. in algorithm portfolios we are dealing with a small set of solvers and all methods devised for algo rithm selection make heavy use of that fact. clearly this ap proach will not work when the number of solvers explodes. isac overcomes this problem by clustering the train ing instances. this is a key step in the isac methodology as described in kadioglu et al. training instances are first clustered into groups and then a highperformance parametrization is computed for each of the clusters. that is in isac clustering is used both for the generation of high quality solver parameterizations and then for the subsequent selection of the parametrization for a given test instance. beyond clusterbased algorithm selection while malitsky and sellmann showed that cluster based solver selection outperforms satzilla this alone does not fully explain why isac often outperforms other instancespecific algorithm configurators like hydra. clustering instances upfront appears to give us an advantage when tuning individual parameterizations. not only do we save a lot of tuning time with this methodology since the training set for the instanceoblivious tuner is much smaller than the whole set. we also bundle instances together hop ing that they are somewhat similar and thus amenable for being solved efficiently with just one parametrization. consequently we want to keep clustering in isac. how ever and this is the core observation in this paper once the parameterizations for each cluster have been computed there is no reason why we would need to stick to these clus ters for selecting the best parametrization for a given test instance. consequently we propose to use an alternate state oftheart algorithm selector to choose the best parametriza tion for the instance we are to solve. to this end after isac finishes clustering and tuning the parameters of existing solvers on each cluster we can then use any algorithm selector to choose one of the parametri sations independent of the cluster an instance belongs to for this final stage we can use any efficient algorithm se lector. in our experiments we will use cshc. we name the resulting approach portfolio tuner isac . comparison of isac with isac and hydra before we return to our goal of devising new cuttingedge solvers for maxsat we want to test the isac methodol ogy in practice and compare it with the best instancespecific algorithm configurators to date isac and hydra. we use the benchmark set from xu hoos and leyton brown where hydra was first introduced. in partic ular there are two nontrivial sets of instances random rand and crafted hand. following the previously established methodology we start our portfolio construction with local search solvers paws thornton et al. rsaps hutter tompkins and hoos saps tompkins hutter and hoos ag wsat wei li and zhang b agwsat wei li and zhang c agwsatp wei li and zhang a gnov elty pham and gretton gwsat li and huang ranov pham and anbulagan vw prestwich and anov hoos . we augment these solvers by adding six fixed parameterizations of satenstein to this set giving us a total of constituent solvers. we clustered the training instances of each dataset and added gga trained versions of satenstein for each cluster resulting in new solvers for random and for crafted. we used a timeout of seconds when training these solvers but employed a seconds timeout to evaluate the solvers on each respective dataset. the times were measured on dual intel xeon . ghz quadcore nehalem processors and gb of ddr memory ghz. in table a we show the test performance of various solvers on the hand benchmark set train and test instances. we conduct runs on each instance for each solver. when referring to a value as average we give the mean time it takes to solve only those instances that do not timeout. the value par includes the timeout instances when computing the average. par then gives a penal ized average where every instance that times out is treated as having taken times the timeout to complete. finally we present the number of instances solved and the corre sponding percentage of solved instances in the test set. the best single solver bs is one of the satenstein pa rameterizations tuned by gga and is able to solve about of all instances. hydra solves while isacgga using only satenstein solves only . using the whole set of solvers for tuning isacmsc solves about of all instances which is worse than always selecting the best base solver. of course we only know a posteriori that this pa rameterization of satenstein is the best solver for this test table sat experiments a hand average par par solved solved bs . . . hydra . . . isacgga . . . isacmsc . . . isac . . . vbs . . . b rand average par par solved solved bs . . . hydra . . . . isacgga . . . isacmsc . . . . isac . . . . vbs . . . . set. however isacs performance is still not convincing. by augmenting the approach using a final portfolio selec tion stage we can boost performance. isac solves of all test instances outperforming all other approaches and closing almost of the gap between hydra and the vir tual best solver vbs an imaginary perfect oracle that al ways correctly picks the best solver and parametrization for each instance which marks an upper bound on the perfor mance we may realistically hope for. the second benchmark we present here is rand. there are test and train instances in this benchmark. in table b we see that the best single solver bs gnovelty solves of the instances in this test set. hydra improves this to roughly equal in performance to isacmsc. isac improves performance again and leads to almost of all instances solved within the timelimit. the improved approach outperforms all other methods and isac closes over of the gap between the original isac and the vbs. note that using portfolios of the untuned sat solvers only is in general not competitive as shown in xu hoos and leytonbrown and kadioglu et al. . to verify this finding we also ran a comparison using untuned base solvers only. on the sat rand data set for example we find that cshc using only base solvers can only solve instances which is not competitive. maximum satisfiability in the preceding section we demonstrated the potential effec tiveness of the new isac approach on sat problems. we now apply this methodology to our main target the maxsat problem. in order to apply the isac methodology we ob viously first need to address how maxsat instances can be characterized. feature computation as we aim to tackle the variety of existing maxsat prob lems we cannot rely directly on the instance features used in matos et al. which considered instances where all clauses are soft with identical weights. we therefore com pute the percentage of clauses that are soft and the statistics of the distribution of weights avg min max stdev. the re maining features we use are a subset of the standard sat features based on the entire formula ignoring the weights. specifically these features cover statistics like the number of variables number of clauses proportion of positive to neg ative literals the number of clauses a variable appears in on average etc. we also experimented with plain sat features and found that this was not competitive compared to the pro posed maxsat features above. solvers to apply isac we also need a parametrized maxsat solver that we can tune. in the past three years sat based maxsat solvers have become very efficient at solv ing industrial maxsat instances and perform well on most crafted instances. also with annual maxsat evaluations since there have been a number of diverse methodolo gies and solvers proposed. akmaxsatls kuegel for example is a branchandbound algorithm with lazy dele tion and a local search for an initial upper bound. this solver dominated the randomly generated partial maxsat prob lems in the maxsat evaluations max . the solver also scored second place for crafted partial maxsat instances. alternatively solvers like shinmaxsat honjyo and tanjo and satj berre tackle weighted partial maxsat problems by encoding them to sat and then resolving them using a dedicated sat solver. finally there are solvers like wpm ansotegui bonet and levy or wbo. manquinho marquessilva and planes that are based on iterative identification of unsatis fiable cores and are well suited for unweighted industrial maxsat. one of the few parametrized highly efficient partial maxsat solvers is qmaxsat koshimura et al. which is based on sat. qmaxsat searches for the opti mum cost from k m i wi to some value smaller than cost. each subproblem is solved by employing the underlying sat solver glucose. qmaxsat inherits its pa rameters from glucose rndinit luby rndfreq vardec cladecay rinc and rfirst audemard and simon . the particular version of qmaxsat qmaxsatg that we use in our evaluation was the winner for the industrial partial maxsat category at the maxsat evaluation. numerical results now we have everything in place to run the isac methodology and devise a new maxsat solver the pri mary objective of this study. we conducted our experimen tation on the same environment as the maxsat evalua tion argelich et al. operating system rocks cluster .. linux .. processor amd opteron pro cessor ghz memory . gb and compilers gcc .. and javac jdk ... we split our experiments into two parts. we first show the performance of isac on partial maxsat instances crafted instances industrial instances and finally combin ing both. in the second set of experiments we train solvers for maxsat ms weighted maxsat wms partial maxsat pms and weighted partial maxsat wpms. in these datasets we will combine instances from the crafted industrial and random subcategories. table fixedsplit maxsat a pms crafted average par par solved solved bs . . . isacgga . . . isacmsc . . . isac . . . . vbs . . . b pms industrial average par par solved solved bs . . . isacgga . . . isacmsc . . . isac . . . . vbs . . . c pms crafted industrial average par par solved solved bs . . . isacgga . . . isacmsc . . . isac . . . . vbs . . . partial maxsat we used three benchmarks in our nu meric analysis obtained from the maxsat evaluation i the families of partial maxsat crafted instances with a total of instances ii the families of partial maxsat industrial instances with a total of instances and the mixture of both sets. this data was split into training and testing sets. crafted had testing and training while industrial instances were split so there awere testing and training. our third dataset merged crafted and industrial instances and had testing and training instances. the solvers we run on the the partial maxsat industrial and crafted instances were qmaxsatg this is the solver we tune pwbo. qmaxsat pm shinmaxsat satj wpm wbo. wmaxsatz wmaxsatz akmaxsat ak maxsatls iutrrrv and iutrrls. more details can be found in argelich et al. . for each of these benchmark sets we built an instance specifically tuned maxsat solver by applying the isac methodology. we use a training set which is always distinct from the test set on which we report results of instances which we cluster. for each cluster we tune a parametrization of qmaxsatg. then we combine these parameterizations with the other maxsat solvers described above. for this set of algorithms we train an algorithm selector using cshc. finally we evaluate the performance of the resulting solver on the corresponding test set. in table a we show the test performance of various solvers. bs shows the performance of the single best un tuned solver from our base set. it solves of all in stances in this set. isacgga which instancespecifically tunes only qmaxsat without using other solvers solves . in isacmsc malitsky and sellmann we in corporate also other highperformance maxsat solvers. performance jumps isacmsc solves over of all in stances within our timelimit of seconds. isac does even better. it solves over of all in stances closing the gap between isacmsc and vbs by almost compared to the previous state of the art bs we increase the number of solved instances from to . seeing that in this category at the maxsat evaluation the top five solvers were ranked just instances apart this improvement is very significant. in table b we see exactly the same trend albeit a bit less pronounced isac closes about of the gap between isac and the vbs. in the subsequent experiment we built a maxsat solver that excels on both crafted and industrial maxsat instances. table c shows the results. the single best solver for this mixed set of instances is the default qmaxsatg and it solves about of all instances within seconds. it is worth noting that this was the stateoftheart in partial maxsat before we conducted this work. tuning qmaxsat g instancespecifically isacgga we improve perfor mance only slightly. isacmsc works clearly better and is able to solve almost of all instances. however the best performing approach is once more isac which solves of all instances in time closing the gap between per fect performance and the stateoftheart in partial maxsat before we conducted this study by over . partial weighted maxsat the previous experiments were conducted on the particular traintest splits. in this section we conduct a fold cross validation on the four categories of the maxsat evaluation argelich et al. . these are plain maxsat instances weighted maxsat partial maxsat and weighted partial maxsat. the results of the cross validation are presented in tables a d. specifically each data set is broken uniformly at ran dom into non overlapping subsets. each of these subsets is then used as the test set one at a time while the in stances from all other folds are used as training data. the ta bles present the average performance over folds. further more all experiments were run with a second time out on the same machines we used in the previous section. we use the the following solvers akmaxsatls akmaxsat bincd wpm pwbo. wbo.cnf qmaxsatg shinmaxsat wmaxsatz and wmaxsatz. we also em ploy the highly parameterized solver qmaxsatg. the ms data set has instances split among random crafted and industrial. each fold has test instances. re sults in table a confirm the findings observed in previous experiments. in this case isacmsc struggles to improve over the best single solver. at the same time isac nearly completely closes the gap between bs and vbs. the partial maxsat dataset is similar to the one used in the previous section but in this case we also augment it with randomly generated instances bringing the count up to instances. the weighted maxsat problems consist of only crafted and random instances creating a dataset of size . finally the weighted partial maxsat problems number . all in all we observe that isac always outperforms the original isac methodology significantly closing the gap be tween isacmsc and the vbs by and . more importantly for the objective of this study we massively improve the prior stateoftheart in maxsat. the tables give the average performance of the single best solver for each fold which may of course differ from fold to fold in the row indexed bs. note this value is better than what the previous best single maxsat solver had to offer. still on plain maxsat isac solves more instances more on partial maxsat more on weighted maxsat table maxsat crossvalidation a ms mix has test instances per fold. average par par solved solved bs . . . . isacmsc . . . . isac . . . . vbs . . . . b pms mix has test instances per fold. average par par solved solved bs . . . . isacmsc . . . . isac . . . . vbs . . . . c wms mix has test instances per fold. average par par solved solved bs . . . . isacmsc . . . . isac . . . . vbs . . . . d wpms mix has test instances per fold. average par par solved solved bs . . . . isacmsc . . . . isac . . . . vbs . . . . and more instances on weighted partial maxsat in stances within the timeout. this is a significant improvement in our ability to solve maxsat instances in practice. these results were independently confirmed at the maxsat evaluation where our portfolios built based on the methodology described in this paper won six out of eleven categories and came in second in another three. conclusion we have introduced an improved instancespecific algo rithm configurator by adding a portfolio stage to the exist ing isac approach. extensive tests revealed that the new method consistently outperforms the best instancespecific configurators to date. the new method was then applied to partial maxsat a domain where portfolios had never been used in a competitive setting. we devised a method to ex tend features originally designed for sat to be exploited to help characterize weighted partial maxsat instances. then we built three instancespecific partial maxsat solvers for crafted and industrial instances as well as a combination of those. moreover we conducted fold cross validation in the four categories of the maxsat evaluation. based on this work we entered our solvers in the maxsat competition where they won six out of eleven categories and came second in another three. these results indepen dently confirm that our solvers mark a significant step in solving weightedpartial maxsat instances efficiently. acknowledgments insight centre is supported by sfi grant sfirc. references adensodiaz and laguna adensodiaz b. and laguna m. . finetuning of algorithms using fractional experimental de sign and local search. operations research . ansotegui and gabas ansotegui c. and gabas j. . solving weighted partial maxsat with ilp. cpaior. ansotegui bonet and levy ansotegui c. bonet m. and levy j. . solving weighted partial maxsat through satisfia bility testing. sat . ansotegui sellmann and tierney ansotegui c. sellmann m. and tierney k. . a genderbased genetic algorithm for the automatic configuration of algorithms. cp . argelich and manya argelich j. and manya f. . par tial maxsat solvers with clause learning. sat . argelich et al. argelich j. li c. manya f. and planes j. . maxsat evaluations. www.maxsat.udl.cat. audemard and simon audemard g. and simon l. . glucose. audet and orban audet c. and orban d. . finding optimal algorithmic parameters using derivativefree optimization. siam j. on optimization . balint et al. balint a. belov a. diepold d. gerber s. jarvisalo m. and sinz c. eds. . proceedings of sat chal lenge solver and benchmark descriptions volume b of department of computer science series of publications b. uni versity of helsinki. berre berre d. . satj a satisfiability library for java. www.satj.org. competition competition s. . www.satcomptition.org. coy et al. coy s. golden b. runger g. and wasil e. . using experimental design to find effective parameter set tings for heuristics. journal of heuristics . darras et al. darras s. dequen g. devendeville l. and li c. . on inconsistent clausesubsets for maxsat solving. cp . fu and malik fu z. and malik s. . on solving the partial maxsat problem. sat . hamerly and elkan hamerly g. and elkan c. . learning the k in kmeans. nips. heras larrosa and oliveras heras f. larrosa j. and oliveras a. . minimaxsat a new weighted maxsat solver. sat . honjyo and tanjo honjyo k. and tanjo t. . shin maxsat. hoos hoos h. . adaptive novelty novelty with adaptive noise. aaai. hoos hoos h. . autonomous search. springer verlag. hutter et al. hutter f. hoos h. leytonbrown k. and stuetzle t. . paramils an automatic algorithm configura tion framework. jair . hutter tompkins and hoos hutter f. tompkins d. and hoos h. . rsaps reactive scaling and probabilistic smooth ing. cp. kadioglu et al. kadioglu s. malitsky y. sellmann m. and tierney k. . isac instancespecific algorithm con figuration. ecai . kadioglu et al. kadioglu s. malitsky y. sabharwal a. samulowitz h. and sellmann m. . algorithm selection and scheduling. cp . koshimura et al. koshimura m. zhang t. fujita h. and hasegawa r. . qmaxsat a partial maxsat solver. jsat . kuegel kuegel a. . improved exact solver for the weighted maxsat problem. pos . li and huang li c. and huang w. . gwsat gradientbased greedy walksat. sat . lin su and li lin h. su k. and li c. . within problem learning for efficient lower bound computation in max sat solving. aaai . malitsky and sellmann malitsky y. and sellmann m. . instancespecific algorithm configuration as a method for nonmodelbased portfolio generation. cpaior . malitsky et al. malitsky y. sabharwal a. samulowitz h. and sellmann m. . algorithm portfolios based on cost sensitive hierarchical clustering. ijcai. manquinho et al. manquinho v. marquessilva j. planes j. and martins r. . wbo.. manquinho marquessilva and planes manquinho v. marquessilva j. and planes j. . algorithms for weighted boolean optimization. sat . marquessilva and planes marquessilva j. and planes j. . on using unsatisfiability for solving maximum satisfiability. corr abs.. matos et al. matos p. planes j. letombe f. and marques silva j. . a maxsat algorithm portfolio. ecai . max . maxsat evaluation. nikolic maric and janici nikolic m. maric f. and janici p. . instance based selection of policies for sat solvers. sat . omahony et al. omahony e. hebrard e. holland a. nugent c. and osullivan b. . using casebased reason ing in an algorithm portfolio for constraint solving. aics. pham and anbulagan pham d. and anbulagan. . ra nov. solver description. sat competition. pham and gretton pham d. and gretton c. . gnov elty. solver description. sat competition. prestwich prestwich s. . vw variable weighting scheme. sat. pulina and tacchella pulina l. and tacchella a. . a multiengine solver for quantified boolean formulas. cp . silverthorn and miikkulainen silverthorn b. and miikku lainen r. . latent class models for algorithm portfolio meth ods. aaai. thornton et al. thornton j. pham d. bain s. and fer reira v. . additive versus multiplicative clause weighting for sat. pricai . tompkins hutter and hoos tompkins d. hutter f. and hoos h. . saps. solver description. sat competition. wei li and zhang a wei w. li c. and zhang h. a. adaptgwsatp. solver description. sat competition. wei li and zhang b wei w. li c. and zhang h. b. combining adaptive noise and promising decreasing variables in local search for sat. solver description. sat competition. wei li and zhang c wei w. li c. and zhang h. c. deterministic and random selection of variables in local search for sat. solver description. sat competition. xu et al. xu l. hutter f. hoos h. and leytonbrown k. . satzilla portfoliobased algorithm selection for sat. jair . xu et al. xu l. hutter f. shen j. hoos h. and leyton brown k. . satzilla improved algorithm selection based on costsensitive classification models. sat competition. xu hoos and leytonbrown xu l. hoos h. and leyton brown k. . hydra automatically configuring algorithms for portfoliobased selection. aaai.  modelbased genetic algorithms for algorithm configuration modelbased genetic algorithms for algorithm configuration carlos ansotegui diei univ. de lleida spain carlosdiei.udl.es yuri malitsky horst samulowitz meinolf sellmann ibm research usa ymalitssamulowitzmeinolfus.ibm.com kevin tierney dsor lab university of paderborn germany tierneydsor.de abstract automatic algorithm configurators are important practical tools for improving program performance measures such as solution time or prediction ac curacy. local search approaches in particular have proven very effective for tuning algorithms. in se quential local search the use of predictive models has proven beneficial for obtaining good tuning re sults. we study the use of nonparametric models in the context of populationbased algorithm configu rators. we introduce a new model designed specif ically for the task of predicting highperformance regions in the parameter space. moreover we intro duce the ideas of genetic engineering of offspring as well as sexual selection of parents. numerical results show that modelbased genetic algorithms significantly improve our ability to effectively con figure algorithms automatically. many algorithms have parameters that greatly affect their performance. these parameters allow algorithms to adapt to different usage scenarios and provide end users with greater flexibility in customizing software specifically for their needs. however finding good parameters is difficult and time consuming because each evaluation of the algorithm being configured is expensive often taking from seconds or hours to assess a single input of which there are many. in comparison typical local search approaches for standard benchmark problems are able to evaluate tens of thousands of solutions per second. two types of configurators have emerged nonmodelbased and modelbased approaches. nonmodelbased techniques generally rely on either an ex perimental design methodology such as the case in cali bra adensodiaz and laguna and ifrace birat tari et al. or a metaheuristic as paramils hutter et al. or gga ansotegui et al. . in contrast modelbased approaches like smac hutter et al. use machine learning in order to predict good parameteriza tions based on samples of the parameter space. we propose gga an automatic algorithm configurator that harnesses research partially supported by the ministerio de economa y competividad research project tassat tincp. the power of modelbased approaches within a nonmodel based genetic algorithm framework. genderbased genetic algorithm configuration gga gga ansotegui et al. is a genetic algorithmbased approach to general algorithm configuration which has been used in practice for a wide array of applications such as tuning solvers in sat kadioglu et al. max sat ansotegui et al. machine reassignment malit sky et al. a and mixedinteger programming kadioglu et al. . we describe the key components of gga but refer readers to ansotegui et al. for further details. overview gga accepts a parameterized algorithm to be configured called the target algorithm a set of algorithm inputs and a performance metric to be optimized. the per formance metric can be the execution time of an algorithm or some value computed by the algorithm for example the result of a simulation or accuracy of a prediction. gga then runs its specialized genetic algorithm that executes the target algorithm with different parameters in order to find a good setting. once gga has reached the target number of genera tions the best parametrization found so far is returned. population a core idea in gga is the partitioning of the population into a competitive and noncompetitive popula tion. the competitive population is directly evaluated on the target algorithm whereas the noncompetitive population simply acts as a source of diversity. this allows gga to use strong intensification procedures on one part of the popula tion while remaining diverse enough to avoid local optima. selection a tournamentbased selection mechanism is used for evaluating the competitive population. tournaments are run on disjoint subsets of parameterizations in parallel on a subset of the instances. when tuning runtime as soon as a set percentage of the parameterizations in a tournament are finished all other participants in the tournament can be ter minated. this saves significant amounts of cpu time that would otherwise be spent evaluating bad parameterizations. genome representation gga represents parameterizations using an andor tree in order to capture the dependencies between parameters. a node in the tree is a parameter or an and node that binds multiple parameters together. sev eral sample trees are shown in figure . in these particular figure the gga recombination operator. trees the root node is a categorical parameter that activates the left branch when it takes the value and the right branch when it takes the value think of a parameter that chooses which subroutine is used with parameters in the left subtree affecting only subroutine and parameters only affecting subroutine in the right tree. recombination and mutation the crossover operator in gga takes two genomes one from the competitive and one from the noncompetitive population. the competitive genome is a tournament winner while the other genome is drawn uniformly at random from the noncompetitive popu lation. the crossover process is shown in figure and it can be viewed as a hybrid of the traditional multipoint and uniform crossovers that considers parameter dependencies. gga performs mutation simply by selecting random pa rameter settings and replacing them with values drawn from either a gaussian for discrete and continuous parameters or a uniform for categorical parameters distribution. nonparametric models for predicting highperformance parameter regions gga evaluates a parameter setting by running the target al gorithm with the parameter setting on a collection of training instances. that is we conduct a black box optimization with an extremely expensive to run black box. one idea to im prove optimization performance is to reduce the high costs of querying the black box directly. to this end the idea of trying to learn a socalled surrogate model that is faster to evalu ate and that approximates the actual black box has been in troduced. the socalled response surface methodology box and wilson e.g. fits a second degree polynomial to the observed black box output and then optimizes over that polynomial to predict an optimal input for the black box. a stateoftheart sequential local search algorithm config urator that uses a surrogate model is smac hutter et al. . smac directly evaluates algorithm performance for some initial parameter settings and then uses this experience to train a random forest that predicts expected algorithm per formance over the parameter inputs. importantly smac also computes the models varianceuncertainty in its predictions. as a next point for direct evaluation of the algorithm on the training inputs smac chooses a parametrization for which the model predicts high performance plus uncertainty. the model is retrained using this information along with sev eral random parameterizations that are also directly evalu ated. a new parametrization is then chosen based on the up dated model and this process is repeated until the designated cpu time for configuration is exhausted. the use of variance in smac is very important. first this leads to an improvement of the model itself by favoring points where the model has little confidence in its own predictions. secondly a vanilla random forest can never return predictions that exceed the most highly rated prior experience. conse quently if variance was not used to score new parametrization candidates the system could repeatedly reevaluate the same parameter settings. surrogate models for genetic algorithms the core idea of this paper is to study the use of surrogate models in genetic algorithms for algorithm configuration. in particular . how can surrogate models help find highquality solu tions faster in the context of a populationbased local search approach . are there surrogate models that are more suited for pre dicting highperformance parameterizations there is already work on combining evolutionary and genetic algorithms with modelbased approaches e.g. rasheed and hirsh jin . these hybrids employ vanilla machine learning models as surrogates. they consider the various uses of surrogates within genetic algo rithms and decide when to retrain the model. in this work we devise specialized surrogate models and tightly integrate them within a genetic algorithm. the core operation that defines the next fitness evaluations in a genetic algorithm is the crossover possibly in combi nation with mutation. usually the crossover method is ran domized in some fashion. we introduce the idea of using a surrogate model for predicting offspring performance and to genetically engineer offspring that are expected to perform better. note that changing the way new parameterizations are introduced into the population can have a drastic effect on the trajectory of the entire search as this modifies the delicate balance of exploration vs. exploitation. within gga the question then becomes what percentage of offspring should be selected by the model and what percentage should be pro duced by the existing crossover operator and how sufficiently high levels of diversity in the gene pool can be maintained. another idea we introduce is that of modelbased sexual selection. as outlined above gga maintains two popu lations one that competes for the right of mating and one within which individuals mate with equal probability. based on the assessment of a surrogate model winning competitive individuals could choose a mating partner based on some def inition of their attractiveness. regarding the second question posed above based on the developments in algorithm portfolios where nonparametric or at least lowbias models have led to the currently best performing approaches xu et al. malitsky et al. b we consider random forest regression to train our sur rogate model. however offtheshelf machine learning ran dom forests were developed to lead to good predictive per formance for all inputs to the function to be approximated. for the purpose of predicting search regions that harbor high quality solutions we are much more interested in predicting areas where nearoptimal inputs can be found and less so in highaccuracy predictions throughout the search space. to this end we introduce a new splitting method for random forests that aims at predicting highquality solutions better than general regression models. according to the discussion above the three core algorith mic contributions of this paper are . genetically engineer ing offspring . selection of noncompetitive mating part ners based on their attractiveness and . building decision trees that will form random forests with a bias towards bet ter predictive performance around function extrema. . genetic engineering the core idea of genetic engineering offspring is very sim ple. once two parents have been chosen to produce offspring together we aim at finding the combination of the parents genes that results in the fittest offspring as predicted by our surrogate model. therefore what happens during genetic en gineering is a limited focused search in the restricted parame ter space spanned by the two parent genomes. these genomes provide at most two specific values for each parameter to be tuned or a single value in the case that both parents agree on the value. in general even such a limited search over a nonparametric model such as a random forest is hard theorem . given natural numbers n k maximizing a func tion true falsen represented as a random forest with k trees is apxhard even if the depth of all trees is lower or equal to two and each variable appears in at most three trees. proof. we first reduce maxsat to the problem of find ing a maximizing input for a random forest. the polyno mial construction works straightforwardly as follows for each clause in the given sat formula with k clauses over n boolean variables we construct a decision tree. we sequen tially branch on the literals in the clause if the literal eval uates to true we connect to a leaf node with value . if it evaluates to false we move to the next literal in the clause. if and only if the last literal is also in the clause that evaluates to false we connect to a leaf with value . each tree there fore returns a value of if and only if the respective clause is satisfied and otherwise. the random forest which returns the average value of its constituent trees thus gives the frac tion of clauses that evaluate to true under a given assignment. since maxsat is apx hard ausiello et al. we get the result. note that this targeted search in the space that two points span in the search space is reminiscent of the aufhebung procedure that produces syntheses in dialectic search kadio glu and sellmann . one way to conduct such a search heuristically is pathrelinking glover et al. another would be to start a fullfledged local search in the focused we stress that all of the topics pertaining to genetics here are no more than a convenient analogy. search space. since we are dealing with a very specific rep resentation of the objective function as a random forest we propose using a targeted sampling method. for each tree in the forest we gather all leaves that any po tential offspring of the two parents could fall into. this can be done in linear time in the size of the given decision tree. now for each leaf we randomly sample a number m of full assignments to the parameters. note that the leaf only con strains some parameters to be set to the value of one specific parent the others can be chosen at will. we then evaluate the full assignment using the entire random forest. we repeat this procedure for all reachable leaves in all trees ultimately re turning the assignment that results in the maximum value as returned by the forest. theorem . for a given m n the procedure above runs in time osm where s is the size of the given random forest. proof. the procedure requires at most s times m random for est evaluations. since one evaluation requires linear time as well the total time is quadratic in the size of the given random forest for any constant m. as a final note when genetically engineering a large per centage of new offspring there is a danger of losing popula tion diversity as genetic engineering intensifies the search. to alleviate this problem we remove a slightly larger percentage of the population in each generation and replace these indi viduals with new randomly generated individuals. as usual in gga these as well as the new offspring are assigned the competitive or noncompetitive gender uniformly at random. . random forests for optimization forecasting the previously described algorithm for producing optimized offspring works with any random forest. offtheshelf meth ods for training random forests are designed to approximate the function to be learned well everywhere. for our purposes we do not require an approximation of the underlying func tion in our case target algorithm performance on the train ing instances given certain parameter inputs. we just need the surrogate model to identify those areas of the parameter space where we expect highperformance parameterizations. when training the random forest and in particular building an individual decision tree we consider an alternative split ting procedure that results in higher resolution of the tree in areas where performance is high and a less refined approx imation in areas where performance is low. more precisely we will focus on the top q percent of topperforming examples in each split and try to separate the top performers from the remaining parametrizations as best as possible. to achieve this we need a valuation function for potential split parametervalue pairs that scores those pairs the highest for which the top performers are best separated from low per formers. one way to achieve this is to focus on the partition that would contain more of the top q of performers. within this partition we want the ratio of high performances over low performances to be maximized. to compute these high it is worth noting here that smac conducts a local search for finding new candidates that combine good expected performance with uncertainty in the surrogate model. and low performances we consider the squared differences of the high and low performing valuations compared to the threshold performance vh of the qth percentile performance. formally we denote the valued training observations at a current node in the tree under construction as e e p . . . pn v . . . er pr . . . prn vr such that vi vi for all i r. let h d rqe t e . . . eh and u eh . . . er. for parameter index j n and splitting value s we set l ei pi . . . p i n v ipij s and r e \ l as the left and right partitions of training examples if we were to split on parame ter pj and splitting value s. to score the potential split we now consider the following values lls eilu v h vi rls eiru v h vi lts eilt v i vh rts eirt v i vh ltn l t and rtn r t . we then compute al ltnlts lls and ar rtnrts rls . finally we set scorepj s al ltn rtn minal ar ltn rtn ar ltn rtn and split on the parametervalue pair that gives the maximum score. apart from the splitting procedure we keep the train ing of the random forests exactly the same. . sexual selection the third main algorithmic idea we contribute is the use of a surrogate model like the one developed in the previous sec tion for sexual selection. in biology the term refers to indi viduals of a population giving preference to mating with more attractive partners as gleaned from external features that indi rectly indicate fitness and health wedekind et al. . in this analogy and we would like to again stress that it is just that an analogy our surrogate model has the same func tion as the external features it indicates which mates might be better partners for producing fitter offspring. the proce dure we will test in the next section works as follows after the competitive individuals have been selected in gga they choose their partners from the noncompetitive subpopulation based on the estimated fitness of the partner. to assess the latter we obviously use the surrogate model. if the top competitive individuals are to mate with k partners in a given generation then they each select k noncompetitive individuals in a probabilistic process where a mate is chosen with higher probability the higher the noncompetitives per formance as estimated by the surrogate model. experimental results when tackling a problem like automatic algorithm configu ration where achieving any kind of performance guarantee is in fact not just hard but undecidable experimentation is key. that is to find out whether a method works in practice we put it to the test. in this section we first conduct a se ries of experiments on small black box optimization problems to gain an initial understanding of whether genetic engineer ing is at all helpful within the context of genetic algorithms and what percentage of offspring ought to be engineered to achieve the best overall results. we then quickly move on to realworld algorithm configuration problems to determine the impact of adding sexual selection as well as to evaluate the impact that different surrogate model optimization methods have on the configuration performance. we conclude the ex periments with a comparison of the modelbased gga with the unaltered configurators gga and smac. . potential of genetic engineering and diversification as we have seen before finding the best as predicted by a random forest surrogate model offspring for two parents is hard. moreover even if we could engineer offspring to per fection the question arises whether optimizing new individu als in a genetic algorithm is beneficial at all. in this section we try to answer these questions by focusing on lowdimensional global optimization problems which allow us to perfectly en gineer offspring. the use of genetic engineering within gga poses a partic ular challenge in terms of balancing the search intensification and diversification. it is possible that the engineering prema turely pushes the search into a local optimum. we therefore devise the following experiment to shed light on the influence of being greedy or relying on randomiziation within gga. we use gga for global optimization while varying the per centage g of offspring that are engineered and the percentage r of noncompetitive individuals which are replaced by new randomly generated individuals. given two parent genomes a child is normally created through a random process that mixes the andor trees stor ing the parameter settings of each parent. to assess the ef fect of perfect genetic engineering using a perfect surro gate model and perfect optimization with a certain proba bility g we will replace this standard crossover operator with a method that bruteforce enumerates all possible combina tions of the two parents and evaluates them on the actual ob jective function. this allows us to find the best possible child given a pair of parents. to assess the importance of diversification in combination with highly greedy recombination we replace a percentage r of the noncompetitive population with completely ran domly generated genomes. this is a common strategy in e.g. randomkey ga approaches bean . we assess the impact of genetic engineering and increased diversification on the performance of gga by using it for globally optimizing ten dimensional and ten dimen sional black box optimization bbo functions as described in hansen et al. from the comparing continuous op timizers coco software hansen et al. . the test functions in coco are highly nonconvex and quick to eval uate which provides a good initial testbed for evaluating dif ferent configurations of gga. we stress however that the bbo instances we consider differ from automatic algorithm configuration in several im portant ways first real algorithms usually have a mixed set we emphasize that due to the clearly exponential runtime of this routine we use this for investigative purposes only population g r . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . table average score across gga executions of func tions through on dimensions and with varying g and r probabilities. the number of generations is fixed at . of continuous ordinal and categorical parameters. more over they frequently have tens at times hundreds of parame ters. finally in algorithm configuration we are often forced to approximate the performance of a target algorithm on a train ing benchmark for a given parameter setting by evaluating the algorithm on varying subsets of instances. we run gga to minimize bbo functions with and dimensions times for each functiondimension pair on different settings of g and r for different population sizes. table shows the results of this experiment. as the opti mum value changes between functions and in some cases is even unknown before averaging results between functions we normalize the values between the best and worst observed result with being the best objective found and the worst. as shown in table when the percentage g of genetical engineering is set to increasing the random replacement probability r worsens the results of gga significantly the top row of each population amount. this means that if gga follows its original crossover approach to generate new chil dren randomly throwing out parameterizations from the non competitive pool and replacing them with random parameter izations is ill advised. however as soon as g is increased the introduction of new random parameterizations is benefi cial likely helping to compensate the greater intensification by increasing the diversification of the noncompetitive pool. most importantly though we observe that genetic engi neering really boosts performance. even without increasing r increasing g to a point where all offspring is engineered results in roughly twice the performance quality compared to ggas standard operator. while we had originally assumed that there was some mix of random and greedy crossover that would yield the best results the results clearly show that to maintain gene pool diversity it is more favorable to inject new random individuals rather than using some fraction of random crossovers. . configuring sat solvers while encouraging the experiments on bbo are not fully indicative of the typical scenarios where algorithm configu ration is utilized. in practice we do not have the luxury of perfectly engineering offspring. moreover the performance of the target solvers is much more erratic and can only be ap proximated through evaluations over multiple instances. fur thermore the number of parameters that need to be set can span into the hundreds and vary among continuous discrete and categorical. in this section we therefore evaluate our pro posed strategies on a realworld scenario the tuning of two highly sophisticated sat solvers glucose . audemard and simon and lingeling version ayvbf biere . we choose these solvers for two main reasons. first both are high quality parameterized solvers that have performed very well in sat competitions. secondly we choose this pair because glucose only has adjustable parameters while lin geling has over . this disparity allows us to test our ap proaches in both extremes. we evaluate the solvers on a collection of industrial in stances spanning the sat competition to the one in . there are instances total. for training we eval uated each solvers default configurations on all instances to isolate the ones where the solver needed at least seconds to solve an instance but could also finish it in under sec onds. we ignore very easy instances as they can be handled by a presolver. we also ignore very hard instances because we want gga to have a chance of solving an instance with out wasting time on something it will never solve. from these subsets of instances we used for training glucose and for training lingeling. subsequently all instances not used for training that could be solved by the default or any parametrization found by the various configurators including both easy ones and those where the default parameters time out were used for testing. using gga the allotted training time was set to hours with the population size set to the maximum number of generations to and the tournament size to . both tun ing and evaluation are performed with a timeout of sec onds on amd opteron processors with gb of ram. from the experiments with bbo we observed that engineer ing all offspring is beneficial when also diversifying the non competitive pool and therefore based on table we set g and r .. surrogate model optimization we compare three methods for surrogatemodelbased genetic engineering ge and the original gga. first we evaluate an offtheshelf random for est that randomly samples possible children of any two given parent genomes and selects the one predicted to have the best performance rf. second we evaluate a ran dom forest that practices the focused sampling search based on the structure of the decision trees with samples per leaf structured. finally we consider again the focused sampling approach but this time based on trees that were built giving higher resolution to the highperformance regions of the pa rameter space targeted. these results are summarized in ta ble . the table compares the algorithms on five metrics per centage of instances solved average time needed on instances solved median run time on instances solved and the penal ized average runtimes par and par where the timeout is included when computing the average time. in contrast ge ge sexual selection glucose gga rf structured targeted rfs targeteds solved . . . . . . average . . . . . . median . . . . . . par . . . . . . par . . . . . . varying surrogate models ge sexual selection lingeling gga rf structured targeted rfs targeteds solved . . . . . . average . . . . . . median . . . . . . par . . . . . . par . . . . . . table comparison of varying gga modifications for tun ing glucose and lingeling configurations on sat instances to average par counts timeouts but does not penalize them. par counts timeouts as if the solver needed times the timeout. in the literature the chief comparison is usually the percentage of instances solved and the par score. we therefore task all configurators to minimize par. table shows that employing an offtheshelf random forest in combination with a random sampling optimization method does not improve the performance of gga. indeed the opposite is true ggas performance deteriorates signifi cantly. the same is true albeit to a lesser extent when using a structured sampling method based on an offtheshelf random forest which aims at predicting well across the board. however if the underlying surrogate model is created to focus on areas of target function extrema performance is markedly improved. for the glucose solver which has few parameters genetic engineering with the targeted surrogate model still achieves an improvement of in the number of instances solved. on lingeling which has many parameters genetic engineering determines a configuration of lingeling that is able to solve about more instances than the con figuration gga finds. note that these two highperformance solvers have undergone rigorous manual tuning and therefore improving their performance further is challenging. sexual selection having shown that a good surrogate model can offer noticeable improvements within gga through ge netic engineering the question becomes whether sexual se lection can give an additional boost to performance. table presents these results in the last two columns. specifically we first add sexual selection to the approach using an off theshelf random forest rfs. we also evaluate the perfor mance of genetic engineering with a specially modified for est targeteds. in both benchmarks the proposed sexual selection strategy on top of genetic engineering is detrimental compared to the targeted genetic engineering method in isolation. after the bbo experiments showed that very aggressively engineering offspring at all times was the best choice under perfect con ditions this result came as a surprise. we believe that using a surrogate model to indirectly evaluate the noncompetitive subpopulation defacto creates competition in this part of the population and thus undermines its ability to serve as diver sity store. more research is required to find out whether alter native methods for matching parents can work better. state of the art we conclude our experiments by compar ing both gga and smac with gga which replaces the randomized crossover operator with a genetic engineering ap proach based on a specially tailored random forests that focus on accurately predicting highquality regions of the parame ter space. gga does not use sexual selection. for evaluat ing smac we adjusted the wallclock time to hours the time needed by gga to conduct generations and tuned versions of the configurator for each training set running the best one on the final test data. in table we show the average test results when configuring glucose and lingeling ten times with each configurator. as we observe gga shows improvements over the nonmodelbased gga as well as the modelbased smac. for glucose gga lowers the par score by over compared to smac. for lingeling which has significantly more parameters and thus offers more room for improvement gga lowers the par score by almost . the performances on the training sets while naturally higher followed a similar trend of gga providing the best performance. furthermore for lingeling a mere ten repeti tions are enough to claim statistical significance of the im provements achieved by gga compared to both the de fault and smac. note however that this significance is lim ited by the parameters of the experimentation in particular the specific solver tuned and traintest split considered. conclusion we showed how training a surrogate model can improve the performance of a genderbased genetic algorithm to create a more powerful algorithm configurator. specifically we showed how to integrate a surrogate model into the gga con figurator by genetic engineering as well as sexual selection whereby the former led to marked improvement whereby the latter was detrimental. we also developed a new surrogate model based on random forests that aims at higher resolution approximations where the target function exhibits good per formance. overall this resulted in a new algorithm configura tor that outperformed both a stateoftheart nonmodelbased and a stateoftheart modelbased configurator when tuning two highly performant sat solvers. glucose default gga smac gga solved . . . . average . . . . median . . . . par . . . . par . . . . lingeling default gga smac gga solved . . . . average . . . . median . . . . par . . . . par . . . . table average performance comparison of stateof the art configurators on sat solvers glucose and lingeling references adensodiaz and laguna b. adensodiaz and m. laguna. finetuning of algorithms using fractional ex perimental design and local search. operations research . ansotegui et al. c. ansotegui m. sellmann and k. tierney. a genderbased genetic algorithm for the au tomatic configuration of algorithms. in proceedings of the th international conference on principles and practice of constraint programming pages . ansotegui et al. c. ansotegui y. malitsky and m. sellmann. maxsat by improved instancespecific algo rithm configuration. in twentyeighth aaai conference on artificial intelligence . audemard and simon g. audemard and l. simon. predicting learnt clauses quality in modern sat solvers. in ijcai proceedings of the st international joint conference on artificial intelligence pasadena califor nia usa july pages . ausiello et al. g. ausiello p. crescenzi g. gam bosi v. kann a .marchetti spaccamela and m. protasi. complexity and approximation. combinatorial optimiza tion problems and their approximability properties . bean j.c. bean. genetic algorithms and random keys for sequencing and optimization. orsa journal on computing . biere a. biere. lingeling essentials a tutorial on design and implementation aspects of the the sat solver lingeling. in pos. fifth pragmatics of sat workshop a workshop of the sat conference part of floc during the vienna summer of logic july vienna austria page . birattari et al. m. birattari z. yuan p. balaprakash and t. stutzle. frace and iterated frace an overview. in empirical methods for the analysis of optimization algo rithms pages . box and wilson g.e.p. box and k.b. wilson. on the experimental attainment of optimum conditions. journal of the royal statistical society series . glover et al. f. glover m. laguna and r. marti. fundamentals of scatter search and path relinking. con trol and cybernetics . hansen et al. n. hansen s. finck r. ros and a. auger. realparameter blackbox optimization bench marking noisy functions definitions. inria re search report rr . hansen et al. n. hansen s. finck and r. ros. coco comparing continuos optimizers. research report inria inria . hutter et al. f. hutter h.h. hoos k. leytonbrown and t. stuetzle. paramils an automatic algorithm config uration framework. jair . hutter et al. f. hutter h.h. hoos and k. leyton brown. sequential modelbased optimization for general algorithm configuration. in learning and intelligent op timization th international conference lion rome italy january . selected papers pages . jin y. jin. surrogateassisted evolutionary computa tion recent advances and future challenges. swarm and evolutionary computation . kadioglu and sellmann s. kadioglu and m. sell mann. dialectic search. in proceedings of the th inter national conference on principles and practice of con straint programming cp pages . kadioglu et al. s. kadioglu y. malitsky m. sell mann and k. tierney. isacinstancespecific algo rithm configuration. in helder coelho rudi studer and michael wooldridge editors proceedings of the th eu ropean conference on artificial intelligence ecai vol ume of frontiers in artificial intelligence and appli cations pages . ios press . malitsky et al. a y. malitsky d. mehta b. osullivan and h. simonis. tuning parameters of large neighborhood search for the machine reassign ment problem. in integration of ai and or techniques in constraint programming for combinatorial optimization problems pages . springer . malitsky et al. b y. malitsky a. sabharwal h. samulowitz and m. sellmann. algorithm port folios based on costsensitive hierarchical clustering. ijcai . rasheed and hirsh k. rasheed and h. hirsh. in formed operators speeding up geneticalgorithmbased design optimization using reduced models. in in proceed ings of the genetic and evolutionary computation confer ence gecco pages . morgan kaufmann . wedekind et al. c. wedekind t. seebeck f. bet tens and a.j. paepke. mhcdependent mate preferences in humans. proceedings of the royal society of london. series b biological sciences . xu et al. l. xu f. hutter j. shen h.h. hoos and k. leytonbrown. satzilla improved algorithm se lection based on costsensitive classification models . sat competition.  nonmodelbased search guidance for set partitioning problems nonmodelbased search guidance for set partitioning problems serdar kadioglu yuri malitsky brown university providence ri serdarkynmcs.brown.edu meinolf sellmann ibm research yorktown heights ny meinolfus.ibm.com abstract we present a dynamic branching scheme for set parti tioning problems. the idea is to trace features of the underlying mip model and to base search decisions on the features of the current subproblem to be solved. we show how such a system can be trained efficiently by introducing minimal learning bias that traditional modelbased machine learning approaches rely on. ex periments on a highly heterogeneous collection of set partitioning instances show significant gains over dy namic search guidance in cplex as well as instance specifically tuned pure search heuristics. search is an integral part of solution approaches for nphard combinatorial optimization and decision problems. once the ability to reason deterministically is exhausted stateofthe art solvers try out different alternatives which may lead to an improved in case of optimization or feasible in case of satisfaction solution. this consideration of alternatives may take place highly opportunistically as in local search ap proaches or systematically as in backtrackingbased meth ods. efficiency could be much improved if we could effec tively favor alternatives that lead to optimal or feasible solu tions and a search space partition that allows short proofs of optimality or infeasibility. after all the existence of an or acle is what distinguishes a nondeterministic from a deter ministic turing machine. this of course means that assum ing p np perfect choices are impossible to guarantee. the important insight is to realize that this is a worstcase statement. in practice we may still hope to be able to make very good choices on average. the view outlined above has motivated research on ex ploiting statistical methods to guide the search. the idea of using survey propagation in sat a. braunstein has led to a remarkable performance improvement of system atic solvers for random sat instances. in stochastic offline programming malitsky and sellmann biased ran domized search decisions are based on an offline training of the solver. this approach later led to the idea of instance specific algorithm configuration isac s. kadioglu . here offline training is used to associate certain features of copyright c association for the advancement of artificial intelligence www.aaai.org. all rights reserved. the problem instance with specific parameter settings for the solver whereby the latter may include the choice of branch ing heuristic to be used. in samulowitz and memisevic branching heuristics for quantified boolean formu lae qbf were selected based on the features of the cur rent subproblem which led to more robust performance and solutions to formerly unsolved instances. in this paper we combine the idea of instancespecific al gorithm configuration with the idea of a dynamic branch ing scheme that bases branching decisions on the features of the current subproblem to be solved. like the isac con figuration system our approach is not based on graphical or any other machine learning models. instead we cluster training instances according to their features and determine an assignment of branching heuristics to clusters that results in the best performance when the branching heuristic is dy namically chosen based on the current subproblems nearest cluster. we test our approach on the mipsolver cplex that we use to tackle set partitioning problems. our experiments show that this approach can effectively boost search perfor mance even when trained on a rather small set of instances. related work in short we followup on the idea of choosing a branch ing heuristic dynamically based on certain features of the current subproblem. this idea to adapt the search to the in stance to be solved is by no means new. algorithm config uration methods like stochastic offline programming malit sky and sellmann or isac s. kadioglu deter mine the search heuristic based on the rootnode features of the problem instance. the local search sat solver saten stein a. r. khudabukhsh parametrizes its search so that it can be tuned for particular problem instance distribu tions. the dynamic search engine s. l. epstein goes further than the previously mentioned approaches in that it adapts the search heuristics based on the current state of the search. in leventhal and sellmann value selec tion heuristics for knapsack were studied and it was found that accuracy of search guidance may depend heavily on the effect that decisions higher in the search tree have on the distribution of subproblems that are encountered deeper in the tree. the latter obviously creates a serious chickenand egg problem for statistical learning approaches the distri proceedings of the twentysixth aaai conference on artificial intelligence bution of instances that require search guidance affects the choice of heuristic but the latter then affects the dis tribution of subproblems that are encountered deeper in the tree. in samulowitz and memisevic a method for adaptive search guidance for qbf solvers was based on lo gistic regression. the issue of subproblem distributions was addressed by adding subproblems to the training set that were encountered during previous runs. inspired by the success of the approach in samulowitz and memisevic we aim to boost the cplex mip solver to faster solve set partitioning problems. to this end we modify the branching heuristics basing it on the features of the current subproblem to be solved. the objective of this study is to find out whether such a system can be ef fectively trained to improve the performance of a general ized solver for a specific application. we will need to spec ify features that characterize instances to a set partitioning problem. then we need to find a methodology that allows us to learn for which subproblem as characterized by their features we wish to invoke which branching heuristic. in samulowitz and memisevic a logistic regres sion model was used to predict the heuristic that results in the lowest runtime with the highest probability. empirical hardness models like this are quite common for this kind of task and have been used successfully for solver portfo lio generation gagliolo and schmidhuber gomes and selman l. xu in the past. the problem with such models is that the learning models place a rather restric tive bias on the functions that can be learned. consequently predictions of which solver or which heuristic performs best for which instance can easily result in choices that underper form by one or two orders of magnitude in actual runtime. recently training methods have been proposed that im pose only a very limited bias. before we explain our ap proach in detail let us first review the idea of instance specific algorithm configuration that is not modelbased. instancespecific algorithm configuration the instancespecific algorithm configuration isac ap proach s. kadioglu works as follows. in the learning phase we are provided with the parametrized solver a a list of training instances t and their corresponding feature vec tors f . first we normalize the features in the set and mem orize the scaling and translation values for each feature. then we use an algorithm to cluster the training in stances based on the normalized feature vectors e.g. gmeans hamerly and elkan . the final result of the clustering is a number of k clusters si and a list of clus ter centers ci. for each cluster of instances si we compute favorable parameters pi via some instanceoblivious tuning algorithm such as gga c. ansoteguigil . when running algorithm a on an input instance x we first compute the features of the input and normalize them using the previously stored scaling and translation values for each feature. then we determine the cluster with the near est center to the normalized feature vector. finally we run algorithm a on x using the parameters learnt for this cluster. learning dynamic search heuristics isac is interesting as it provides a lowbias learning ap proach for instancespecific choices of an algorithm. an as pect that is missing however is a dynamic choice of heuris tics during search. we can adapt the approach by modifying the systematic solver that we use to tackle the combinatorial problem in question. we propose the following approach . first we cluster our training instances based on the nor malized feature vectors like in isac. . we parametrize the solver by leaving open the association of branching heuristic to cluster. . at runtime whenever the solver reaches a new search node or at selected nodes we compute the features of the current subproblem to be solved. . we compute the nearest cluster and use the corresponding heuristic to determine the next branching constraint. now the problem has been reduced to finding a good assignment of heuristics to clusters. at this point we have stated the problem in such a way that we can use a stan dard instanceoblivious algorithm configuration system to find such an assignment. we will use gga c. ansotegui gil for this purpose. note how our approach circumvents the chickenandegg problem mentioned in the beginning that results from the tight correlation of the distribution of subproblems encoun tered during search and the way how we select branching constraints. namely by associating heuristics and clusters simultaneously we implicitly take into account that changes in our branching strategy result in different subproblem dis tributions and that the best branching decision at a search node depends heavily on the way how we will select branch ing constraints further down in the tree. that being said the clusters themselves should reflect not only the rootnode problems but also the subproblems that may be encountered during search. to this end we can add subproblems encountered during the runs of individual branching heuristics on the training instances to the clus ters. this changes the shape of the clusters and may also create new ones. however note that we do not use these sub problems to learn a good assignment of heuristics to clusters which will be purely based on the original training instances. this assures that we do not base the assignment of heuristics to clusters on subproblems that will not be encountered. boosting branching in cplex for spp we will now apply the methodology established above to improve branching in the stateoftheart mip solver cplex when solving set partitioning problems. setpartitioning is one of the most prominent combinatorial optimization prob lems with applications from crewscheduling to combinato rial auctions to coalition structure generation. definition given items . . . n and a collection of m sets of these items which we call bags and a cost associated with each bag the set partitioning problem spp consists in finding a set of bags such that the union of all bags contains all items the bags are pairwise intersectionfree and the cost of the selection is minimized. we express this problem as an ip where xi is a binary variable deciding whether bag i should be included ci is the cost of including the bag and sj represents the bags that item j appears in min m i cixi s.t. isj xi j n xi i m to apply our methodology we need to define instance fea tures for set partitioning problems and we need to devise various branching heuristics that our solver can choose from. set partitioning features in order to characterize spp instances we first compute the following vectors the normalized cost vector c m the vector of bag densities sini...m the vector of item costs ijsi c ij...n the vector of item coverings i j simj...n the vector of costs over density cisii...m the vector of costs over square density cisii...m the vector of costs over k log kdensity cisi log sii...m and the vector of rootcosts over square density cisi i...m. as features we then compute the averages median stan dard deviations and the entropies of all these statistics for all vectors. in addition to the eight vectors above we add one more feature that represents the number of sets divided by the number of items therefore ending up with features. one of the benefits of this feature set is that it is invariant under column and row permutations of the problem matrix. branching heuristics we also need to provide a portfolio of different branching selection heuristics. we consider the following all of which we implement using cplexs builtin branching methods. mostfractional rounding fractional rounding one of the simplest mip branching techniques is to select the variable that has a relaxed lp solution whose fractional part is closest to . and to round it first. mostfractional up fractional up for binary ips like spp it has often been noted that rounding the branching vari able up first is beneficial. the common understanding is that forcing a variable to will force many other binaries to and thus increases the integrality of many variables when diving in this direction. this in turn may lead to shallower subtrees and integer feasible solutions faster. with this mo tivation in mind we introduce this branching heuristic which selects the variable with the most fractional lp value and enforce its ceiling as lower bound first. best pseudocost lower estimate first pseudo best one of the most substantial contributions to search in mixed integer programming was the discovery that the running av erage of the per unit costdeprivation of prior branching de cisions on a variable provides a very good estimate of the per unit deprivation that we are likely to encounter when impos ing a new branching constraint on a variable. in practice it was found that these pseudocosts m. benichou are surprisingly accurate and they are widely used in stateof theart mip solvers. for this branching heuristic we select the variable that has the lowest pseudocosts and branch in the direction that is estimated to hurt the objective least first. best pseudocost up pseudo up with the same moti vation as for mostfractional branching we choose the vari able with the lowest pseudocosts but this time we always round the variable up first. lowest fractional set down nonunary we intro duce a new nonunary branching heuristic for spp that is based on a construction heuristic for capacitated network de sign problems holmberg and yuan . inspired by this primal heuristic we propose the follow ing. we select the variables in the order of lowest fractional lp value up until the total sum of all fractional values is closest to .. for example say the variables with the lowest fractional values are x with a current lp value of . x with a value of . x with . and x with .. then we would select x x x as their sum is .. had we in cluded x the sum would have been . which has an abso lute difference from . of . whereas the sum without x is only . away. on the other hand had x had a fractional value of . we would have included it as the sum is now . which is only . away from the desired value of .. now we split the search space by requiring that all vari ables equal or that their sum is at least . we branch in the zero direction first. in the example above the branching constraints added are x x x first and on backtrack x x x . note that both constraints are violated by the current lp relaxation. highest fractional set up nonunary we use a modification of the previous approach but this time we fo cus on the highest variables first. we select in order the variables with the highest fractional lp values. for each we consider the missing fraction which is minus the cur rent lp value. again we add these missing fractions until we achieve a sum that is closest to .. in this case we set all variables to first and on backtrack enforce that their sum is lower or equal the number of variables in the set minus . for example assume x has a current lp value of . x . x . and x .. then we branch by enforc ing x x x first. upon backtracking we add the constraint x x x . numerical results implementation we embedded the above heuristics in the stateoftheart mip solver cplex version .. note that we only modify the branching strategy by implementing a branch callback function. when comparing with default cplex we use an empty branch callback to ensure the comparability of the approaches. the empty branch callback causes cplex to compute the branching constraint using its internal default heuristic. we do not change any other cplex behavior the system uses all the standard features like presolving cut ting planes etc. also the search strategy i.e. what open node to consider next is left to cplex. note however that when cplex dives in the tree it considers the first node re turned by the branch callback first so that e.g. it does make a difference whether we round a variable up or down first. trace in our new approach which we refer to as trace the branch callback works as follows. first we compute the features of the current subproblem. we do this by adapt ing the new upper and lower bounds on variables as given by cplex in an internal data structure which incrementally adjusts the feature values. we do this for two reasons. first it is difficult to efficiently get access to the internal presolved problem from cplex. secondly due to cuts added and our nonunary branching constraints the resulting mip will in general no longer be a pure set partitioning problem for which the features were defined. by using cplex bounds on the variables at least we benefit from any inferences that cplex may have conducted which may allow it to set vari ables to one of their bounds even if these variables were not branched on in the path from the root to the current node. we determine how to branch by using the normalized fea tures to find the nearest cluster center and use the heuris tic associated with that cluster. to learn a good assignment of heuristics to clusters offline we employed gga. on our training set with instances learning took cpu days. we will compare trace with the cplex default as well as each of the pure heuristics with pure we mean that we choose one heuristic and stick to it throughout the search each of which we use to solve all our instances. this com parison by itself is what is commonly seen in operations re search papers when it comes to determining the quality of branching heuristics. online best cluster approach obca we add two more approaches to our comparison. the first is the online best cluster approach obca. obca determines offline which pure heuristic works best for each cluster. during the search it determines which cluster is nearest to the current subproblem and uses the associated heuristic for branching. the difference to trace is that the latter uses gga to assign heuristics to clusters. note that trace may very well assign a heuristic to a cluster that is not the best when used through out the entire search. naturally obcas assignment of heuristics to clusters is highly biased by the instances in those clusters. conse quently we consider a version of obca where we add sub problems to the set of instances which are encountered when note that cplex switches off certain heuristics as soon as branch callbacks even empty ones are being used so that the entire search behavior is different. solving the original training instances using the pure heuris tics. this way we might hope to give better search guidance for the subproblems that we will encounter during search. we refer to this version as obca isac the second approach that we add to our comparison is isac. we consider the choice of a pure branching heuristic as a cplex parameter and use isac to determine for which in stance it should employ which pure search heuristic. again we consider a version where we only use our training in stances to determine the clusters as well as an isac where we also add the subproblems encountered during search of pure heuristic to the training set. benchmark instances an important drawback of the research presented here and algorithm tuning and solver portfolios in general is that we need enough training instances to allow the sys tem to learn effectively. this is a hard precondition that must be met before the work presented here can be applied. any benchmark set that consists of only a few dozen in stances is not enough to allow any meaningful learning. in fact we would argue that benchmarks with a low number of instances say less than can hardly be used to draw any conclusions that would generalize. however the fact is that a lot of research is still based on such benchmark sets and often it is even the case that approaches are designed and tested on the very same set of instances. from the research on algorithm tuning we understand in the meantime that re sults obtained in this way cannot be assumed to generalize to other instances. to access a meaningful number of both training and test instances we developed an spp instance generator. unlike most generators in the literature we designed the generator in such a way that it produces a highly heterogeneous set of benchmark instances. the generator first picks uniformly at random a number of bags between and as well as a number of items between and . it then flips three fair coins. the first coin determines whether the costs for each bag are chosen uniformly at random between and or whether this random number also gets multiplied by the number of items in each respective bag making bags with more items more costly in general than bags with low num bers of items. the second coin determines whether for the instance under construction all sets will contain the same number of items or whether the number of items for each bag is determined individually by choosing a density uni formly at random between and of the total number of items. the last coin determines how we fill each bag. we either pick a subset of the desired size uniformly at random out of all such subsets or we cluster the items that get added to bags by choosing a normal distribution around some item index and adding items in the proximity of that target item with higher probability than other items. finally to ensure feasibility for each item the generator adds one bag which contains only that item at a high cost of . the generated spp instances were evaluated with default cplex. to ensure instances of meaningful hardness we kept training set default fractional oracle isac isac obca obca trace instances up average . . . . . . . . . . . . . . median . . . . . . . . min . . . . . . . . max par . . . . . . . . . . . . . shifted geo mean . . . . . . . . average nodes k k k k k k k k k k k k k k k k nodes per second solved unsolved solved . table training results. all times are cpu times in seconds. timeout was seconds. the first instances for which cplex needed at least search nodes to solve the instance but took at most five min utes to solve. we split these instances into training and testing instances. note that this is a very modest training set. a learning approach like the one presented here will generally benefit a lot from larger training sets with at least training instances especially when the instances exhibit such a great diversity. we limited ourselves to a lower number of training instances for several reasons. first it makes learning more challenging. secondly it is more realistic that only a limited number of training instances would be available although again we must assume there are more than a few dozen. finally for isac and obca we wanted to add subproblems encountered while solving the original training instances using the pure heuristics. doing so resulted in clusters with a total of over in stances which were used for training by isac and obca as well as to determine the clusters for trace recall however that for the latter we do not use the subproblem instances for training by gga. resultswe present our experimental results in table and . all experiments were run on dell poweredge ms with xeon . cpus and gb of memory. in the tables apart from the usual average cpu time stan dard deviation median time and number of instances solved timeout was seconds we also give the par score a weighted average where unsolved instances are scored with times the timeout and the shifted geometric mean. the latter is the geometric mean of the runtimes plus sec onds and is used for benchmark sets where runtimes of in dividual instances can differ greatly. this causes the long running instances to dominate the average runtime compar ison. the runtimes are shifted by ten seconds to prevent in stances that are solved in extremely short amounts of time to greatly influence the mean after all in practice we rarely care whether an instance is solved in or milliseconds. considering the pure heuristics first in all measures we observe that on both training and test set the somewhat simplistic most fractional up and most fractional round ing heuristics fare best and even outperform cplex default heuristic. in the table we only present the performance of the best pure heuristic fractional up. in the column oracle the tables give the performance of the fastest pure heuristic for each individual instance. this is a natural limit for the isac approach that at the very best could choose the fastest pure heuristic for each instance. as we can see there is significant room for improvement. unsurprisingly isac is able to realize some of this potential on the training set yet it does not generalize too well. on the test set it would have been better to just use the best heuristic that we had found on the training set. adding subproblems to the training set in isac does not help either. considering obca we find that changing the branch ing heuristic during search is clearly beneficial. obca can reduce the runtime by over compared to the cplex default. surprisingly the performance of obca is much worse. recall that we added some subproblems to the train ing set to allow obca to get a more realistic view into the problems where it will need to make a branching deci sion. clearly this did not work at all. if anything we misled obca by making it consider subproblems it is unlikely to see when the branching heuristic is changed during search. to avoid exactly this problem we had first invented trace. recall that trace only considers subproblem instances for clustering purposes but bases its assignment of heuristics to clusters solely on the performance when running one of the original training instances while changing the branching heuristics in accordance to the heuristiccluster assignment during search. as the experimental results show trace sig nificantly reduces the runtime by over when compared with the cplex default. note that the new branching heuristic is not directly embedded into the system and therefore can not exploit branching heuristics like strongbranching which requires a tight integration with the solver. in light of this an improvement by over one of the most efficient set par titioning solvers is very encouraging and a proof of concept that dynamic branching strategies can be learned effectively even on a relatively small heterogenous set of instances. at the same time trace works very robustly. its stan dard deviations in runtime are lower that those of any pure branching heuristic and the spread of runtimes min to max is also greatly reduced. none of the instances are barely solved within the allowed timeout which cannot be said for any pure heuristic. on the other hand we see that changing heuristics dur ing search imposes a noticeable cost the number of nodes test set default fractional oracle isac isac obca obca trace instances up average . . . . . . . . . . . . . . . . median . . . . . . . min . . . . . . . . max par . . . . . . . . . . . . . . . shifted geo. mean . . . . . . . average nodes k k k k k k k k k k k k k k k k nodes per second solved unsolved solved . table test set results. all times are cpu times in seconds. timeout was seconds. per second is clearly less due to the costly recomputations of the subproblem features. this is outweighed by a very significant reduction in choice points though trace consis tently visits only about of the number of nodes of the cplex default. conclusion we introduced the idea to use an offline algorithm tun ing tool to learn an assignment of branching heuristics to training instance clusters which is used dynamically dur ing search to determine a preferable branching heuristic for each subproblem encountered during search. this approach named trace was evaluated on a set of highly diverse set partitioning instances. we found that the approach clearly outperforms the cplex default and also the best pure branch ing heuristic considered here. while not limited by it it comes very close to choosing the performance of an oracle that magically tells us which pure branching heuristic to use for each individual instance. we conclude that mixing branching heuristics can be very beneficial yet care must be taken when learning when to choose which heuristics as early branching decisions deter mine the distribution of instances that must be dealt with deeper in the tree. we solved this problem by using the offline algorithm tuning tool gga to determine a favorable synchronous assignment of heuristics to clusters so that in stances can be solved most efficiently. our approach requires a reasonable amount of training in stances as well as a number of branching heuristics and of course meaningful features that can characterize subprob lems during search. for practitioners who actually need their problems to be solved repeatedly access to a good number of training instances is less of a problem as it poses for academics. for us the main obstacle of applying trace to other problems is therefore the definition of good problem features. we are currently working on features for general mip problems which we hope can alleviate this issue for a wide variety of problems. references a. braunstein m. mezard r. z. . survey propaga tion an algorithm for satisfiability. random structures and algorithms . a. r. khudabukhsh l. xu h. h. h.k. l.b. . saten stein automatically building local search sat solvers from components. proc. of the th int. joint conference on ar tificial intelligence ijcai . c. ansoteguigil m. sellmann k. t. . a genderbased genetic algorithm for the automatic configuration of solvers. proc. of the th int. conference on the principles and prac tice of constraint programming cp . gagliolo m. and schmidhuber j. . learning dynamic algorithm portfolios. annals of mathematics and artificial intelligence . gomes c. and selman b. . algorithm portfolios. artificial intelligence journal . hamerly g. and elkan c. . learning the k in kmeans. neural information processing systems mit press cam bridge. holmberg k. and yuan d. . lagrangean heuristic based branchandbound approach for the capacitated net work design problem. operations research . l. xu f. hutter h. h. k. l.b. . satzilla portfolio based algorithm selection for sat. jair . leventhal d. and sellmann m. . the accuracy of search heuristics an empirical study on knapsack problems. integration of ai and or techniques in constraint program ming for combinatorial optimization problems cpaior . m. benichou j.m. gauthier p. g. g. h.g. r.o. v. . experiments in mixedinteger programming. math. pro gramming . malitsky y. and sellmann m. . stochastic offline programming. proc. of the th int. conference on tools with artificial intelligenceijait. s. kadioglu y. malitsky m. s. k. t. . sac instance specific algorithm configuration. proc. of the th european conference on artificial intelligence ecai. s. l. epstein e. c. freuder r. j. w. a. m. b. s. . the adaptive constraint engine. proc. of the th int. conference on the principles and practice of constraint programming cp . samulowitz h. and memisevic r. . learning to solve qbf. proc. of the nd national conference on artificial intelligence aaai.  proteus a hierarchical portfolio of solvers and transformations barry hurley lars kotthoff yuri malitsky and barry osullivan insight centre for data analytics department of computer science university college cork ireland b.hurleyl.kotthoffy.malitskyb.osullivanc.ucc.ie abstract. in recent years portfolio approaches to solving sat prob lems and csps have become increasingly common. there are also a num ber of different encodings for representing csps as sat instances. in this paper we leverage advances in both sat and csp solving to present a novel hierarchical portfoliobased approach to csp solving which we call proteus that does not rely purely on csp solvers. instead it may decide that it is best to encode a csp problem instance into sat selecting an appropriate encoding and a corresponding sat solver. our experimental evaluation used an instance of proteus that involved four csp solvers three sat encodings and six sat solvers evaluated on the most chal lenging problem instances from the csp solver competitions involving global and intensional constraints. we show that significant performance improvements can be achieved by proteus obtained by exploiting alter native viewpoints and solvers for combinatorial problemsolving. introduction the pace of development in both csp and sat solver technology has been rapid. combined with portfolio and algorithm selection technology impressive perfor mance improvements over systems that have been developed only a few years previously have been demonstrated. constraint satisfaction problems and satis fiability problems are both npcomplete and therefore there exist polynomial time transformations between them. we can leverage this fact to convert csps into sat problems and solve them using sat solvers. in this paper we exploit the fact that different sat solvers have different performances on different encodings of the same csp. in fact the particular choice of encoding that will give good performance with a particular sat solver is dependent on the problem instance to be solved. we show that in addition to using dedicated csp solvers to achieve the best performance for solving a csp the best course of action might be to translate it to sat and solve it using a sat solver. we name our approach proteus after the greek god proteus the shapeshifting water deity that can foretell the future. our approach offers a novel perspective on using sat solvers for constraint solving. the idea of solving csps as sat instances is not new the solvers sugar azucar and cspsatj are three examples of satbased csp solving. sugar ar x iv . v cs .a i fe b has been very competitive in recent csp solver competitions. it converts the csp to sat using a specific encoding known as the order encoding which will be discussed in more detail later in this paper. azucar is a related sat based csp solver that uses the compact order encoding. however both sugar and azucar use a single predefined solver to solve the encoded csp instances. our work does not assume that conversion using a specific encoding to sat is the best way of solving a problem but considers multiple candidate encodings and solvers. cspsatj uses the satj library as its sat backend and a set of static rules to choose either the direct or the support encoding for each constraint. for intensional and extensional binary constraints that specify the supports it uses the support encoding. for all other constraints it uses the direct encoding. our approach does not have predefined rules but instead chooses the encoding and solver based on features of the problem instance to solve. our approach employs algorithm selection techniques to dynamically choose whether to translate to sat and if so which sat encoding and solver to use otherwise it selects which csp solver to use. there has been a great deal of re search in the area of algorithm selection and portfolios we refer the reader to a recent survey of this work . we note three contrasting example approaches to algorithm selection for the constraint satisfaction and satisfiability problems cphydra csp satzilla sat and isac sat. cphydra contains an algorithm portfolio of csp solvers which partitions cputime between compo nents of the portfolio in order to maximize the probability of solving a given problem instance within a fixed time limit. satzilla at its core uses cost sensitive decision forests that vote on the sat solver to use for an instance. in addition to that it contains a number of practical optimizations for example running a presolver to quickly solve the easy instances. isac is a cluster based approach that groups instances based on their features and then finds the best solver for each cluster. the proteus approach is not a straightforward appli cation of portfolio techniques. in particular there is a series of decisions to make that affect not only the solvers that will be available but also the information that can be used to make the decision. because of this the different choices of conversions encodings and solvers cannot simply be seen as different algorithms or different configurations of the same algorithm. the remainder of this paper is organised as follows. section motivates the need to choose the representation and solver in combination. in section we summarise the necessary background on csp and sat to make the paper selfcontained and present an overview of the main sat encodings of csps. the detailed evaluation of our portfolio is presented in section . we create a portfoliobased approach to csp solving that employs four csp solvers three sat encodings and six sat solvers. finally we conclude in section . multiple encodings and solvers to motivate our work we performed a detailed investigation for two solvers to assess the relationship between solver and problem encoding with features of . . . . . . . . . . . r un t im e fo r m in is at s s at is fia bi lit y constraint tightness t runtime for minisat on satencoded urb csp direct order support satisfiability a performance using minisat. . . . . . . . . . . . r un t im e fo r cl as p s s at is fia bi lit y constraint tightness t runtime for clasp on satencoded urb csp direct order support satisfiability b performance using clasp. fig. . minisat and clasp on random binary csps. the problem to be solved. for this experiment we considered uniform random binary urb csps with a fixed number of variables domain size and number of constraints and varied the constraint tightness. the constraint tightness t is a measure of the proportion of forbidden to allowed possible assignments to the variables in the scope of the constraint. we vary it from to where means that all assignments are allowed and that no assignments are part of a solution in increments of .. at each tightness the mean runtime of the solver on random csp instances is reported. each instance contains variables with domain size and constraints. this allowed us to study the performance of sat encodings and solvers across the phase transition. figure plots the runtime for minisat and clasp on uniformly random binary csps that have been translated to sat using three different encodings. observe that in figure a there is a distinct difference in the performance of minisat on each of the encodings sometimes an order of magnitude. before the phase transition we see that the order encoding achieves the best performance and maintains this until the phase transition. beginning at constraint tightness . the order encoding gradually starts achieving poorer performance and the support encoding now achieves the best performance. notably if we rank the encodings based on their performance the ranking changes after the phase transition. this illustrates that there is not just a single encoding that will perform best overall and that the choice of encoding mat ters but also that this choice is dependent on problem characteristics such as constraint tightness. around the phase transition we observe contrasting performance for clasp as illustrated in figure b. using clasp the ranking of encodings around the phase transition is direct support order whereas for minisat the ranking is order direct support. note also that the peaks at the phase transition differ in magnitude between the two solvers. these differences underline the im portance of the choice of solver in particular in conjunction with the choice of encoding making the two choices in isolation does not consider the interdepen dencies that affect performance in practice. in addition to the random csp instances our analysis also comprises challenging benchmark problem instances from the csp solver competitions that involve global and intensional constraints. figure illustrates the respective performance of the best cspbased and satbased methods on these instances. unsurprisingly the dedicated csp methods often achieve the best performance. there are however numerous cases where considering satbased methods has the potential to yield significant performance improvements. in particular there are a number of instances that are unsolved by any csp solver but can be solved quickly using satbased methods. the proteus approach aims to unify the best of both worlds and take advantage of the potential performance gains. background . the constraint satisfaction problem constraint satisfaction problems csp are a natural means of expressing and reasoning about combinatorial problems. they have a large number of practical applications such as scheduling planning vehicle routing configuration net work design routing and wavelength assignment . an instance of a csp is represented by a set of variables each of which can be assigned a value from its domain. the assignments to the variables must be consistent with a set of constraints where each constraint limits the values that can be assigned to vari ables. finding a solution to a csp is typically done using systematic search based on backtracking. because the general problem is npcomplete systematic search algorithms have exponential worstcase run times which has the effect of limiting the scalability of these methods. however thanks to the development of effective heuristics and a wide variety of solvers with different strengths and weaknesses many problems can be solved efficiently in practice. . . v irt ua l b es t s a t virtual best csp fig. . performance of the virtual best csp portfolio and the virtual best satbased portfolio. each point represents the time in seconds of the two approaches. a point below the dashed line indicates that the virtual best sat portfolio was quicker whereas a point above means the virtual best csp portfolio was quicker. clearly the two ap proaches are complementary there are numerous instances for which a satbased ap proach does not perform well or fails to solve the instance but a csp solver does extremely well and viceversa. . the satisfiability problem the satisfiability problem sat consists of a set of boolean variables and a propositional formula over these variables. the task is to decide whether or not there exists a truth assignment to the variables such that the propositional formula evaluates to true and if this is the case to find this assignment. sat instances are usually expressed in conjunctive normal form cnf. the representation consists of a conjunction of clauses where each clause is a dis junction of literals. a literal is either a variable or its negation. each clause is a logical or of its literals and the formula is a logical and of each clause. the following sat formula is in cnf x x x x x x x this instance consists of four sat variables. one assignment to the variables which would satisfy the above formula would be to set x true x false x true and x true. sat like csp has a variety of practical real world applications such as hard ware verification security protocol analysis theorem proving scheduling rout ing planning digital circuit design . the application of sat to many of these problems is made possible by transformations from representations like the con straint satisfaction problem. we will study three transformations into sat that can benefit from this large collection of solvers. the following sections explain the direct support and directorder encodings that we use. we will use the following notation. the set of csp variables is represented by the set x . we use uppercase letters to denote csp variables in x lowercase xi and xv refer to sat variables. the domain of a csp variable x is denoted dx and has size d. . direct encoding translating a csp variable x into sat using the direct encoding also known as the sparse encoding creates a sat variable for each value in its domain x x . . . xd. if xv is true in the resulting sat formula then x v in the csp solution. this means that in order to represent a solution to the csp exactly one of x x . . . xd must be assigned true. we add an atleastone clause to the sat formula for each csp variable as follows x x x x . . . xd. conversely to ensure that only one of these can be set to true we add atmost one clauses. for each pair of distinct values in the domain of x we add a binary clause to enforce that at most one of the two can be assigned true. the series of these binary clauses ensure that only one of the sat variables representing the variable will be assigned true i.e. v w dx xv xw. constraints between csp variables are represented in the direct encoding by enumerating the conflicting tuples. for binary constraints for example we add clauses as above to forbid both values being used at the same time for each disallowed assignment. for a binary constraint between a pair of variables x and y we add the conflict clause xv yw if the tuple x v y w is forbidden. for intensionally specified constraints we enumerate all possible tuples and encode the disallowed assignments. example direct encoding. consider a simple csp with three variables x xy z each with domain . we have an alldifferent constraint over the variables alldifferentxy z which we represent by encoding the pairwise dis equalities. table shows the complete directencoded cnf formula for this csp. the first clauses encode the domains of the variables the remaining clauses encode the constraints between x y and z. there is an implicit conjunction between these clauses. table . an example of the direct encoding. domain clauses x x x x x x x x x y y y y y y y y y z z z z z z z z z x y x y x y x y x z x z x z x z y z y z y z y z . support encoding the support encoding uses the same mechanism as the direct encoding to encode csp domains into sat each value in the domain of a csp variable is encoded as a sat variable which represents whether or not it takes that value. however the support encoding differs on how the constraints between variables are encoded. given a constraint between two variables x and y for each value v in the domain of x let syxv dy be the subset of the values in the domain of y which are consistent with assigning x v. either xv is false or one of the consistent assignments from y . . . yd must be true. this is encoded in the support clause xv isyxv yi . conversely for each value w in the domain of y a support clause is added for the supported values in x which are consistent with assigning y w. an interesting property of the support encoding is that if a constraint has no consistent values in the corresponding variable a unitclause will be added thereby pruning the values from the domain of a variable which cannot exist in any solution. also a solution to a sat formula without the atmostone con straint in the support encoding represents an arcconsistent assignment to the csp. unit propagation on this sat instance establishes arcconsistency in optimal worstcase time for establishing arcconsistency . example support encoding. table gives the complete supportencoded cnf formula for the simple csp given in example . the first clauses encode the domains and the remaining ones the support clauses for the constraints. there is an implicit conjunction between clauses. . order encoding unlike the direct and support encoding which model x v as a sat variable for each value v in the domain of x the order encoding also known as the regular encoding creates sat variables to represent x v. if x is less than or equal to v denoted xv then x must also be less than or equal to v table . an example of the support encoding. domain clauses x x x x x x x x x y y y y y y y y y z z z z z z z z z x y x y y x y y x y y y x x y x x y x x x z x z z x z z x z z z x x z x x z x x y z y z z y z z y z z z y y z y y z y y xv. therefore we add clauses to enforce this consistency across the domain as follows dv xv xv. this linear number of clauses is all that is needed to encode the domain of a csp variable into sat in the order encoding. in contrast the direct and support encodings require a quadratic number of clauses in the domain size. the order encoding is naturally suited to modelling inequality constraints. to state x we would just post the unit clause x. if we want to model the constraint x v we could rewrite it as x v x v. x v can then be rewritten as x v . to state that x v in the order encoding we would encode xv xv. a conflicting tuple between two variables for example x v y w can be written in propositional logic and simplified to a cnf clause using de morgans law xv xv yw yw xv xv yw yw xv xv yw yw xv xv yw yw example order encoding. table gives the complete orderencoded cnf for mula for the simple csp specified in example . there is an implicit conjunction between clauses in the notation. . combining the direct and order encodings the direct encoding and the order encoding can be combined to produce a po tentially more compact encoding. a variables domain is encoded in both repre sentations and clauses are added to chain between them. this gives flexibility in the representation of each constraint. here we choose the encoding which gives the most compact formula. for example for inequalities we use the order encod ing since it is naturally suited but for a disequality we would use the direct encoding. this encoding is referred to as directorder throughout the paper. table . an example of the order encoding. domain clauses x x x x x y y y y y z z z z z x y x y x x y y x x y y x z x z x x z z x x z z y z y z y y z z y y z z . algorithm portfolios the algorithm selection problem is to select the most appropriate algo rithm for solving a particular problem. it is especially relevant in the context of algorithm portfolios where a single solver is replaced with a set of solvers and a mechanism for selecting a subset to use on a particular problem. algorithm portfolios have been used with great success for solving both sat and csp instances in systems such as satzilla isac or cphydra . most approaches are similar in that they relate the characteristics of a problem to solve to the performance of the algorithms in the portfolio. the aim of an algorithm selection model is to provide a prediction as to which algorithm should be used to solve the problem. the model is usually induced using some form of machine learning. there are three main approaches to using machine learning to build algorithm selection models. first the problem of predicting the best algorithm can be treated as a classification problem where the label to predict is the algorithm. second the training data can be clustered and the algorithm with the best performance on a particular cluster assigned to it. the cluster membership of any new data decides the algorithm to use. finally regression models can be trained to predict the performance of each portfolio algorithm in isolation. the best algorithm for a problem is chosen based on the predicted performances. our approach makes a series of decisions whether a problem should be solved as a csp or a sat problem which encoding should be used for converting into sat and finally which solver should be assigned to tackle the problem. approaches that make a series of decisions are usually referred to as hierarchical models. and use hierarchical models in the context of a sat portfolio. they first predict whether the problem to be solved is expected to be satisfiable or not and then choose a solver depending on that decision. our approach is closer to which first predicts what level of consistency the alldifferent constraint should achieve before deciding on its implementation. to the best of our knowledge no portfolio approach that potentially trans forms the representation of a problem in order to be able to solve it more effi ciently exists at present. experimental evaluation . setup the hierarchical model we present in this paper consists of a number of layers to determine how the instance should be solved. at the top level we decide whether to solve the instance using as a csp or using a satbased method. if we choose to leave the problem as a csp then one of the dedicated csp solvers must be chosen. otherwise we must choose the sat encoding to apply followed by the choice of sat solver to run on the satencoded instance. each decision of the hierarchical approach aims to choose the direction which has the potential to achieve the best performance in that subtree. for exam ple for the decision to choose whether to solve the instance using a satbased method or not we choose the satbased direction if there is a sat solver and encoding that will perform faster than any csp solver would. whether this par ticular encodingsolver combination will be selected subsequently depends on the performance of the algorithm selection models used in that subtree of our decision mechanism. for regression models the training data is the best perfor mance of any solver under that branch of the tree. for classification models it is the label of the subbranch with the virtual best performance. this hierarchical approach presents the opportunity to employ different de cision mechanisms at each level. we consider regression classification and clustering algorithms which are listed below. for each of these algorithms we evaluate the performance using fold crossvalidation. the dataset is split into partitions with approximately the same size and the same distribution of the best solvers. one partition is used for testing and the remaining partitions as the training data for the model. this process is repeated with a different par tition considered for testing each time until every partition has been used for testing. we measure the performance in terms of par score. the par score for an instance is the time it takes the solver to solve the instance unless the solver times out. in this case the par score is ten times the timeout value. the sum over all instances is divided by the number of instances. instances. in our evaluation we consider csp problem instances from the csp solver competitions . of these we consider all instances defined using global and intensional constraints that are not trivially solved during seconds of feature computation. we also exclude all instances which were not solved by any csp or sat solver within the time limit of hour. altogether we obtain nontrivial instances from problem classes such as timetabling frequency assignment jobshop openshop quasigroup costas array golomb ruler latin square all interval series balanced incomplete block design and many others. this set includes both small and large arity constraints and all of the global constraints used during the csp solver competitions alldifferent element weighted sum and cumulative. for the satbased approaches numberjack was used to translate a csp instance specified in xcsp format into sat cnf. features. a fundamental requirement of any machine learning algorithm is a set of representative features. we explore a number of different feature sets to train our models i features of the original csp instance ii features of the directencoded sat instance iii features of the supportencoded sat instance iv features of the directorderencoded sat instance and v a combination of all four feature sets. these features are described in further detail below. we computed the features used in cphydra for each csp instance using mistral for reasons of space we will not enumerate them all here. the set includes static features like statistics about the types of constraints used average and maximum domain size and dynamic statistics recorded by running mistral for seconds average and standard deviation of variable weights number of nodes number of propagations and a few others. instances which are solved by mistral during feature computation are filtered out from the dataset. in addition to the csp features we computed the sat features used by satzilla for each of the encoded instances and different encodings. the features encode a wide range of different information on the problem such as problem size features of the graphbased representation balance features the proximity to a horn formula dpll probing features and local search probing features. constraint solvers. our csp models are able to choose from complete csp solvers abscon choco gecode and mistral . satisfiability solvers. we considered the following complete sat solvers clasp cryptominisat glucose lingeling riss and minisat . learning algorithms. we evaluate a number of regression classification and clustering algorithms using weka . all algorithms unless otherwise stated use the default parameters. the regression algorithms we used were linear regression paceregression reptree mrules mp and smoreg. the clas sification algorithms were bayesnet bftree conjunctiverule decisiontable ft hyperpipes ibk nearest neighbour with and neighbours j jgraft jrip ladtree multilayerperceptron oner part randomforest randomforest with random trees randomtree reptree and simplelogis tic. for clustering we considered em farthestfirst and simplekmeans. the farthestfirst and simplekmeans algorithms require the number of clusters to table . performance of the learning algorithms for the hierarchical approach. the category bests consists of the hierarchy of algorithms where at each node of the tree of decisions we take the algorithm that achieves the best par score for that particular decision. classifier mean par number solved vbs proteus mp with csp features category bests mrules with csp features mp with all features linearregression with all features linearregression with csp features multilayerperceptron with csp features lm with csp features randomforest with csp features ibk with csp features randomforest with all features be given as input. we evaluated with multiples of through of the number of solvers in the respective data set given as the number of clusters. the number of clusters is represented by n n and so on in the name of the algorithm where n stands for the number of solvers. we use the llama toolkit to train and test the algorithm selection models. . portfolio and solver results the performance of each of the sat solvers was evaluated on the three sat encodings of csp competition benchmarks with a timeout of hour and limited to gb of ram. the csp solvers were evaluated on the original csps. our results report the par score and number of instances solved for each of the algorithms we evaluate. the par is the sum of the runtimes over all instances counting times the timeout if that was reached. data was collected on a cluster of intel xeon e processors .ghz running centos .. this data is available online. the performance of a number of hierarchical approaches is given in table . the hierarchy of algorithms which produced the best overall results for our dataset involves mp regression with csp features at the root node to choose sat or csp mp regression with csp features to select the csp solver lin earregression with csp features to select the sat encoding linearregression with csp features to select the sat solver for the direct encoded instance lin earregression with csp features to select the sat solver for the directorder httpc.ucc.iebhurleyproteus httpc.ucc.iebhurleyproteus solve as csp solve as sat encode with direct encoding encode with directorder encoding encode with support encoding linear regression with csp features mp regression with csp features mp regression with csp features linear regression with csp features linear regression with csp features linear regression with directorder features fig. . overview of the machine learning models used in the hierarchical approach. encoded instance and linearregression with the directorder features to select the sat solver for the support encoded instance. the hierarchical tree of specific machine learning approaches we found to deliver the best overall performance on our data set is labelled proteus and is depicted in figure . we would like to point out that in many solver competitions the difference between the top few solvers is fewer than additional instances solved. in the sat challenge for example the difference between the first and second place single solver was only instances and the difference among the top solvers was only instances. the results we present in table are therefore very significant in terms of the gains we are able to achieve. our results demonstrate the power of proteus. the performance it delivers is very close to the virtual best vbs that is the best performance possible if an oracle could identify the best choice of representation encoding and solver on an instance by instance basis. the improvements we achieve over other approaches are similarly impressive. the results conclusively demonstrate that having the option to convert a csp to sat does not only have the potential to achieve significant performance improvements but also does so in practice. an interesting observation is that the csp features are consistently used in each of the top performing approaches. one reason for this is that it is quicker to compute only the csp features instead of the csp features then converting to sat and computing the sat features in addition. the additional overhead of computing sat features is worthwhile in some cases though for example for linearregression which is at its best performance using all the different feature sets. note that for the best tree of models cf. figure it is better to use the features of the directorder encoding for the decision of which solver to choose for a supportencoded sat instance despite the additional overhead. we also compare the hierarchical approach to that of a flattened setting with a single portfolio of all solvers and encoding solver combinations. the flattened portfolio includes all possible combinations of the encodings and the sat table . ranking of each classification regression and clustering algorithm to choose the solving mechanism in a flattened setting. the portfolio consists of all possible combination of the encodings and the sat solvers and the csp solvers for a total of solvers. classifier mean par number solved vbs proteus linearregression with all features mp with csp features linearregression with csp features lm with all features lm with csp features mp with all features randomforest with all features smoreg with all features randomforest with all features ibk with csp features solvers and the csp solvers for a total of solvers. table shows these results. the regression algorithm linearregression with all features gives the best performance using this approach. however it is significantly worse than the performance achieved by the hierarchical approach of proteus. . greater than the sum of its parts given the performance of proteus the question remains as to whether a different portfolio approach that considers just csp or just sat solvers could do better. ta ble summarizes the virtual best performance that such portfolios could achieve. we use all the csp and sat solvers for the respective portfolios to give us vb csp and vb sat respectively. the former is the approach that always chooses the best csp solver for the current instance while the latter chooses the best sat encodingsolver combination. vb proteus is the portfolio that chooses the best overall approachencoding. we show the actual performance of proteus for comparison. proteus is better than the virtual bests for all portfolios that con sider only one encoding. this result makes a very strong point for the need to consider encoding and solver in combination. proteus outperforms four other vb portfolios. specifically the vb cphydra is the best possible performance that could be obtained from that portfolio if a perfect choice of solver was made. neither satzilla nor isacbased portfolios consider different sat encodings. therefore the best possible performance either of them could achieve for a specific encoding is represented in the last three lines of table . these results do not only demonstrate the benefit of considering the differ ent ways of solving csps but also eliminate the need to compare with existing portfolio systems since we are computing the best possible performance that any table . virtual best performances ranked by par score. method mean par number solved vb proteus proteus vb csp vb cphydra vb sat vb directorder encoding vb direct encoding vb support encoding of those systems could theoretically achieve. proteus impressively demonstrates its strengths by significantly outperforming oracle approaches that use only a single encoding. conclusions we have presented a portfolio approach that does not rely on a single problem representation or set of solvers but leverages our ability to convert between problem representations to increase the space of possible solving approaches. to the best of our knowledge this is the first time a portfolio approach like this has been proposed. we have shown that to achieve the best performance on a constraint satisfaction problem it may be beneficial to translate it to a satisfiability problem. for this translation it is important to choose both the encoding and satisfiability solver in combination. in doing so the contrasting performance among solvers on different representations of the same problem can be exploited. the overall performance can be improved significantly compared to restricting the portfolio to a single problem representation. we demonstrated empirically the significant performance improvements pro teus can achieve on a large set of diverse benchmarks using a portfolio based on a range of different stateoftheart solvers. we have investigated a range of dif ferent csp to sat encodings and evaluated the performance of a large number of machine learning approaches and algorithms. finally we have shown that the performance of proteus is close to the very best that is theoretically possible for solving csps and significantly outperforms the theoretical best for portfolios that consider only a single problem encoding. in this work we make a general decision to encode the entire problem using a particular encoding. a natural extension would be to mix and vary the encoding depending on attributes of the problem. an additional avenue for future work would be to generalize the concepts in this paper to other problem domains where transformations like csp to sat exist. acknowledgements. this work is supported by science foundation ireland sfi grant in.i and fp fetopen grant . the insight centre for data analytics is supported by sfi grant sfirc. references . csp solver competition benchmarks. httpwww.cril.univartois.fr lecoutrebenchmarks.html . ansotegui c. manya f. mapping problems with finitedomain variables into problems with boolean variables. in the th international conference on theory and applications of satisfiability testing sat . audemard g. simon l. glucose . in the sat competition. proceedings of sat competition p. . biere a. lingeling plingeling and treengeling entering the sat competition . proceedings of sat competition . biere a. heule m.j.h. van maaren h. walsh t. eds. handbook of satis fiability frontiers in artificial intelligence and applications vol. . ios press february . een n. sorensson n. minisat .. httpminisat.se . gebser m. kaufmann b. neumann a. schaub t. clasp a conflictdriven answer set solver. in logic programming and nonmonotonic reasoning . pp. . springer . gecode team gecode generic constraint development environment httpwww.gecode.org . gent i.p. arc consistency in sat. in proceedings of the th european confer ence on artificial intelligence ecai. pp. . gent i.p. kotthoff l. miguel i. nightingale p. machine learning for constraint solver design a case study for the alldifferent constraint. in rd workshop on techniques for implementing constraint programming systems trics. pp. . gomes c.p. selman b. algorithm portfolios. artificial intelligence . haim s. walsh t. restart strategy selection using machine learning techniques. in proceedings of the th international conference on theory and applications of satisfiability testing sat. pp. . springerverlag berlin heidelberg . hall m. frank e. holmes g. pfahringer b. reutemann p. witten i.h. the weka data mining software an update. sigkdd explor. newsl. nov . hebrard e. mistral a constraint satisfaction library. in proceedings of the third international csp solver competition . hebrard e. omahony e. osullivan b. constraint programming and combi natorial optimisation in numberjack. in integration of ai and or techniques in constraint programming for combinatorial optimization problems th interna tional conference cpaior . pp. . huberman b.a. lukose r.m. hogg t. an economics approach to hard com putational problems. science . kadioglu s. malitsky y. sellmann m. tierney k. isac instancespecific algorithm configuration. in coelho h. studer r. wooldridge m. eds. ecai. frontiers in artificial intelligence and applications vol. pp. . ios press . kasif s. on the parallel complexity of discrete relaxation in constraint sat isfaction networks. artificial intelligence oct httpdx. doi.org.o httpwww.cril.univartois.frlecoutrebenchmarks.html httpwww.cril.univartois.frlecoutrebenchmarks.html httpminisat.se httpwww.gecode.org httpdx.doi.org.o httpdx.doi.org.o . kotthoff l. llama leveraging learning to automatically manage algorithms. tech. rep. arxiv. arxiv jun httparxiv.orgabs. . kotthoff l. algorithm selection for combinatorial search problems a survey. ai magazine to appear . le berre d. lynce i. cspsatj a simple csp to sat translator. in pro ceedings of the second international csp solver competition . lecoutre c. tabary s. abscon toward more robustness. in proceedings of the third international csp solver competition . manthey n. the sat solver rissg at sc . proceedings of sat competi tion p. . omahony e. hebrard e. holland a. nugent c. osullivan b. using case based reasoning in an algorithm portfolio for constraint solving. proceeding of the th irish conference on artificial intelligence and cognitive science . rice j.r. the algorithm selection problem. advances in computers . rossi f. van beek p. walsh t. handbook of constraint programming. foun dations of artificial intelligence elsevier new york ny usa . roussel o. lecoutre c. xml representation of constraint networks format xcsp .. corr abs. . soos m. cryptominisat .. . tamura n. tanjo t. banbara m. system description of a satbased csp solver sugar. in proceedings of the third international csp solver competition. pp. . tanjo t. tamura n. banbara m. azucar a satbased csp solver using compact order encoding tool presentation. in proceedings of the th in ternational conference on theory and applications of satisfiability testing sat. pp. . springer . choco team choco an open source java constraint programming library . walsh t. sat v csp. in principles and practice of constraint programming cp . vol. pp. . springerverlag . xu l. hoos h.h. leytonbrown k. hierarchical hardness models for sat. in principles and practice of constraint programming cp. pp. . xu l. hutter f. hoos h.h. leytonbrown k. satzilla portfoliobased al gorithm selection for sat. journal of artificial intelligence research pp. httparxiv.orgabs. proteus a hierarchical portfolio of solvers and transformations  reactr realtime algorithm configuration through tournament rankings reactr realtime algorithm configuration through tournament rankings tadhg fitzgerald insight centre for data analytics university college cork ireland tadhg.fitzgeraldinsightcentre.org yuri malitsky ibm t.j. watson research centre new york usa ymalitsus.ibm.com barry osullivan insight centre for data analytics university college cork ireland barry.osullivaninsightcentre.org abstract it is now readily accepted that automated algorithm configuration is a necessity for ensuring optimized performance of solvers on a particular problem do main. even the best developers who have carefully designed their solver are not always able to manu ally find the best parameter settings for it. yet the opportunity for improving performance has been repeatedly demonstrated by configuration tools like paramils smac and gga. however all these techniques currently assume a static environment where demonstrative instances are procured before hand potentially unlimited time is provided to ad equately search the parameter space and the solver would never need to be retrained. this is not always the case in practice. the react system proposed in demonstrated that a solver could be con figured during runtime as new instances arrive in a steady stream. this paper further develops that approach and shows how a ranking scheme like trueskill can further improve the configurators performance making it able to quickly find good parameterizations without adding any overhead on the time needed to solve any new instance and then continuously improve as new instances are evalu ated. the enhancements to react that we present enable us to even outperform existing static config urators like smac in a nondynamic setting. introduction automated algorithm configuration is the task of automati cally finding parameter settings for a tunable algorithm which improve its performance. it is an essential tool for anyone wishing to maximize the performance of an existing algo rithm without modifying the underlying approach. modern work on algorithm portfolios malitsky et al. b xu et al. malitsky and sellmann tells us that there is often no single approach that will produce opti mal performance solving time solution quality etc. in every situation. this is why modern developers not knowing all the conditions and scenarios where their work will be employed leave many of the parameters of their algorithms open to the user. as a sideeffect of this however solvers might have hundreds of parameters that can be changed to fine tune their behavior to a specific benchmark. even for solvers with a handful of parameters it is difficult to search through all the possibly nonlinear relations between parameters. to alleviate the difficult and time consuming process of manually fiddling with parameters a number of tools have been recently introduced. paramils hutter et al. for example employs an iterated local search to explore the pa rameter space focussing on areas where it found improve ments in the past. alternatively smac hutter et al. tries to build an internal model that predicts the performance of a parameterization trying the ones most probable to im prove upon the current behavior. finally gga ansotegui et al. utilizes a genetic approach competing a number of parameterizations in parallel and allowing the best ones to pass on their parameter settings to the subsequent generation. while there is no consensus on which of these approaches is best all of them have been repeatedly validated in practice sometimes leading to orders of magnitude improvements over what was found by human experts hutter et al. . despite their undeniable success all existing configurators take a static view of the problem. they assume a trainonce scenario where a rich benchmark set of instances exists and significant time can be spent offline searching for the best pa rameterizations. furthermore they assume that once a good parameterization is found it will be utilized without modi fication forever. while applicable in many situations there are cases when these three assumptions do not hold. imag ine for example the case of repeated combinatorial auctions like those commonly utilized for placing ads on webpages. new companies ads and keywords are constantly being in troduced which means that the size and number of goods are constantly in flux. furthermore new instances arrive in a continuous stream. this means that the benchmark instances are always changing there is no time to train offline and the problems we are solving change meaning that the parame ters need to be constantly updated. existing configuration approaches are illequipped to deal with this setting. some work exists on online learning in the closely related area of algorithm portfolios. sunny a lazy portfolio ap proach for constraint solving amadini et al. builds a schedule of csp solvers in a portfolio online without any prior training. sunny finds similar instances to the cur rent instance using knearest neighbours. it then selects a proceedings of the twentyfourth international joint conference on artificial intelligence ijcai minimum subset of solvers that could the solve the greatest number of neighbouring instances and schedules them based the number of neighbouring instances solved. alternatively evolving instancespecific algorithm configuration malit sky et al. a is a portfolio approach that performs some offline learning but is able to evolve the portfolio it uses as it processes a stream of instances in order to adapt to changing instances. it does this by dynamically reclustering incoming instances and assigning a set of solvers to each cluster. react realtime algorithm configuration through tour naments provides a way to perform online algorithm config uration without prior learning fitzgerald et al. . the idea was that once a new instance arrived n parameteriza tions of a solver would attempt to solve it in parallel. once the instance was solved all other solvers were terminated and the winners score was upgraded. because the current best parameterization was always among the n versions eval uated a continuously improving bound on performance could be guaranteed without offline training. furthermore once a parameterization failed to win a certain percentage of the time it was discarded with a new random one taking its place in the current pool. in this way react was able to also search the parameter space while it was solving new in stances. although straightforward in its implementation the original work showed that react could very quickly find high quality parameterizations in realtime. this paper extends the react approach by introducing enhancements to every part of the original. specifically we show which parameterizations should be evaluated once a new instance arrives. we also show how and which instances should be introduced into the pool of potentials to be evalu ated. most importantly we show how a ranking scheme com monly utilized to rank players in games can be exploited to more accurately measure the quality of the parameterizations in the current pool. by introducing all of these changes we show how the resulting approach reactr realtime algo rithm configuration through tournament rankings improves over the original and even surpasses existing static configu ration techniques when evaluated in a static setting. approach demonstrated as a powerful approach in practice react showed that it was feasible to configure a solver in realtime as new instances were being solved. at its core the algorithm can be abstracted to algorithm taking in four parameters number of available cores n parameterized solver s a stream of instances i and the size m of the parameter pool to keep track of. internally a pool p of parameterizations is initial ized where each has a corresponding score recorded in e. solving the stream of instances one at a time a collection of n competitors c is selected to tackle the instance and all are evaluated in parallel. here as soon as any parameterization finishes solving the instance all are terminated and the re sults are recorded in e. finally the scores are processed and the pool of potential solvers is updated to replace any poorly performing parameterizations with a new set. originally the pool of parameterizations was initialized by creating random assignments for all the parameters. to en algorithm components of the react algorithm function reactn s i m p initializeparameterizationssm e n . an n vector initialized to for i i do c getcompetitorsp n e solveinstancec i e updatescorese pe updateparameterizationspe sure that the performance was always improving the cur rent best parameterization was always among the evaluated competitors while all others were sampled uniformly at ran dom. this way once one parameterization finished solving the instance it was recorded as defeating all the other com petitors. an internal counter would subsequently keep track of the number of times a configuration defeated another. us ing this counter a parameterization was removed as soon as any other parameterization defeated it twice as many times as the two competed as long as there were at least a mini mum number of direct comparisons of the two. finally all removed configurations were replaced by new randomly gen erated ones. it follows that the main reason for reacts success was that it always ran the best parameterization and had a very aggressive removal policy throwing out anything as soon as it was even hinted to be subpar. the trouble with this strategy however was that it failed to store a history of candidate con figuration successes. due to this a new configuration could potentially remove a tried and tested candidate simply by get ting a few lucky runs when it was first added to the pool. therefore it should be clear from algorithm that the success of react resides in the strategies used for each of the steps. particularly important is the consideration of which configurations to evaluate next which parameteriza tions should be discarded and how new parameterizations should be added to the pool. this paper targets each of these questions showing how the employment of a leaderboard of the current pool facilitates all other decisions. the remain der of this section details the strategies employed to develop reactr. . ranking parameterizations in the world of competitive games it is critical to have an un biased way to compare an individuals performance to every one else. a trivial way to do this is to simply have everyone play everyone else. naturally for popular games like chess go checkers etc there are a plethora of people playing at any given time with players coming and going from the rankings on a whim. this is also the situation that react faces inter nally. at any given time there are a predetermined number of competitors in the pool which can be removed and replaced by new entrants. at the same time react needs a method for quickly determining the comparative quality of each of the contestants in the pool to know the ones worth utilizing and those that can be discarded. it therefore makes sense to employ a leaderboard ranking algorithm. historically in order to improve the chess rating system at the time the elo rating system elo was introduced with the idea that the point difference in two players ratings should correspond to a probability predictor on the outcome of a match. specifically and somewhat arbitrarily the elo rating was designed so that a difference of points corre sponded to an expected score of . with an average player having points. after a tournament if it was found that the actual score sa was higher lower than the expected score ea it was assumed that the rank was too low high and thus needed to be adjusted linearly as ra ra ksa ea where k was a constant limiting the rate of change. an alternative version was later introduced in glicko glickman which has a builtin mea sure of the accuracy of a players rating rd as well as the expected fluctuation in the rating . specifically the method was designed so that the more games a player is involved in the more confident glicko was about the ranking and the more consistently a player performs the lower the volatility. to represent this a glicko rating is provided as a confidence range with the lower upper value being the rank minus plus twice the rd. while both elo and glicko are still in use to this day they are primarily designed for two player games. therefore for games involving players all combinations of pairs must be created and updated independently in order for either of these classic approaches to work. to solve this multiplayer problem for online games the bayesian ranking algorithm trueskill herbrich et al. was invented. like glicko trueskill measures both a players average skill as well as the degree of uncertainty standard deviation assum ing a gaussian distribution for a players skill. trueskill uses bayesian inference in order to rank players. after a tour nament all competitors are ranked based on the result with the mean skill shifting based on the number of players be low a players rank and the number above weighted by the difference in initial average ratings. the result of this is that a player that is expected to win higher value gains lit tle by beating a lower ranked opponent. however if a lower ranked player beats a higher ranked player then the lower ranked player will receive a large increase in their value. as a player competes in more tournaments trueskill becomes more confident in the that is assigned and so the uncertainty value is reduced after every tournament played. for reactr the confidence metric provided by glicko and trueskill is a highly desirable feature that could help determine whether it is worth continuing to evaluate a param eterization or whether it can be safely discarded. we imple mented and experimented with both ranking algorithms with figure showing a typical result. what the figure shows is the cumulative time the reactr algorithm with the two ranking methods needs to go through all instances from a particular benchmark. while we will describe the benchmarks later in the paper what is important to note here is that in a typical situation trueskill is able to help bring better parameteriza tions to the top resulting in better overall performance in the expected score is the probability of winning plus half the prob ability of a tie. figure cumulative solving time using glicko and trueskill for ranking. long run. for this reason that in all subsequent parts of this paper we only rely on the trueskill ranking methodology. we use a highlyrated open source python implementation of trueskill for our experiments zongker using all of the default settings .. . choosing the competitors using trueskill for ranking means that each member in our current pool of thirty potential parameterizations has an asso ciated score declaring its quality as well as confidence rating of this score. to guarantee that the overall solver will never produce bad performance the best known configuration is al ways among those that is evaluated. but other than that it is not immediately clear exactly which other parameteriza tions should be evaluated in each tournament. we refer to the method used to select the configurations to run from the leaderboard as the sampling strategy. restricting ourselves to only selecting six configurations the number of cores we typically use for parallel execu tion we compare several strategies on an assortment of auc tion problems. again these problems and the parameterized solver we use are described in detail in a later section. for now we treat this dataset and the thirty randomly generated parameterizations as a black box. the objective is to choose to run solvers such that the cumulative time necessary to go through all the instances is minimized. utilizing this setup and the trueskill approach to rank the solvers in our pool we compare five sampling strategies. specifically we compare strategies that take the top n solvers and the n solvers uniformly at random. the results are shown in figure . what we observe in this figure is that running the top three and a random set of three is the worst strategy resulting the solver spending the most time needing to go through all the instances. alternatively running just the top single solver and five random ones allows reactr to better find a good parameterization in the pool and thus lowering the total time. having said this we also note that there is not much differ ence in which strategy is selected. therefore we hedge our figure cumulative solving time for reactr with differ ent sampling strategies. bets by in practice running the top two known solvers and the others chosen informally at random. we intuitively as sume that by running the top two solvers we would be in bet ter shape in knowing the quality of the best solvers. . cleaning the pool even with a high quality ranker such as trueskill if there are no good parameterizations in our pool of contenders we will never be able to improve our overall performance. therefore it is imperative for reactr to constantly discard the ob viously inferior parameterizations and to replace them with new contenders. this leaves the question as to which strat egy to adopt for removing parameterizations. a more aggres sive removal strategy allows us to evaluate more new param eters thus increasing the chances of finding better parame terizations however this comes with more risk of removing good parameterizations prematurely. a more conservative re moval strategy allows us to evaluate each parameterization more fully but because parameterizations are removed more slowly less of the configuration space is explored. the ques tions we must answer are therefore how many configurations should we keep and how confident should we be that a con figuration is poor before removing it. because the evaluation of reactr on real data requires us to run a solver for nontrivial amount of time over a large number of instances finding the best strategy for removing instances can be extremely expensive. to overcome this we simulate the process using synthetic data. specifically we know that because our instances are relatively homogeneous in practice any parameterization of a solver would have a par ticular expected performance with some variance. we further assume that there is a particular mean expected performance and a mean variance. therefore each parameterization of a solver is simulated by a normal distribution random number generator with a fixed mean and standard deviation. once this simulated solver is removed it is replaced by another one where the new mean and variance are assigned randomly ac cording to a global normal random number generator. what this means is that most of our simulated solvers have a sim ilar performance with those being significantly better than others being increasingly unlikely. because in practice we introduce new parameterizations sampled uniformly at ran dom this methodology of representing solvers is very close to reality as independent not shown tests revealed. given these simulated solvers the objective of reactr is to find a solver that leads to the shortest cumulative solv ing time for instances. of course since these solvers are random we repeat the experiment several times. what we observe is displayed in figure . specifically we observe that the best strategy for removing parameterizations from the pool of contenders is by keeping the top solvers and anything in which we have a greater than .. uncertainty rating. figure heatmap showing the combined effect of the num ber of kept parameterizations and trueskills confidence rat inglower values indicate higher confidence. the colorbar shows the cumulative solving time in seconds. . replenishing the pool every parameterization which is removed from the leader board must be replaced by a newly generated configuration. in order to balance exploration of new configurations and the exploitation of the knowledge we have already gained re actr uses two different generation strategies. diversity is ensured by generating parameterizations where the value of each parameter is set to a random value from the range of allowed values for that parameter. additionally reactr exploits the knowledge it has al ready gained through ranking by using a crossover operation similar to that used in genetic algorithms. for this two par ents are chosen from among the five highest ranked configu rations then with equal probability each parameter takes one of the parents values. additionally like a standard genetic algorithm some small percentage of the parameters are al lowed to mutate. that is rather than assuming one of the parent values a random valid value is assigned instead. we allow for a variable to control the balance of exploita tion to exploration or the percentage of generated vs random parameterizations we introduce. again for now treating the specific collection of instances and parameterized solver as a blackbox figure shows the effect on cumulative time when reactr varies the ratio of the amount of exploitation to do. figure cumulative solving time for reactr with differ ent exploitation ratio settings. from this graph we can see that at least in the relation of these two methodologies it is much better to actively ex ploit existing knowledge generating a greater proportion of parameterizations using crossover. experimental setup we evaluate the reactr methodology on three datasets. the first two are variations of combinatorial auction problems which were used in the evaluation of the original react con figurator. a combinatorial auction is a type of auction where bids are placed on groups of goods rather than single items. these instances are encoded as mixedinteger programming mip problems and solved using the stateoftheart com mercial optimizer ibm cplex ibm . the problems were generated using the combinatorial auction test suite cats leytonbrown et al. which can generate in stances based on five different distributions aiming to match realworld domains. the two combinatorial auction datasets are generated based on the regions and arbitrary domains. regions sim ulates combinatorial auctions where the adjacency in space of goods matters. in practice this would be akin to selling parcels of land or radio spectrum. alternatively the arbitrary domain foregoes a clear connection between the goods being auctioned as may be the case with artwork or antiques. these two domains generate instances which are dissimilar enough to warrant different strategies for solving them and by exten sion different parameter configurations for the solver. the regions dataset was generated with the number of goods set to standard deviation and the bid count to standard deviation . similarly goods stan dard deviation and bids standard deviation were used for the arbitrary dataset. these particular val ues were chosen for generation as they produce diverse in stances that are neither too easy nor too hard for our solver. furthermore the dataset is cleaned by removing any instance which is solvable in under seconds using the cplex de fault settings. these are removed because they are quickly solvable even by poor configurations and in practice usually handled by a presolver. similarly instances that take more than seconds to solve using the cplex defaults were re moved as these are considered too hard and little information is gained where all solvers timeout. after this filtering the regions dataset contains instances split into a training set of and a test set of while the arbitrary dataset has instances training and test. the solver timeout for both datasets is set to seconds. the third dataset comes from the configurable sat solver competition cssc ubc . this dataset was independently generated using fuzzsat brummayer et al. . fuzzsat first generates a boolean circuit and then converts this to cnf. we solve these instances using the pop ular sat solver lingeling biere . the circuits were generated using the options i and i . similar to what was done with the auction datasets any instances that could be solved in under second were removed. the result ing dataset contained instances which was split into training and test. we use a timeout of seconds for this dataset the same as that used in the cssc . it is important to note here that reactr by its nature is an online algorithm and therefore does not require a sepa rate training dataset. however in order to compare to exist ing methodologies a training set is necessary for those ap proaches to work with. all experiments were run on a system with x intel xeon e processors.ghz and gb ram. though there are cores available on each machine we limit ourselves to so as not to run out of memory or otherwise influence timings. results we show two separate scenarios for reactr. first we con sider a scenario where there is a collection of training data available beforehand or alternatively a training period is al lowed. we refer to this approach as reactr merged where the configurator is allowed to make a single pass over the training instances to warmstart its pool of configurations. secondly we evaluate reactr test which assumes no prior knowledge of the problem and starts the configuration process only when it observes the first test instance. for comparison we evaluate both versions of reactr against a stateoftheart static configurator smac. for com pleteness we investigated smac when trained for and hours. this way we cover scenarios when a new solver is configured each night as well as the best config uration smac can find in general. we observed that on our particular datasets performance improvements stagnated after hours training except in the case of the regions dataset for which we show the hour training. furthermore because reactr uses six cores six versions of smac are trained using the shared model mode option which allows multiple smac runs to share information. upon evaluation all six configurations are run and the time of the fastest performing smac tuning on each instance is logged. by doing this the cpu time used by smac and reactr is comparable. we also show the results for both the previous version of react on the merged dataset described above react a circuit fuzz using lingeling. b arbitrary ca dataset using cplex. c regions ca using cplex. figure rolling average solving time on various dataset and solver combinations. table summary of training testing and total time needed for the various configurations on the benchmark datasets. time taken s solver reactr smac number of hours default test merged regions train solve total arbitrary train solve total circuit fuzz train solve total merged and the default solver parameterizations. figure a shows the rolling average total time to dateinstances processed on the circuit fuzz dataset. we can see that both versions of reactr easily outperform the lin geling defaults. what is more interesting is that reactr is able to outperform smac trained for hours after a single pass over the training set taking under hours. even with out the warmstart reactr is able to find parameters that significantly better than the defaults and not too far off those that were otherwise configured. in figure b we see that on the arbitrary combinatorial auction dataset configuration is extremely important and that all configurators are able to find the good parameteri zations. in figure c however we once again see that both versions of reactr find significantly better parameteriza tions than those that can be found after hours of tuning smac and even the configuration found after hours of tuning smac. finally table shows the amount of time each configura tion technique requires to go through the entire process. this shows the amount of time needed to train the algorithm and the amount of time needed to go through each of the test in stances. the times are presented in s of seconds. note that in all cases reactr merged requires less time to train and also finds better configurations than smac. however if training time is a concern then reactr test requires signif icantly less total time than any other approach. conclusion it is clear from multitudes of past examples that whenever one needs to use a solver on a collection of instances it is imperative to utilize an algorithm configurator to automati cally set the solvers parameters. but while there are now a number of existing high quality configurators readily avail able unfortunately all assume a static view of the world. in practice training instances are not always available before hand problems tend to change over time and there are times when there is no extra time to train an algorithm offline. it is under these cases that react was shown to thrive able to achieve high quality parameterizations while tuning the solver as it was solving new instances. this paper showed how the crucial components of the original react method ology could be enhanced by incorporating the leaderboard ranking technique of trueskill. the resulting method real time algorithm configuration through tournament rankings reactr was shown to surpass even a stateoftheart con figurator smac across multiple domains. acknowledgments this publication has emanated from research conducted with the financial support of science foundation ireland sfi un der grant number sfirc. references amadini et al. roberto amadini maurizio gab brielli and jacopo mauro. sunny a lazy portfolio ap proach for constraint solving. theory and practice of logic programming tplp pages . ansotegui et al. carlos ansotegui meinolf sell mann and kevin tierney. a genderbased genetic algo rithm for the automatic configuration of algorithms. in principles and practice of constraint programmingcp pages . springer . biere armin biere. lingeling. sat race . brummayer et al. robert brummayer florian lons ing and armin biere. automated testing and debugging of sat and qbf solvers. in theory and applications of sat isfiability testingsat pages . springer . elo arpad e elo. the rating of chessplayers past and present volume . batsford london . fitzgerald et al. tadhg fitzgerald yuri malitsky barry osullivan and kevin tierney. react realtime algorithm configuration through tournaments. in proceed ings of the seventh annual symposium on combinatorial search socs prague czech republic au gust . . glickman mark e glickman. example of the glicko system. boston university . herbrich et al. ralf herbrich tom minka and thore graepel. trueskill a bayesian skill rating system. in advances in neural information processing systems pages . hutter et al. frank hutter holger h hoos kevin leytonbrown and thomas stutzle. paramils an auto matic algorithm configuration framework. journal of ar tificial intelligence research . hutter et al. frank hutter holger h hoos and kevin leytonbrown. sequential modelbased optimiza tion for general algorithm configuration. in learning and intelligent optimization pages . springer . ibm ibm . ibm ilog cplex optimization studio ... leytonbrown et al. kevin leytonbrown mark pearson and yoav shoham. towards a universal test suite for combinatorial auction algorithms. in proceedings of the nd acm conference on electronic commerce pages . acm . malitsky and sellmann yuri malitsky and meinolf sellmann. instancespecific algorithm configuration as a method for nonmodelbased portfolio generation. in inte gration of ai and or techniques in contraint program ming for combinatorial optimzation problems pages . springer . malitsky et al. a yuri malitsky deepak mehta and barry osullivan. evolving instance specific algorithm configuration. in sixth annual symposium on combina torial search . malitsky et al. b yuri malitsky ashish sabharwal horst samulowitz and meinolf sellmann. algorithm port folios based on costsensitive hierarchical clustering. in proceedings of the twentythird international joint con ference on artificial intelligence pages . aaai press . ubc ubc . configurable sat solver compe tition. xu et al. lin xu frank hutter holger h hoos and kevin leytonbrown. satzilla portfoliobased algorithm selection for sat. journal of artificial intelligence re search pages . zongker doug zongker. trueskill.py . httpsgithub.comdougztrueskill.  june wspcinstruction file ijtaistocasticofflineprogramming international journal on artificial intelligence tools c world scientific publishing company stochastic offline programming yuri malitsky department of computer science brown university p.o. box providence ri u.s.a ynmcs.brown.edu meinolf sellmann department of computer science brown university p.o. box providence ri u.s.a sellocs.brown.edu received day month year revised day month year accepted day month year we propose a framework which we call stochastic offline programming sop. the idea is to embed the development of combinatorial algorithms in an offline learning environment which helps the developer choose heuristic advisors that guide the search for satisfying or optimal solutions. in particular we consider the case where the developer has several heuristic advisors available. rather than selecting a single heuristic we propose that one of the heuristics is chosen randomly whenever the heuristic guidance is sought. the task of the sop is to learn favorable instancespecific distributions of the heuristic advisors in order to boost the averagecase performance of the resulting combinatorial algorithm. applying this methodology to a typical optimization problem we show that substantial improvements can in fact be achieved when we perform learning in an instances specific manner. keywords parameter tuning offline learning combinatorial optimization. . introduction solving hard combinatorial problems efficiently requires search. one of the most difficult tasks when designing combinatorial solvers is to make good heuristic deci sions which guide the search. while related communities have only recently started to exploit the potential of automatic tuning see e.g. the performance tuning tool in cplex the constraints community has a long history of research in this di rection and has proposed some pioneering ideas for the automatic configuration of algorithms see e.g. ref. . this paper follows this tradition by proposing a frame work for automatically selecting and combining heuristic advisors. this is achieved this work was supported by the national science foundation through the career cornflower project award number . june wspcinstruction file ijtaistocasticofflineprogramming y. malitsky and m. sellmann by extrapolating experience gathered offline by experimenting with a given bench mark of representative problem instances. several seminal papers have advocated the idea of exploiting statistics and ma chine learning technology to increase the efficiency of combinatorial algorithms. one such approach protocols during the solution process of a constraint satisfac tion problem which variable assignments cause a lot of filtering and bases future branching decisions on this data. this technique called impactbased search is one of the most successful in constraint programming and has become part of the ilog cp solver. satz a very successful systematic solver for sat uses the prop agation impact to determine the next branching variable. other solvers compute or estimate the solution density that results for a subproblem after a potential vari able assignment. for incomplete solvers reactive tabu search adapts parameters of a tabu search algorithm online. another algorithm was proposed that learns online which starting points for simple local search heuristics such as hillclimbing lead to good solutions. with respect to offline learning systems have been devel oped that automatically tune algorithm parameters for a given set of benchmark instances. moreover socalled algorithm portfolios have also been introduced. it was proposed to run in parallel or interleaved on a single processor multiple stochastic solvers that tackle the same problem. this approach was shown to work much more robustly than a single solver an insight that has led to the technique of randomization with restarts which is commonly used in all stateoftheart complete sat solvers. an alternate but similar idea considers a set of algorithms for a given problem and bases the decision of which algorithm to employ on certain features of the given problem instance whereby informative features and their correlation with the goodness of the algorithms in the portfolio such as shortage of running time are learned offline. all these approaches have two things in common. first it is possible to construct worstcase scenarios where any statistical inference method fails completely. for instance consider impactbased search for solving sat. take any two sat formulae both over variables x . . . xn. let us introduce a new variable x and add x to all clauses in and x to all clauses in . the sat problem we want to solve is the conjunction of all modified clauses in and . say we branch on variable x and set it to false first. impactbased search gathers statistics in the resulting left subtree to guide the search in the right subtree. however after setting x to false for the left subtree the resulting problem is to find a satisfying assignment for . in the right subtree we set x to true and the task is to find a satisfying assignment for . since and were chosen independently from one another it is not reasonable to assume that the statistics gathered when solving are in any way meaningful for the solution of . and obviously and can be chosen in such a way that the statistics gathered when solving are completely misleading when solving . the second aspect that the statistical approaches have in common is that they have all led to very impressive improvements in practice despite the above worst june wspcinstruction file ijtaistocasticofflineprogramming stochastic offline programming case argument. that is to say there is substantial practical evidence that exploiting online or offline statistical knowledge can boost the averagecase performance of combinatorial solvers. in some sense one may argue that the very fact that statistical inference does not work in the worstcase is what makes it statistical inference. that is because if we could draw any hard conclusions we would revert to deterministic inference and filter variable domains or derive new redundant constraints. however statistical inference only kicks in when our ability to reason about the given problem deterministically is exhausted. in this paper based on the above mentioned practical evidence we introduce a framework which exploits offline learning as part of the programming process. . algorithm families and instancespecific tuning our approach is motivated and heavily builds upon the previous work on algo rithm portfolios as well as the previous work on automatic parameter tuning and automatic algorithm configuration. particularly we introduce stochastic of fline programming which intertwines the development of combinatorial algorithms with automated instancespecific selection of an algorithm from an entire family of algorithms. our vision is that the algorithm development must not be limited to one fixed algorithm. instead the developer ought to have the freedom to propose an entire family of algorithms. then provided with a method which associates any given benchmark problem instance with a vector of meaningful feature values machine learning technology learns which algorithm in the family is best suited for a given instance. the final procedure solving the combinatorial problem therefore computes the features of the given instance and then picks which algorithm to employ accord ingly. we call this very general framework stochastic offline programming sop since it makes automated offline learning part of the programming process. the adjective stochastic is used because the learning is based on the assumption that the given training instances and their associated feature vectors are indicative of the instances that the resulting algorithm will be used on later. that is we assume that the given benchmark instances are sampled from the same distribution as the real instances and thus provide us with information on the kind of instances we ought to expect in the future. note that our goal to boost the averagecase performance requires the specification of an instance distribution and in sop we assume that this dis tribution is given indirectly by a sample of training instances and their associated feature vectors. sop is similar to and yet differs in some essential ways from existing approaches. algorithm portfolios like satzilla for example suffer from having to evaluate the performance of each algorithm before settling on the best one for a specific instance. this exhaustive evaluation limits the approach to work with only a few algorithms relying on the accidental variance in algorithm performance on different input in june wspcinstruction file ijtaistocasticofflineprogramming y. malitsky and m. sellmann stances. in contrast sop is meant to consider an entire family of algorithms usually infinitely many with the developer actively providing variance through instance specific algorithm performance. the idea to consider an entire family of algorithms resembles the situation of tuning algorithm parameters as done by algorithms like paramils. the main difference of our framework is that algorithm parameters are not chosen universally and instead the choice of parameters depends on the specific features of the instance that needs to be solved. . sop for homogeneous benchmarks we consider the following scenario given a combinatorial problem the developer has devised an algorithm which at some point needs to make heuristic decisions. for example a branchandbound algorithm where a branching assignment needs to be chosen at every choice point in the search tree. or the developer may have devised a constructive greedy algorithm which at every step uses a heuristic criterion to assign the value of a new variable. now rather than choosing one heuristic out of the several available heuristics we propose that the developer leaves the choice of the heuristic at every step to chance. that is when a new choice must be made one of the heuristics is chosen according to a predefined distribution of heuristics doh. the objective of stochastic offline programming is then to automatically devise an algorithm which chooses the doh that is best suited for a given problem instance. to this end sop is provided with the combinatorial algorithm an algorithm which associates an input instance with a vector of instancecharacterizing features as well as a set of training instances. .. selecting a doh for homogeneous instances we first consider the case when the benchmark instances are homogeneous in the sense that the feature vectors associated with the instances show no or only very little variance. out of the given algorithm family we are then to select one algo rithm which works well for a set of these homogeneous instances. this setting differs from algorithm portfolios in that we are dealing with an infinite number of potential dohs which renders infeasible approaches which try to learn the performance for each algorithm in the portfolio. on the other hand the homogeneous benchmark setting of sop also differs from and is strictly simpler than standard parameter tuning since we know that the parameters are associated with the probability of selecting each heuristic. we are therefore able to exploit this information by expect ing that small changes in the doh will likely result in small changes in algorithm performance. we propose algorithm to compute a good doh for a set of homogeneous benchmark instances. the procedure presented is provided with an algorithm family a for a combinatorial problem as well as a set s of training instances which are considered to have similar features. upon termination the procedure returns a doh for the given algorithm and benchmark set. june wspcinstruction file ijtaistocasticofflineprogramming stochastic offline programming sophomogeneous algorithm a benchmarkset s distr randdistr l r while termination criterion not met do a b chooserandpair m distra distrb x l y r l r length px is perfadistram xbm x i py is perfadistram y bm y i while length do if px py then py px r y length r l y x x l l length px is perfadistram xbm x i else px py l x length r l x y y l r length py is perfadistram y bm y i end if end while distr distram xbm x end while return distr algorithm sop for homogeneous benchmarks the problem of computing this favorable doh distr can be stated as a contin uous optimization problem minimizedistr is perfadistri such that distr is a probability distribution over the advisors used by a. to solve this problem we employ a local search procedure. we initialize distr randomly. in each iteration we randomly pick two heuristic advisors a b and redistribute their joint probability mass m among themselves while keeping the probabilities of all other advisors the same. we heuristically expect that the onedimensional problem which optimizes which percentage of m is assigned to advisor a the remaining percentage is then already determined to go to advisor b is convex. we search for the best percentage using a method for minimizing onedimensional convex functions over closed intervals which is based on the golden section see figure we consider two points x y within the interval and measure their performance px and py . the performance at x is assessed by running the algorithm a on the given benchmark with distribution distr am xbm x which denotes the distribution resulting from distr june wspcinstruction file ijtaistocasticofflineprogramming y. malitsky and m. sellmann p x p y iteration iteration l y l y r rx x fig. . minimizing a onedimensional convex function by golden section. when assigning probability mass xm to advisor a and probability mass xm to advisor b. now if the function is indeed convex if px py px py then we know that the minimum of this onedimensional function lies in the interval y x . we continue splitting the remaining interval which shrinks geometrically fast until the interval size length falls below a given threshold . by choosing points x and y based on the golden section in each iteration we only need to evaluate one new point rather than two. moreover the points considered at each iteration are reasonably far apart from each other to make a comparison meaningful which is important for us as our function evaluation may be noisy due to the randomness of the algorithm invoked and points very close to each other are likely to produce very similar results. .. a greedy heuristic for set covering to make the discussion less abstract let us consider a specific example of an opti mization problem and algorithm whose averagecase performance we hope to boost by computing a favorable distribution of heuristic advisors. given items . . . n and a set of sets of these items which we call bags and a cost associated with each bag the set covering problem scp consists in finding a set of bags such that the union of all bags contains all items and the cost of the selection is minimized. note as with any other tuning algorithm our benchmark is not the set covering problem but the specific complete or incomplete algorithms for its optimization. just like ref. where the benchmarks were solvers for sat and not sat itself here we show how the performance of two concrete algorithms for set cover can be boosted. june wspcinstruction file ijtaistocasticofflineprogramming stochastic offline programming greedy algorithm a simple greedy algorithm for the scp is to select bags one by one until we cover all items. several heuristics have been proposed in the literature on how the next bag ought to be selected. for instance we can select the bag that costs the least min c that covers the most new items max k that minimizes the ratio of costs over the number of newly covered items min ck that minimizes the ratio of costs over newly covered items times the loga rithm of newly covered items min ck log k that minimizes the ratio of costs over the square of newly covered items min ck and the bag that minimizes the ratio of square root of costs over the square of newly covered items min c k . greedy algorithms like this are frequently used to find high quality solutions within complete solution approaches. the traditional way of designing algorithms is to try all greedy variants on some training problems and to select the one which yields the best results on average. but an alternate an innovative idea was presented by balas and carrera. rather than choosing just one greedy variant or choosing a random variant each time the greedy algorithm is employed it was suggested to choose randomly the heuristic for selecting the next bag within the greedy algo rithm. it was reported that significantly improved solutions are found when this randomized greedy algorithm is run times every time the complete algorithm calls the primal heuristic. this approach is outlined in algorithm . note that if scpgreedy s . . . sm c . . . cm bestvalue for i . . . do solution cost while isolution si . . . n do r pickadvisor j selectbagrsolution s . . . sm c . . . cm solution solution j cost cost cj end while if bestvalue cost then bestvalue cost bestsolution solution end if end for return bestsolution algorithm randomized greedy set covering algorithm june wspcinstruction file ijtaistocasticofflineprogramming y. malitsky and m. sellmann we are to run the greedy construction times anyway we could also run each of the pure heuristics of which there are only . the significance of the finding by balas and carrera is that hybridizing the heuristics by choosing one of them uniformly at random at each step of the greedy construction and repeating this randomized construction times yields results which are better than running the best pure heuristic. we test algorithm on this randomized greedy algorithm. rather than choosing a selection heuristic uniformly at random in lines we intend to learn which distributions of selection heuristics are most promising. in our experiments we found that the iterations proposed by balas and car rera often resulted in high variance in the quality of the outcomes. this variance then led to very noisy data complicating the effectiveness of algorithm in finding the global optima. therefore in all our experiments we instead repeat the random ized construction times. benchmark sets to experiment with our sop framework we require training and test sets for the scp. we consider three different randomly generated benchmark sets whereby each set has instances with items and bags each. set each bag contains exactly of the items which are chosen uni formly without replacement. the cost of each bag is chosen uniformly at random between and . sets for each item for each bag we flip a coin and with a probability of we insert the item into that bag. the cost of each bag is chosen uniformly at random between and . sets for each bag we repeatedly sample an item from the set of items for insertion into the bag. we assume the items are numbered and that the sample is taken from a gaussian distribution which has its mean at a random yet fixed item. the standard deviation of this distribution is . the process of sampling and inserting items is repeated until the bag contains exactly of the items. again the cost of each bag is chosen uniformly at random between and . for each of the three classes above there are really two benchmark sets one for training and one for test purposes and each containing scp instances. experimental results in each iteration of algorithm we assume that the one dimensional subproblem that is solved by the golden section is convex. in figure we plot the average solution quality over runs of algorithm for each doh that is achieved when mixing heuristics advisors min c and min ck on an instance in set . the error bars give the standard deviation over runs at each doh. with this graph we confirm our assumptions that a probabilistic mixture of two heuristics is in fact better than either of the heuristics by themselves. we also show that search space is in fact convex validating our use of algorithm to find the best combination of heuristics. we observe curves like the one shown here for all instances and for all combinations of heuristic advisors that we looked at. note that june wspcinstruction file ijtaistocasticofflineprogramming stochastic offline programming fig. . optimality gap when mixing advisors min c and min ck. algorithm would also work when the convexity assumption was in fact false but then it may only provide a locally optimal solution. however for our application we may expect that algorithm provides nearoptimal dohs. in table we compare the solution quality achieved by different dohs in terms of percent of optimality gap closed. to this end we precomputed optimal scp solutions for all training and test instances in set set and set . the initial gap is defined by the solution quality that is achieved when using the single advisor which gives on average the best solutions for the training instances. in column gap we observe that the best pure greedy approach leaves an optimality gap between and . we compare the following choices for the advisor selection in line of algo rithm . all performs a roundrobin through all advisors. that is in the ith construction of a greedy solution it always returns the same advisor and moves on to the next advisor in iteration i. note that for this way of choosing advisors it is of course not necessary to perform iterations since there are only different advisors. clearly we expect a much better performance from mixing advisors and running the randomized algorithm times to make up for the additional time spent. uniform denotes the algorithm proposed by balas and carrera where table . percent of optimality gap closed over the best singleadvisor heuristic. the standard deviations are provided inside the parentheses. benchmark gap all uniform sop oracle set train . . . . . . . . . test . . . . . . . . . set train . . . . . . . . . test . . . . . . . . . set train . . . . . . . . . test . . . . . . . . . june wspcinstruction file ijtaistocasticofflineprogramming y. malitsky and m. sellmann in each iteration of the greedy construction we choose a different advisor by pick ing one uniformly at random. sop denotes the greedy algorithm which uses the doh which sop found for the given training set. finally oracle denotes the performance when we use the best doh for each individual instance which we pre computed and use as an upper bound on the solution quality that can be achieved by the simple greedy heuristic that we are trying to tune. for set we observe that running the greedy heuristic times rather than just once while using a different advisor each time and returning the best cover in the end closes around of the optimality gap that the best pure greedy construction left open. surprisingly for this benchmark set we do not find it worthwhile to invest the time to construct covers while mixing advisors uniformly at random in the construction of each cover. in fact the solution quality even slightly declines compared to all. the situation is similar for benchmark set . only for set we find that mixing advisors and spending extra iterations results in significantly better covers. looking at column oracle we find that by spending an extra iterations we can expect to close at most around of the optimality gap for sets and and around for set . comparing with sop we see that the latter comes surprisingly close to realizing this potential moreover when comparing the training and test performances that sop achieves we see that extrapolating the offline experience gained on the training set is feasible and results in only slightly decreased performances on the test sets. . sop for heterogeneous benchmarks we now consider the general case where we are given a set of benchmark instances which show significant diversity with respect to their associated feature vectors. in this scenario we intend to learn a function which associates any given feature vector not represented in the training set with a doh which we can expect to work well for instances with respective features. .. selecting a doh for heterogenous instances for this task we tried two different algorithms. the first followed the standard machine learning approach of multinomial logistic regression. the idea aimed to learn a weight for each featureadvisor pair. then with the help of those weights compute a distribution of heuristics doh by taking for each advisor the weighted sum of the features of the instance times the respective featureadvisor weights and using this as the argument to the exponential function. this way each advisor is associated with a weight and the distribution is then computed through normalizing the weights. this use of regression to predict the doh is similar to the work proposed by hutter et.al. in their work hutter et.al. aim to find the parameter setting for a solver that will minimize its expected run time on a specific instance. offline the june wspcinstruction file ijtaistocasticofflineprogramming stochastic offline programming sopheterogeneousclusteralgorithm a benchmarkset s int k initfeaturemetric repeat c . . . ck clusterskfeaturemetric for i to k do fi sophomogeneousaci ci centerci end for for all i j k do for all instances a ci do dacj max perfa fj a perfa fi a end for end for until adjustfeaturemetricdfalse return f . . . fk c . . . ckfeaturemetric algorithm clusterbased sop for heterogeneous benchmarks algorithm first learns a function gf p that predicts the runtime given a feature vector f that describes the problem instance and a parameter setting p. therefore for any new instance f is fixed and the minimum of g is assumed to correspond to the parameter settings that will yield the shortest expected run time. however while this approach has shown encouraging results it relies heavily on the assumption that the found parameters will work well together. this is simply not the case for our problem setting. for example if we equate the parameter setting with our doh it is not clear what should be done if a high value is returned for two of the heuristics. this could mean that alternating between the two heuristics will lead to improved performance. on the other hand it is equally likely that the solver should use only one of the heuristics exclusively. furthermore to train accurate prediction models a large and diverse training set is required. due to these complications we move away from regression and propose a second algorithm for the heterogenous case. our second algorithm is based on the idea of grouping the benchmark instances in clusters and precomputing a promising doh for each cluster. treating each such cluster as a set of homogeneous instances we can then employ algorithm which works on a much lowerdimensional space. though in order to cluster the instances we require a distance metric in the feature space. naturally the distance between two feature vectors ought to reflect how well a doh works on instances with respective features. in particular we want to separate and thus introduce a large distance between feature vectors v v where a promising doh for instances with features v works badly for instances with features v and vice versa. the idea is realized in algorithm . first we initialize the metric in the feature space arbitrarily for example using the euclidean norm. then in each iteration we cluster the benchmark set into k groups of instances whereby we assume that june wspcinstruction file ijtaistocasticofflineprogramming y. malitsky and m. sellmann k is a parameter of the algorithm. to cluster the instances we use lloyds kmeans clustering algorithm. for each cluster we compute the optimal doh using algo rithm as well as the center of gravity. we now assess what the distance between individual instances and the centers of gravity of the different clusters ought to be. to this end for each pair of clusters i j we compute the difference between the performance on all instances in cluster i which is achieved by the best doh for that cluster and the doh of the other cluster. the distance between an instance a in cluster ci and the centers of gravity of cluster cj is then the maximum of this regret and . using these desired distances we adjust the feature metric and iterate until the feature metric does not change anymore. then we return the best dohs as well as the centers of gravity for each cluster and the final feature metric. equipped with this tuple any formerly unseen instance with associated feature vector can easily be assigned to one of the clusters and the appropriate doh can then be used for its optimization. this procedure for finding the best clusters is straightforward. the only issue is the parameter k which determines the number of clusters and thus the final number of different algorithms which will be used to optimize different instances. we need to strike a good balance here between our wish to provide many different algorithms so that we can provide a tailormade algorithm for new instances and the need to devise robust algorithms which work well on a variety of instances with the same or similar features. in later sections we will show how to automate the choice of k but for now the choice of k must be determined heuristically by the user. .. tuning a greedy heuristic for heterogenous benchmarks we consider again the greedy heuristic for the scp and the different advisors from section .. to make the choice of doh instancespecific we require features which characterize an instance. features we gather the following data for scp instances the normalized cost vector c m the vector of bag densities sini...m the vector of item costs ijsi c ij...n the vector of item coverings i j simj...n the vector of costs over density cisii...m the vector of costs over square density cisii...m the vector of costs over k log kdensity cisi log sii...m and the vector of rootcosts over square density cisii...m. as features we compute the maxima minima averages standard deviations and the logarithms of all these statistics for all vectors. to assess the performance achieved by a particular doh see the calls to function perf in algorithm we run the modified algorithm again five times and take the average of the solutions returned. june wspcinstruction file ijtaistocasticofflineprogramming stochastic offline programming benchmark sets to experiment with our sop framework we also require het erogenous training and test sets for the scp. we consider two more benchmark sets whereby again each set consists of a training set and a test set of scp instances. set the training set consists in instances from each of the sets set set and set . the test set is the union of all test sets of set . set for each instance we first choose at random whether the constraint matrix is filled by row or by column and whether the respective row or column density is constant or variable. then we randomly select the density or desired mean density at or and flip a coin to decide whether row or column densities that are set to one are chosen uniformly at random or with a gaussian bias. costs are uniformly chosen from . a training set with instances and a test set with instances are generated this way. additional test sets are benchmark classes scp scp and scp from the or library. experimental results as in section . we run various algorithms on our bench marks whereby the offline learning algorithms are run on the training set and eval uated on the training as well as the test sets. we compute again the optimality gap of the pure greedy heuristic that works best for the training set. the percentage of this gap that is closed by the contenders is given in table . again we observe that using a uniform distribution is often not much better and sometimes even worse than using all of the advisors in a pure roundrobin fashion. our homogenous sop approach already provides significantly improved dohs even though the effect on these heterogenous sets of scp instances is not as dramatic as seen before on the homogenous benchmarks set . in particular the comparison with the oracle data where we computed the best doh for each individual instance shows that there is still room for improvement. on set we observe that the heterogenous sopclustering approach comes close to realizing the total potential that running the simple greedy algorithm times with a mix of our six different advisors holds in store for us. on the test set that was generated by the same generator as the training set the results are outstanding. on set which is much more diverse we observe that instancespecific parameter table . percent of optimality gap closed over the best singleadvisor heuristic. the standard deviations are provided inside the parentheses. benchmark gap all uniform sophomo sophetero oracle regress cluster set train . . . . . . . . . . . . . test . . . . . . . . . . . . . set train . . . . . . . . . . . . . test . . . . . . . . . . . . . scp . . . . . . . . . . . . . scp . . . . . . . . . . . . . scp . . . . . . . . . . . . . june wspcinstruction file ijtaistocasticofflineprogramming y. malitsky and m. sellmann tuning significantly outperforms all other algorithms although it clearly does not achieve the performance of a perfect oracle. what is remarkable is that the quality of the clustering approach does not decline much for the test sets scp which are drawn from the or library moreover note that both heterogenous sop approaches keep their performance much more stable and closer to the training set over all four test sets in this benchmark than homogenous sop. . sop for tree search now we consider a complete tree search approach. for optimization problems like set covering systematic approaches are commonly based on branchandbound. the latter draws its strength from the upper and lower bounds which are used for pruning purposes. for the lower bound we use a linear relaxation which is computed by soplex .. there exist techniques to strengthen the lower bound especially by adding valid inequalities. these techniques are outside the scope of this paper. here we focus on strengthening the upper bound which is commonly computed by a primal heuristic like the one presented in algorithm . thus with the help of sop we intend to improve the upper bound so that our branchandbound approach provides high quality solutions very quickly. in principle the situation is similar to that in the previous sections. there are some fundamental differences though. first we have seen before that the simple greedy heuristic even when run multiple times and with highquality dohs leaves an optimality gap of depending on the benchmark set that is tackled. em bedded in a branchandbound approach we expect the upper bound to converge to near optimality with as little search as possible. the second difference is that the primal heuristic is now called for every choice point. the distribution of problem instances encountered at the various choice points is likely to differ a lot from that represented by the input benchmark set. consequently we set up the following algorithm that will be optimized by sop. at every choice point we compute an upper bound with the help of algorithm whereby we resort to the use an iteration limit of as originally proposed by balas and carrera. we use the smaller number of iterations instead of in line since the primal heuristic is being called at every choice point for only slightly varying problems. also at every choice point we compute an lpbased lower bound and backtrack if the optimal relaxation is integer or when the global upper bound is already lower or equal to the local lower bound. if that is not the case we branch and continue search in depthfirst manner. for these experiments the branching decisions are based on a single fixed branching heuristic. for sop training purposes we stop this treesearch after nodes and note the final solution quality. in table we show the optimality gap left open when using the best single advisor for the training set. we observe that this gap is now much smaller due to the search that is performed by our target algorithm. we also note that the best pure primal heuristic for the training sets of sets and work comparably well on june wspcinstruction file ijtaistocasticofflineprogramming stochastic offline programming table . percent of optimality gap closed by the treesearch algorithm over the best singleadvisor heuristic for homogeneous benchmarks. the standard deviations are provided inside the parentheses. benchmark gap all uniform sop cluster offline set train . . . . . . . . . test . . . . . . . . . set train . . . . . . . . . test . . . . . . . . . set train . . . . . . . . . test . . . . . . . . . table . percent of optimality gap closed over the best singleadvisor heuristic using tree search on heterogeneous benchmarks. the standard deviations are provided inside the parentheses. benchmark gap all uniform sop cluster set train . . . . . . . test . . . . . . . set train . . . . . . . test . . . . . . . the corresponding test sets. for set however we find that the best pure advisor overtunes it leaves a rather small optimality gap on the training set but loses a lot of quality more than when used on the generalized test set. note that this effect influences all other approaches for which we measure performance percent of optimality gap closed over the best pure advisor the training performance of the other approaches appears worse and their test performance appears better. overall we find that sop using clustering outperforms all other approaches leaving an average optimality gap of to . after only nodes of search. in column offline we show the performance when using the dohs which were found in the experiment corresponding to table . while these dohs still perform on par or better than a uniform selection of advisors learning dohs which perform well within the treesearch framework is clearly better. we repeat the same experiment for the heterogeneous benchmarks set and set . in table we see once more that offline learning based on training instances that are stochastically related to the test cases offers the possibility of significantly boosting the performance of combinatorial algorithms. . enhancements we have shown that by clustering training instances it is possible to train a solver in an instancespecific manner that will have much better performance than the traditional best single advisor approach. furthermore we show that by splitting the training data into clusters and training on each set separately it is even possible to outperform a doh that was tuned on all of the training data. there are however two major drawbacks to the sop methodology as was pre june wspcinstruction file ijtaistocasticofflineprogramming y. malitsky and m. sellmann sented so far. first in order to correctly cluster the instances it is necessary to learn the weights for the distance metric to insure that instances that behave well under a particular doh are all in the same cluster. this computation is costly as it requires several iterations of tuning and reclustering. the second issue is that it is not clear how many clusters to split the data into. if there are too few clusters we lose some of our potential to tune parameters more precisely for different parts of the instance feature space. on the other hand if there are too many clusters there can be very few instances in each cluster which jeopardizes the robustness and generality of the parameter sets that we optimize for these clusters. in this section we present extensions that resolve these issues. following the description of the proposed extensions we present experimental results of applying these techniques to sop. .. normalizing features instead of learning a feature metric over several iterations we propose to normalize the features so that over the set of training instances each feature spans exactly the interval . that is for each feature there exists at least one instance for which this feature has value and at least one instance where the feature value is . for all other instances the value lies between these two extremes. through this normalization we reduce the influence of the larger valued features. so far in sop features with high values e.g. average cost per bag tend to dominate the low value features e.g. average number of items per bag when it comes to the outcome of the initial clusterings. this is because the distance between two feature vectors can be very large solely because a single feature value in one instance is much larger than the corresponding value in another instance even though all of the smaller valued features are almost identical. by scaling the feature space after every iteration the sop procedure gradually learns which of the features are important to the clustering. larger valued features are given smaller weights while the crucial smaller valued features are given a much higher weight. however even though it proves to be accurate in our experiments the drawback is that this process needs several iterations to converge where each such iteration is computationally expensive. by normalizing the features before the initial clustering step all features are given equal importance. thus when clustering the algorithm focuses on separating instances where the features vary drastically. in our experiments we found that the resulting clusters are similar to those found at the later stages of the original algorithm but by using normalization only a single tuning step is required. .. gmeans clustering we have shown that using kmeans over the more traditional regression learning yields superior performance. yet as we stated earlier the choice of the k used by june wspcinstruction file ijtaistocasticofflineprogramming stochastic offline programming gmeansx k i c s kmeansx k while i k do c s kmeanssi v c c w vi yi vixiw if andersondarlingtesty failed then ci c si s k k ck c sk s else i i end if end while return c s k algorithm gmeans clustering algorithm this approach is crucial for optimal performance. we address this issue by using gmeans a clustering algorithm which automat ically determines a favorable number of clusters. the underlying assumption here is that a good cluster exhibits a gaussian distribution around the cluster center. the algorithm presented in algorithm first considers all inputs as forming one large cluster. in each iteration we pick one of the current clusters and try to assess whether this cluster is already sufficiently gaussian. to this end gmeans splits the cluster in two by running means clustering. we can then project all points in the cluster onto the line that runs through the centers of the two subclusters. in this way we obtain a onedimensional distribution of points. gmeans then checks whether this distribution is normal using the widely accepted andersondarling statistical test. if the current cluster does not pass the test then it is split into the two previously computed clusters and we continue with the next cluster in our current set. we found that the gmeans algorithm works very well for our purposes. the only problem we encountered was that sometimes clusters could be very small and contain only very few instances. therefore to obtain robust parameter sets we do not allow clusters that contain fewer than a certain threshold of instances. once gmeans finishes for those clusters failing this size requirement beginning with the smallest we redistribute the corresponding instances to the nearest bigger clusters where proximity is measured by the distance of each instance to the center of the bigger cluster. june wspcinstruction file ijtaistocasticofflineprogramming y. malitsky and m. sellmann .. experimental results table presents the evaluation of the proposed enhancements on set . here sop is the original algorithm that over a series of iterations learns a distance metric to be used by kmeans to find the best doh. sopn first normalizes the features of the training sets before proceeding to using kmeans to cluster. sopn is run for only a single tuning step. alternatively sopg learns a distance metric over the course of several iterations but instead of using kmeans for reclustering it uses gmeans. sopng combines the two enhancements and performs only a single iteration where it first normalizes the features and then employs gmeans to cluster. for ub we assume that the clustering algorithm correctly separated all of the instances into three clusters each containing instances only from set set and set respectively. it is important to note here that simply by normalizing the features once before clustering we are able to achieve performance comparable to running multiple iter ations of sop until the distance metric converges. furthermore by automatically selecting the number of clusters we use we do not lose out on performance. it is also important to note that all versions of sop are performing very close to ub leading to the conclusion that the clusters that are being used are close to optimal. table presents the evaluation of the proposed enhancements on set . the algorithm names remain the same as in table except since we are unable to know the perfect clustering as was the case with ub we compare the algorithms to oracle which computes the best doh for each instance. looking at sopn we notice a drop in performance of the distributions when evaluated on the or library benchmarks scp and . this is caused due the equal weight on all of the features forcing some of the instances to be assigned to the wrong cluster. however we also note that once gmeans splits the data into more clusters the performance on these benchmarks improves. table . percent of optimality gap closed over bestsingle advisor heuristic on the set benchmark. the standard deviations are provided inside the parentheses. benchmark gap sop sopn sopg sopng ub set train . . . . . . . . . . . test . . . . . . . . . . . table . percent of optimality gap closed over bestsingle advisor heuristic on the set benchmark. the standard deviations are provided inside the parentheses. benchmark gap sop sopn sopg sopng oracle set train . . . . . . . . . . . test . . . . . . . . . . . scp . . . . . . . . . . . scp . . . . . . . . . . . scp . . . . . . . . . . . june wspcinstruction file ijtaistocasticofflineprogramming stochastic offline programming table . percent of optimality gap closed over the best singleadvisor heuristic using tree search on heterogeneous benchmarks. the standard devi ations are provided inside the parentheses. benchmark gap sop sopng set train . . . . . test . . . . . set train . . . . . test . . . . . finally as can be seen in table normalizing features and using gmeans also correlates to the tree search solver introduced in section . . conclusions and future work we have introduced the idea of stochastic offline programming a programming framework for automatically choosing and combining different heuristic advisors. the instancespecific randomized combination of advisors is based on offline expe rience gathered on a training set which is sampled from the same distribution as the test sets that the algorithm is expected to perform well on. extensive tests on a greedy incomplete algorithm and a systematic tree search algorithm for the set covering problem provide a proof of concept it is indeed pos sible to combine heuristic advisors in a randomized fashion and favorable instance specific distributions can be learned which clearly outperform the best pure advisor as well as a uniform combination of advisors. moreover on heterogenous bench marks we found that there is no one best distribution of heuristics doh that works well on all instances. for this case we showed that we can learn how to select a doh based on the features of a given problem instance. our future work regards the test of sop on algorithms for other problems. the sop framework is general enough to cope with any combination of advisors and any objective. for example we intend to use it to combine branching heuristics to minimize the expected runtime of a constraint solver. as this will require a lot more cpu time that we could afford in the development and calibration of the homogeneous and heterogenous sop algorithms we are working towards an efficient parallel implementation of sop. moreover we intend to generalize the framework so that it can learn more than one doh simultaneously when heuristic guidance is needed for more than one task within a combinatorial algorithm. references . b. adensodiaz and m.laguna. finetuning of algorithms using fractional experi mental design and local search. operations research . . e. balas and m. carrera. a dynamic subgradientbased branchandbound procedure for set covering. operations research . . r battiti and g tecchiolli. the reactive tabu search. orsa journal on computing . june wspcinstruction file ijtaistocasticofflineprogramming y. malitsky and m. sellmann . m. birattari t. stuetzle l. paquete k. varrentrapp. a racing algorithm for con figuring metaheuristics. gecco . . j.a. boyan and a.w. moore. learning evaluation functions to improve optimization by local search. journal of machine learning research . . s.p. coy b.l. golden g.c. runger e.a. wasil. using experimental design to find effective parameter settings for heuristics. journal of heuristics . . a. fukunaga. automated discovery of local search heuristics for satisfiability testing. evolutionary computation . . c. gomes and b. selman. algorithm portfolios. artificial intelligence . . g. hamerly and c. elkan. learning the k in kmeans. nips . . f. hutter and y. hamadi. parameter adjustment based on performance prediction towards an instanceaware problem solver. technical report msrtr microsoft research cambridge uk . . f. hutter y. hamadi h.h. hoos and k. leytonbrown. performance prediction and automated tuning of randomized and parametric algorithms. cp pp. . . f. hutter h. hoos t. stuetzle. automatic algorithm configuration based on local search. aaai . . m. jordan. why the logistic function a tutorial discussion on probabilities and neural networks. mit computational cognitive science report . . c.m. li and anbulagan. heuristics based on unit propagation for satisfiability. ij cai . . s.p. lloyd. least squares quantization in pcm. ieee transactions on information theory . . s. minton. automatically configuring constraint satisfaction programs. constraints . . e. nudelman k. leytonbrown h.h. hoos a. devkar y. shoham. understanding random sat beyond the clausestovariables ratio. cp . . j.e. beasley. orlibrary. journal of the operational research society . . ph. refalo. impactbased search strategies for constraint programming. cp pp. . . r. wunderling. paralleler und objektorientierter simplexalgorithmus technische universitat berlin . . l. xu f. hutter h. h. hoos and k. leytonbrown. satzilla portfoliobased algo rithm selection for sat. jair volume pages . . a. zanarini and g. pesant. solution counting algorithms for constraintcentered search heuristics. cp .  structurepreserving instance generation yuri malitsky marius merschformann barry osullivan and kevin tierney ibm t.j. watson research center new york usa yuri.malitskygmail.com decision support operations research lab university of paderborn germany tierneymerschformanndsor.de insight centre for data analytics university college cork ireland b.osullivaninsightcentre.org abstract. realworld instances are critical for the development of stateofthe art algorithms algorithm configuration techniques and selection approaches. how ever very few true industrial instances exist for most problems which poses a problem both to algorithm designers and methods for algorithm selection. the lack of enough real data leads to an inability for algorithm designers to show the effectiveness of their techniques and for algorithm selection it is difficult or even impossible to train a portfolio with so few training examples. this pa per introduces a novel instance generator that creates instances that have the same structural properties as industrial instances. we generate instances through a large neighborhood searchlike method that combines components of instances together to form new ones. we test our approach on the maxsat and sat prob lems and then demonstrate that portfolios trained on these generated instances perform just as well or even better than those trained on the real instances. introduction one of the largest problems facing algorithm developers is a distinct lack of industrial instances with which to evaluate their approaches. yet it is the use of such instances that helps ensure the applicability of new methods and procedures to the realworld. algorithm configuration and selection techniques are particularly sensitive to the lack of industrial instances and are prone to overfitting as it is difficult to build valid learning models when little data is present. although a plethora of random instance generators exist the structure of industrial instances tends to be different than that of randomly generated instances as has been shown for the satisfiability sat problem . in this work we therefore present a novel framework for instance generation that creates new instances out of existing ones through a large neighborhood searchlike iterative process of destruction and reconstruction of structures present in the in stances. specifically given an instance to modify m and a pool of similar instances p we destroy elements of m that fit certain properties such as variable connectivity and merge portions of the instances in p into m to create a set of new instances. we compute the features of each new generated instance and accept the instance if it falls into the cluster of instances defined by p . to the best of our knowledge our framework is the first approach able to generate industriallike instances directly from real data. aside from the immediate benefits of providing a good training set for portfolio techniques this type of instance generation has the potential of opening new avenues for future research. in particular the underlying assumption of most portfolio tech niques is that a representative feature vector can be used to identify the best solver to be employed on that instance. techniques like isac take this idea further by claiming that instances with similar features are likely to have the same underlying structure and can therefore be solved using the same solver. structurepreserving instance generation uses and furthers this notion that in order to predict the best solver for a given instance one should create a plethora of instances with very similar features train a model on them and then make a prediction for the original instance. additionally given recent results regarding the benefits of having multiple correlated instances for csps our instance generator may be able to help solvers more quickly find solutions as it can provide such correlated instances. in theory structurepreserving instance generation can even be used to generate instances significantly different from those observed before. this could in turn allow the targeted creation of portfolios that can anticipate any novel instances not part of the original training set. it would also allow for a systematic way of studying the problem space to identify regions of hard and easy problems as in but with a stronger basis in real instances. this would allow algorithm designers to create new approaches to specifically target challenging problems. in this work we primarily focus on the wellknown sat problem as well as its optimization version maximum sat maxsat. these two problems pose ideal test beds for our approach as although the number of industrial instances is low it is still larger than what is available in most other domains. for example the maxsat com petition in had only industrial instances in the unweighted category as opposed to crafted and random instances. we show that the instances gen erated by our method have similar runtime profiles as the industrial instances they are based on that they have similar features and that they can be used in algorithm selection techniques with no loss of performance and in some cases even provide small gains. we then show that our technique will even help for problems with larger available datasets as is the case with sat and the available industrial instances from the sat competition . we also evaluate our generator using the qscore from which is specifically designed for evaluating instance generators and re ceive near perfect scores. finally our source code is available in a public repository at httpsbitbucket.orgeusorpbspig. related work numerous random instance generators exist for satcsp problems such as to name a few. some generators try to hide solutions within the problem or generate a specific number of solutions and respectively whereas others convert problems from other fields to satcsp problems e.g. . the approach of slater creates instances by connecting modules of sat instances with a shared component a structure that is often present in industrial instances. for maxsat several generators exist e.g. which generates bin packinglike problems. the gen erator from motoki can create maxsat problems with a specific number of unsat isfied clauses. however in all of these generators the structures inherent in industrial problems are not present. an extended version of this work provides a more extensive literature review see httpsbitbucket.orgeusorpbspig the most industrylike satmaxsat instances are generated by ansotegui et al. through the modification of a random instance generator to use a powerlaw distribu tion. in contrast our framework is able to specifically target certain types of industrial instances. our approach is similar to instance morphing the primary difference being our focus on instance features and a destroyrepair paradigm. furthermore mor phing is meant to connect the structures of instances while our goal is to also find new combinations of structures leading to new areas of the instances feature space. burg et al. propose a way of generating sat instances by clustering the variables based on their degree in a weighted variable graph in burg et al. . several ap proaches are tested to try to rewire the instance by adding and removing connections between the variables. the authors compute the features from nudelman et al. for the original instances and the generated ones noting that several features of the gener ated instances no longer resemble the original instances. in contrast the instances we generate have similar features to their original instances and stay in the same cluster as the original instance pool. an evolutionary algorithm approach is used by smithmiles and van hemert to generate traveling salesman problem instances that are uniquely hard or easy for a set of algorithms. this approach starts from a random instance under some assumptions about the size and structure of the resulting instance. in lopes and smithmiles realworldlike instances for a timetabling problem are generated using an existing in stance generator. while similar to our approach their work focuses mainly on gener ating instances that are able to discriminate between solvers in terms of performance. furthermore our approach does not require an existing instance generator. tsp in stances are also evolved in nallaperuma et al. for a parameter prediction model for an ant colony optimization model. however all these works focus on creating hard instances for particular solvers rather than instances that resemble industrial instances. the most similar work to ours is from smithmiles and bowly in which in stances are generated for the graph coloring problem targeting specific instance features. the authors project their features into a two dimensional space with a principal com ponent analysis and then check if the instance features to be generated are in a feasible region of the space. a genetic algorithm is then used to try to find an instance matching the input features. this approach is more general than ours but it is not known how well it works with industrial instances or other problem types. structurepreserving instance generation our instance generation algorithm is motivated by the differing structures found in sat instances especially between the industrial crafted and random categories of instances. figure provides some visualizations of sat instances based on their clause graphs. in these graphs each node represents a clause in the formula with an edge specifying that the two clauses share at least one variable. the nodes are also color coded from red to blue where nodes with only a few edges are colored red and those with the most edges are colored blue. a forcebased algorithm is used to spread nodes apart. in this way nodes that share many edges between each other are pulled together into clumps while the others are pushed away. note that the industrial instances which in figure are a b and c tend to contain a core set of clauses that share at least one variable with many other clauses. a aprove i b aes keyfind i c eqatreebraun. . . i d battleshipsat c e ccpsfacto c f unifkr.v. . . r fig. . visualizations of industrial i crafted c and random r instances made with gephi . nodes are clauses and an edge exists if the corresponding clauses share a variable. in addition a large number of small subsets of clauses are built on the same set of variables. a few variables link the subsets of clauses to the common core. in contrast instances d and e which are from the crafted category from the sat competition and f which is from the random category show significantly more connectivity be tween clauses and less modules or groupings of nodes within the graph. given a pool of instances p and an instance to modify m structurepreserving instance generation works as shown in algorithm to create a set of generated instances gen . the instance pool should be a set of homogeneous instances such as the instances in a particular cluster from the isac method . while using a heterogeneous pool would still result in instances recall that in this work we aim to create instances with similar properties. in cases where an industrial instance has no similar instances our method can still be used with a pool consisting of only the instance to modify. the parameters and define the minimum and maximum of the size of the generated instances in proportion to m respectively. we use these values as general guidelines rather than hard constraints in order to prevent the instance from growing too large or too small. our proposed algorithm can be thought of as a modified large neighborhood search in which the incumbent solution in this case the instance to modify is iteratively de stroyed and repaired. the destroy function identifies and removes a particular struc ture or component of m. the destroy process is run at least once and is continued until the instance drops below its maximum size. the repair function then identifies and extracts structures from one or more randomly chosen instances from p and inserts algorithm structurepreserving instance generation algorithm. function spigm p gen m m repeat do m destroym selectstructm while sizem sizem do i random instance in p d max varsm varsm m repairm selectstructi d while sizem sizem if acceptm p m then gen gen m until terminate return gen them into m. this is repeated until m is larger than the minimum instance size. the d parameter taken by the repair method makes sure that the total number of variables in the problem also stays constant. an acceptance criterion determines whether or not the modified instance should be added to the dataset of instances being built. we base this acceptance on the features of the instance and check whether each individual feature is close to the features of the cluster formed by p m. for problems like sat or csp where an unsatisfiable component or tautology could be introduced a check can be performed to ensure that the instance did not become trivial to solve. the algorithm terminates when enough instances are generated. here it may be argued that an alternate search strategy may also work as well or even better than the one outlined in this section. while alternatives are certainly possible they are beyond the scope of this work although some have been tried. for example one can imagine a combination of local search strategies where each method adjusts an instance to match some combination of features. this modifies the internals of an instance while keeping the instance features relatively unchanged or moves them back if they change too much. the issue with this method is that the features of interest for problems like sat are highly interdependent making any fine grained control over them an arduous task at best. furthermore it is frequently very easy to make an instance trivial to solve by introducing an infeasibility a position that is very difficult to remedy. alternatively one can argue that as long as the provided acceptance criteria is uti lized as is it is possible to employ a local search to just try a number of instantiations. while possible in theory this approach can take a considerable amount of time before stumbling over even a single seemingly useful instance. the problem space of instance generation is simply too vast. therefore while we do not claim that the approach pre sented here is the only way of generating instances or even the best way it is a system atic procedure that allows rich datasets to be quickly generated that we can empirically demonstrate works well in practice. application to sat and maxsat we present an instantiation of the structurepreserving instance generation framework on the npcomplete sat problem and nphard maxsat problem. a sat problem consists of a propositional logic formula f given in conjunctive normal form. the goal of the sat problem is to find an assignment to the variables of f such that f evaluates to true. maxsat is the optimization version of sat in which the goal is to find the largest set of clauses of f that have some satisfying assignment. a version of maxsat can also have a weight associated with each clause with the objective then being to maximize the sum of satisfied clauses. in this work however we concentrate only on the unweighted variant of maxsat and describe our structure identification routines selectstruct destroy repair and acceptance operators. due to the simi larity of the sat and maxsat problems our instance generation procedure is the same with the exception of the acceptance criteria which we modify to avoid trivial sat instances. structure identification many industrial satmaxsat instances consist of a number of connected components that are bound together through a core of common variables see figure . our goal is to identify one of these components in an instance at random and remove it. to this end we propose two heuristics for identifying such structures that we use in both the destroy and repair functions with probability in each iteration. the first heuristic identifies a set of clauses shared by a particular variable whereas the second identifies a clause and selects all of the clauses it shares a variable with. our goal in the variablebased selection heuristic is to identify components of in stances with a shared variable as shown in algorithm . we first calculate the mean number of clauses that a variable is in. variables in many clauses are likely to be a part of the core of an instance that connects various subcomponents whereas variables in the average number of clauses are more likely to be part of the subcomponents them selves. the algorithm selects a set of variables e in the average number of clauses if there are any. if e is empty the algorithm relaxes its strictness of how many clauses a variable should be in until some clauses are found. finally the algorithm selects a variable from e at random and returns all of the clauses that variable is present in. in contrast to our previous heuristic the clausebased selection heuristic focuses on clauses with an average out degree. the out degree of a clause is defined as the number of clauses sharing at least one variable in common with the clause. this corresponds to the out degree of the clauses node in the clausevariable graph. algorithm accepts an instance i and a parameter described below. the heuristic selects a random clause and compares its out degree to the average out degree of all the clauses. if the clauses out degree is within standard deviations of the average clause out degree we accept the clause and return all of the clauses it is connected to in the clausevariable graph. destroy our destroy function accepts an instance i and a set of clauses c selected by selectstructvar or selectstructclause that are to be removed from the instance. first all clauses in c are removed i.e. clausesi clausesi \c and then all variables that no longer belong to any clause are removed from varsi. repair our repair procedure maps the variables contained within a previously selected set of clauses to the variables present in the instance to modify and adds new variables algorithm variable based structure selection heuristic. function selectstructvari e f a meanvarinclausesi while e do e v varsi clausesv a f f f return inclausesrandom variable in e algorithm clause based structure selection heuristic. function selectstructclausei c mcod meanclauseoutdegreei scod stdclauseoutdegreei while c do c random clause in i cod clauseoutdegreec if cod mcod scod then c c clausesi c and c share at least one variable. return c with some probability. to avoid confusion we refer to the instance being modified as the receiver and the instance providing clauses as the giver. algorithm shows the repair process which is initialized with the receiving instance r the set of clauses to add c and some number of variables to add to the instance d. the parameter d is used to increase the size of the receiver if too many variables are deleted during the destruction phase. additionally we note that c contains clauses from the giver meaning the variables in those clauses do not match those in the receiving instance. thus the main action of the repair method is to find a mapping m that allows us to convert the variables in c into similar variables in the receiver. we map the variables in c into the variables of r by computing the following three features for each variable in the varfeatures function. we use these features because our goal is to map variables with similar connectivity to other parts of the instance with each other and they are easy to compute. . number of clauses the variable is in divided by the total number of instance clauses. . percent of clauses the variable is in in which the variable is positive. . average of the number of variables of each clause the variable v is in. on line of algorithm we decide whether to map vg to an existing variable in r or to a new variable. the function trivialvg returns true if vg is not i positive in at least one clause in c and ii negated in at least one clause c. this ensures that if we map vg to a new variable a valid assignment of vg is not entirely obvious. we assign vg to a new variable with probability dvg as with this probability we add roughly d new variables in the absence of trivial variables. we compute the l norm between the giver variables and the receivers variables on line and should we decide not to add a new variable to r we map vg to the variable in algorithm satmaxsat instance repair procedure. function repairr c d vg cc varsc fr varfeaturesr fg varfeaturesvg m for vg vg do if trivialvg and rnd dvg then v new variable varsr varsr v m m vg v else dists frv fgvg v varsr v argminvvarsrdistsv v m m m vg v clausesr clausesr mapvarscm return r r that most closely resembles its features that is not yet assigned to a different variable. this is a greedy procedure that finds the best match for each variable individually. finally the algorithm performs the variable mapping and merges the clauses of c into r. we omit the details of the merging process as it is straight forward. acceptance criteria we compute a set of well known features for sat and maxsat problems from in order to determine whether to accept a modified instance. we compute the average and standard deviation for each feature across the entire pool of instances including the instance to modify. an instance is accepted if all of its features are within three standard deviations of the mean. that is we compare a feature to the cluster center on a feature by feature basis. however some features do not vary at all in a cluster meaning they have a standard deviation of . in such cases even small changes to an instance can result in a rejection of all generated instances although the instance is for the most part within the cluster. thus when absolutely no instance could be generated we relax the conditions for features that do not vary within the cluster and allow them to vary by some epsilon value. we note that in our experiments such instances were still well situated within clusters when measured with the euclidean distance to the cluster center. for sat problems we extend this acceptance criteria with an execution of the in stance with a sat solver. if the instance is solvable in under seconds it is discarded. we do this because our generation procedure sometimes introduces unsatisfiable com ponents to satisfiable problems that are easily found and exploited by solvers. this clearly breaks the structure of the instance that we are striving to preserve thus the instance must be discarded. note that this does not guarantee that the instance will be satisfiable all we are checking is that the generated instance is not trivially solvable. this issue generally only happens to a couple of instances per generation procedure. we do note use local search probing features in this work. computational evaluation we perform an evaluation of structurepreserving instance generation on instances from the maxsat and sat competitions. we show that the instances generated using our method preserve structure well enough such that they are effectively solved using the same algorithms as the original instances. to evaluate this we train an algorithm se lection approach on the generated data and evaluate it on the subset of original test instances that were neither part of the training nor the generation. it is assumed that if our generated instances can allow us to train a portfolio to identify the most appropriate solver for the instance at hand then they successfully embody the same key structures as the original industrial data. for sat we also use theqscore method of to show the quality of our instance generator. all experiments were performed on a cluster of intel xeon e processors with gb of ram for random instances and gb for industrial instances for maxsat as industrial maxsat instances are very large and gb of ram for all sat instances. . maxsat for maxsat we evaluate our technique on both the random as well as industrial in stances from the maxsat competition . using the random dataset in addition to the industrial dataset shows that our method can be used for any group of similar instances even though our main target is industrial instances. here we generate our datasets according to a manually established similarity measure based on the filenames of the instances. for each pool of instances we perform instance generations for each instance of the pool with a different random seed. for the experiments presented in this section we limit each generation attempt to destroyrepair iterations. this process generated instances based off of random instances and instances based off of industrial instances one measure to ensure the quality of the generated instances is to compare the runtime of a solver on both the original and the new dataset. should the runtime perfor mance of an algorithm be similar on both the original and the new dataset we can con clude that the new dataset is similar to the old one. this is a desirable property for our instance generator and is based on a fundamental argument on which algorithm port folios are built and what makes them so successful in practice that a solveralgorithm performs analogously well or poorly on instances that are similar. figure shows the average solution time of the original instances and their gener ated counterparts for the akmaxsat and msuncore solvers for several clusterings of instances on the random and industrial datasets respectively. the clusters were gener ated based on the categories of the instances. the solutions times are comparable for both random and industrial instances with the exception of clusters and on the in dustrial dataset in which the generated instances are too hard. this runtime performance similarity strongly indicates that our generator preserves the structure of instances dur ing generation. the maxsat dataset contains instances but we remove instances over mb after performing unit propagation as spig cannot fit them in ram. akmaxsat a akmaxsat on the random dataset. msuncore b msuncore on the industrial dataset. fig. . the average solution time in cpu seconds and standard deviation for each cluster in terms of original left bar light gray and generated right bar dark gray instances. note that by comparable runtimes we do not mean identical which would be an undesirable quality since generated instances should be slightly different from the orig inals. furthermore even when running a solver on the same instance runtimes can vary. the results displayed for the industrial dataset are somewhat noisy due to the fact that very few original instances exist as a basis for comparison. for example if we had more instances for cluster in reality we only have a single instance it could very well be the case that they are hard to solve as well but the one instance we have turned out to be solved through a smart or lucky branching decision by msuncore. another important result of our cpu runtime experiments is that industrial solvers perform well on our generated industrial instances whereas random solvers tend to timeout. the opposite is also true when we run an industrial solver on our generated random instances the industrial solvers tend to timeout but the random solvers perform well. this means that our instance generation framework is able to preserve instance structure nearly regardless of what type of instance it is used on. we note however that we do not intend for our instance generator to be used on random or crafted instances as perfectly good generators already exist for these categories of instances. we show results from these categories only to serve as an evaluation of the overall approach. one might not even expect our generator to work at all on random instances as they tend to have little structure. we believe the effectiveness of our approach for such instances is simply due to random changes to a random instance not having a huge effect. industrial instances or any instance with some kind of globallocal structure however require an approach like the one we provide so that generated instances do not get malformed through completely random changes. our final experimental comparison on the maxsat dataset observes the effect of training a simple portfolio on only the generated instances as opposed to the original ones. table shows the performance of a portfolio trained and evaluated using leave oneout cross validation. note here that for evaluating the generated dataset none of the instances generated from the test instances were included in the training set. we compare the performance of only using the overall best solver to a portfolio that uses either a random forest or a support vector machine svm to predict the runtime of each original generated model average time unsolved average time unsolved best single random forest svm radial vbs table . comparison of a portfolio trained leaveoneout on only the original maxsat in stances and one that is trained on the generated instances. the average time is given in seconds. solver selecting the solver with the best expected performance. there are of course a plethora of other popular and more powerful portfolio techniques that could be used and compared but random forests and svms are readily available to anyone and have been previously shown to be effective for runtime prediction and here we show that even they are able to perform well in our case. the virtual best solver vbs gives the performance of an oracle that always picks the fastest solver. due to the limited training set the best the portfolio can do is match the performance of a single solver. however if we train the same solvers on the generated instances and evaluate on the original instances we are able to see improvements over the best solver meaning the extra generated instances provide value to the portfolio approach. our generator is able to help fill in gaps between training instances in the feature space allowing learning algorithms to avoid having to make a guess as to which algorithm will work best within such gaps. instead learning approaches have data on the instances in these gaps and can make informed decisions for their portfolio. . sat we next use the instances from the sat competition to conduct further exper iments. for the competition these instances are split into three categories each consist ing of instances application industrial crafted and random. and although this competition has taken place annually for the last decade it is important to note that the majority of the industrial instances repeat each year which means our experiments use most of the instances available.we first evaluate our generated dataset using the q score technique from . we then use an algorithm selection approach to confirm the usefulness of our generator. algorithm selection with cshc due to the increased number of available instances over the maxsat scenario we apply a more automated technique for grouping sat instances for generation. in maxsat the instances were grouped based on a manually defined similarity metric associated with the filenames. for sat however the industrial instances are clustered based on their features using the gmeans algorithm from an approach that automatically determines the best number of clusters based on how gaussian distributed each cluster is. limiting the minimum size of a cluster to re sulted in a total of clusters. we typically use instances for a cluster to ensure we have a reasonable statistical evidence that a particular solver works better than another. we sample of the instances from each cluster to compose our training set and to form the subsets of similar instances for generation. we perform a more standard portfolio evaluation of the generator for sat since so many instances are available. for this evaluation we use the top solvers from the average par solved best single random crafted industrial generated generated vbs table . comparison of a portfolio evaluated on industrial sat instances when trained on either randomly generated instances crafted instances a subset of industrial instances generated instances or generated instances. sat competition glucose glue bit lingeling f lingeling aqw mipsat rissg strangenight zenn cshcappllc and cshcappllg. our test set for all the subsequent experiments is the collection of industrial instances remaining after the subset of training instances is removed. this list is then further reduced by removing those instances for which no solver finds a solution within seconds. this leaves a total of instances. we compare the portfolios based on three metrics average time without timeouts average par and number of in stances solved solved. par is a penalized average where a timeout counts as having taken times the timeout time. for our underlying portfolio technique we utilized cshc the technique that won the open track at the sat competition and was behind the portfolio of isac that won the maxsat evaluation in and . the core premise of this portfolio technique is a branching criteria for a tree that ensures that after each partition the instances assigned to subnode maximally prefer a particular solver other than the one used in the parent node. training a forest of such trees then ensures the robustness of the algorithm. the results of the experiments are presented in table . the best standalone solver is lingeling aqw which solves a total of instances. we trained the portfolio on a variety of training sets random instances crafted instances industrial in stances generated industrial instances and generated industrial instances. not surprisingly training on random or crafted instances does not perform well. in both cases less instances can be solved than just using the best single solver and the par scores are significantly higher. this further confirms a wellknown result in the algorithm selection literature that training and test sets need to be similar in order for the learning algorithm to be successful. we include these results to emphasize the fact that if our generated instances were significantly different from the datasets they were generated from we would expect similarly bad performance on the test set. indeed us ing even just industrial instances already results in better performance than the best single solver in terms of average time par and the number of instances solved. using generated industrial instances shows similar performance to the original industrial instances in terms of the number of the par score and number of instances solved although the average runtime is higher. this is already enough evidence to fur ther confirm that our instance generation routine is successful at preserving instance structures in sat as in maxsat. however because we are not limited by the number of instances we generate we can create much larger training samples. we therefore eval uate our portfolio trained on generated instances and observe that instances can be solved more than with the original training set. in a competition setting this improvement is often the difference between first place and finishing outside the top three solvers. this provides further support that our approach fills in gaps in the in stance feature space and that this provides critical information to selection algorithms that improves their performance. qscore the qscore introduced by bayless et al. in provides a mechanism for assessing whether or not a dataset of instances can act as a proxy for some other set of instances. in other words using the qscore we can check whether the instances we generate share similar properties with the original dataset of industrial instances. the score is based on the performance of parameters found through algorithm configuration using a method like or . we use smac as it was previously used for cal culating the qscore in . we configure the lingeling and spear solvers each three times for five days on the same generated instances used in our algorithm selection experiments and all original industrial instances which we label s and t respec tively. adopting the notation of which we refer to for full details the qscore is computed by cat t ca s t where c is the par score of a parameteriza tion on the specified dataset a specified an algorithm configuration and t and s are the best performing configurations on the test set of all tuned configurations and on the generated set respectively. we found the qscore . for lingeling and . for spear on our generated instances. we note that . is the best possible score. this indicates that the datasets we generate lead to high quality parameter configurations that generalize to the orig inal instances. interestingly the best parameter configuration for lingeling on the test set was one of the parameterizations trained on the generated instances. however its training set evaluation was beaten by another parameterization thus we do not use it in the calculation of the qscore for the set s. this is especially noteworthy given that our generated instance set is not even based on all of the industrial instances but is nonethe less being compared to parameters specifically tuned on all industrial instances. . structure comparison as a final evaluation of our instance generation methodology we present a comparison of the original and generated instances when their features are projected into a two dimensional space. we do this using a standard principal component analysis pca. figure presents the results for both the maxsat and sat datasets. the figure shows that there is not a perfect matching between the generated and original instances. while future work can focus on reducing the spread between these instances we note that a perfect matching is not desirable as we do not want exact replicas of our instance pool. instead we want to cover a range of scenarios of similar instances which can be seen in many parts of the projection. this subsequently leads to a better trained portfolio. furthermore note that the generated instances tend to be close to their original counterparts in this projected space. this means that although they are not completely identical the generated instances are still fairly representative of their originals. a maxsat b sat fig. . projection of the instances into d using pca on their features. original training instances are represented as blue circles the generated instances are represented by red triangles. conclusion and future work one of the current key problems in solver development is the limited number of in stances on which algorithms can be compared. this is especially the case for industrial instances where datasets are extremely limited and difficult to expand. to remedy this this paper presented a novel methodology for generating new instances with structures similar to a given dataset. we then demonstrated the quality of the generated datasets by training portfolios on them and evaluating them on the original instances. this showed that not only do the instances have similar structures as the originals but that those structures also allow a portfolio and algorithm configuration to correctly learn the best solver for provided instances. for future work will evaluate our instance genera tion framework on other types of problems such as csps and mips as well as explore how to improve the generated instances coverage of the feature space. acknowledgements we thank the paderborn center for parallel computing for the use of the oculus cluster for the experiments in this paper. barry osullivan was sup ported in part by science foundation ireland sfi under grant number sfirc. references . ansotegui c. bejar r. fernandez c. mateu c. edge matching puzzles as hard satcsp benchmarks. in cp. lncs vol. pp. . ansotegui c. bonet m.l. levy j. li c.m. analysis and generation of pseudoindustrial maxsat instances. in ccia. faia vol. pp. . ios press . ansotegui c. giraldezcru j. levy j. the community structure of sat formulas. in cimatti a. sebastiani r. eds. theory and applications of satisfiability testing sat pp. . no. in lncs springer jan . ansotegui c. levy j. on the modularity of industrial sat instances. in f. c. geffner h. manya f. eds. ccia. faia vol. pp. . ios press . ansotegui c. sellmann m. tierney k. a genderbased genetic algorithm for the au tomatic configuration of algorithms. in gent i. ed. principles and practice of constraint programming cp. lncs vol. pp. . springer . ansotegui c. malitsky y. sellmann m. maxsat by improved instancespecific algo rithm configuration. aaai . argelich j. li c.m. manya f. planes j. eighth maxsat evaluation . balint a. belov a. heule m. jarvisalo m. proceedings of sat competition solver and benchmark descriptions. tech. rep. university of helsinki . bartak r. on generators of random quasigroup problems. in recent advances in con straints lncs vol. pp. . springer . bastian m. heymann s. jacomy m. gephi an open source software for exploring and manipulating networks. in aaai conference on weblogs and social media . bayless s. tompkins d. hoos h. evaluating instance generators by configuration. in pardalos p.m. resende m.g.c. vogiatzis c. walteros j.l. eds. lion . lncs vol. pp. . springer . bejar r. cabiscol a. manya f. planes j. generating hard instances for maxsat. in in ternational symposium on multiplevalued logic ismvl . pp. may . burg s. kottler s. kaufmann m. creating industriallike sat instances by clustering and reconstruction. in sat pp. . springer . dinur i. goldwasser s. lin h. the computational benefit of correlated instances. in proceedings of the conference on innovations in theoretical computer science. pp. . acm . gent i.p. hoos h.h. prosser p. walsh t. morphing combining structure and random ness. in hendler j. subramanian d. eds. aaai. pp. . gomes c.p. selman b. problem structure in the presence of perturbations. in kuipers b. webber b.l. eds. aaai. pp. . hamerly g. elkan c. learning the k in kmeans. in neural information processing sys tems nips . hutter f. hoos h. leytonbrown k. sequential modelbased optimization for general algorithm configuration. in lion pp. . springer . kadioglu s. malitsky y. sellmann m. tierney k. isac instancespecific algorithm configuration. in ecai. faia vol. pp. . ios press . katsirelos g. simon l. eigenvector centrality in industrial sat instances. in milano m. ed. cp. lncs vol. pp. . springer . krzakala f. zdeborova l. hiding quiet solutions in random constraint satisfaction prob lems. physical review letters . lopes l. smithmiles k. generating applicable synthetic instances for branch problems. operations research . malitsky y. sabharwal a. samulowitz h. sellmann m. algorithm portfolios based on costsensitive hierarchical clustering. ijcai . motoki m. test instance generation for max sat. in van beek p. ed. cp pp. . no. in lncs springer jan . nallaperuma s. wagner m. neumann f. parameter prediction based on features of evolved instances for ant colony optimization and the traveling salesperson problem. in ppsn xiii lncs vol. pp. . springer . nudelman e. leytonbrown k. hoos h.h. devkar a. shoham y. understanding ran dom sat beyond the clausestovariables ratio. in cp pp. . springer . pari p.r. lin j. yuan l. qu g. generating random sat instances with specific solu tion space structure. in mcguinness d.l. ferguson g. eds. aaai. pp. . shaw p. using constraint programming and local search methods to solve vehicle routing problems. in cp pp. . springer . slater a. modelling more realistic sat problems. in mckay b. slaney j.k. eds. aus tralian joint conference on ai. lncs vol. pp. . springer . smithmiles k. bowly s. generating new test instances by evolving in instance space. computers operations research . smithmiles k. van hemert j. discovering the suitability of optimisation algorithms by learning from evolved instances. ann math artif intell . van gelder a. spence i. zeroone designs produce small hard sat instances. in strich man o. szeider s. eds. sat pp. . no. in lncs springer jan  ar x iv . v cs .a i ja n transformationbased feature computation for algorithm portfolios barry hurley serdar kadioglu yuri malitsky and barry osullivan insight centre for data analytics department of computer science university college cork ireland b.hurleyy.malitskyc.ucc.ie b.osullivancs.ucc.ie oracle america inc. burlington ma usa serdarkcs.brown.edu abstract. instancespecific algorithm configuration and algorithm port folios have been shown to offer significant improvements over single algo rithm approaches in a variety of application domains. in the sat and csp domains algorithm portfolios have consistently dominated the main com petitions in these fields for the past five years. for a portfolio approach to be effective there are two crucial conditions that must be met. first there needs to be a collection of complementary solvers with which to make a portfolio. second there must be a collection of problem features that can accurately identify structural differences between instances. this paper focuses on the latter issue feature representation because unlike sat not every problem has wellstudied features. we employ the wellknown satzilla feature set but compute alternative sets on different sat en codings of csps. we show that regardless of what encoding is used to convert the instances adequate structural information is maintained to differentiate between problem instances and that this can be exploited to make an effective portfoliobased csp solver. introduction significant strides have recently been made in the application of portfoliobased algorithms in the fields of constraint satisfaction quantified boolean formu lae and most notably in sat . having a collection of solvers these approaches compute a set of representative features about a problem instance and then use this information to decide what is the most effective solver to em ploy. these decisions can be made based on regression techniques in which a classifier is trained to predict expected runtime of each solver and choosing the one with best predicted performance. alternatively a ranking algorithm can be trained to directly predict the best solver for each instance . the features can also be used for clustering where the best solver is chosen for each cluster of instances. in practice regardless of the approach portfolio algorithms have been shown to be dramatically better than using a single solver. algorithm portfolios also rely on a good set of features to describe the prob lem instance being solved. this can be seen as a major drawback since one needs httparxiv.orgabs.v to use specific features for each problem at hand or worse has to come up with a set of features if none exists. if there are not enough informative features present it is impossible to train a classifier to differentiate between classes of instances. on the other hand if there are too many features it is possible to over fit the classifier to the training data. furthermore a large feature set is likely to have noisy features which could be detrimental to the quality of the learned classifier. in the sat domain the features used by the solvers dominating the competitions have been thoroughly analyzed and studied over the last decade. unfortunately many other fields do not have such well established feature set. even in the case of constraint satisfaction problems where a feature set has been proposed careful filtering can dramatically improve the quality of portfolios . however while there might not be an existing feature set for npcomplete problems there exist polynomialtime transformations to any other npcomplete problem. in this paper we propose to take advantage of this by transforming csp instances to sat as a preprocessing step before computing its features. we show that such a transformation retains the necessary information needed to differentiate the classes of instances. in particular we show the effectiveness of this approach on constraint satisfaction problems. we choose the csp domain for two reasons. first it has a large number of solvers that can be used to make a diversified portfolio. second because a feature set exists for csps we can compare the quality of a portfolio trained on sat features to the domain specific csp features. there has been a lot of work exploring the effect of transforming csp in stances into sat. perhaps the most relevant work is by ansotegui and manya which evaluated the performance of sat solvers on six satencodings on graph colouring random binary csps pigeon hole and all interval series problems . solvers such as sugar azucar and cspsatj have similarly tackled csp problems by encoding them into sat and then solving them with a prede fined sat solver. however as far as we are aware this paper represents the first time that a portfolio has been created using features gained after transforming a problem from one domain to another. encodings there are a number of known polynomialtime transformations or encodings from constraint satisfaction problems to sat . in this paper we focus on three commonly used encodings the direct order and support encodings. . direct encoding in the direct encoding for each csp variable x with domain . . . d a sat variable is created for each domain value i.e. x x . . . xd. if x is true in the resulting sat formula then the csp variable x is assigned the value in the csp solution. therefore in order to represent a solution to the csp exactly one of x x . . . xd must be assigned true. we add an atleastone clause and atmostone clauses to the sat formula for each csp variable x x x . . . xd v w dx xv xw at least one at most one constraints between csp variables are represented in the direct encoding by enumerating the conflicting tuples. for a binary constraint between the pair of variables x and y if the tuple x v y w is forbidden then we add the conflict clause xv yw. . support encoding the support encoding uses the same mechanism as the direct encoding to translate a csp variables domain into sat. however the support encoding differs on how the constraints between variables are encoded. given a constraint between two variablesx and y for each value v in the domain ofx let syxv dy be the subset of the values in the domain of y which are consistent with assigning x v. either xv is false or one of the consistent assignments from y . . . yd must be true represented by the clause xv isyxv yi this must be repeated by adding clauses for each value in the domain of y and listing the values in x which are consistent with each assignment. . order encoding unlike the direct and support encoding which model x v as a sat variable the order encoding creates sat variables to represent x v. if x is less than or equal to v then x must also be less than or equal to v . to enforce this across the domain we add the clauses dv xv xv the order encoding is naturally suited to modelling inequality constraints. to state x we would just post the unit clause x. if we want to model the constraint x v we could rewrite it as x v x v. x v can then be rewritten as x v . to state that x v under the order encoding we would encode xv xv. a conflicting tuple between two variables for example x v y w can be written in propositional logic and simplified to a cnf clause using de morgans law xv xv yw yw xv xv yw yw xv xv yw yw xv xv yw yw feature computation in addition to the pure direct support and order encodings discussed in the previous section we also consider variants of these encodings in which the clauses that encode the domains of the variables are not included. we omit the domains in order to test whether focusing only on the constraints present in a csp is enough to differentiate the instances. we now briefly describe the features used for csp and sat. csp features. we compute features for each of the original csp instances plus for each of the six encodings. we record features directly from the csp instance using mistral . this includes static features such as statistics about the types of constraints used average and maximum domain size and dynamic statistics recorded by running mistral for seconds average and standard deviation of variable weights number of nodes number of propagations and a few others. sat features. we use the features computed using the newest feature com putation tool from ubc . these features include problem size features graph based features balance features proximity to horn formula features dpll prob ing features and local search probing features. numerical results we implemented a tool to translate a csp instance specified in xcsp format into sat cnf. at present it is capable of encoding inequality and binary ex tensional constraints using the direct support and order encoding. benchmarks. for our evaluation we consider csp problem instances from the csp solver competition. of these we consider the instances that contain ei ther inequality or binary extensional constraints. this presents a pool of instances containing graph colouring random quasirandom black hole quasigroup completion quasigroup with holes langford towers of hanoi and pigeon hole problems. portfolio approach. to train our portfolios we used the isac methodology which has been shown to work better than a regression based approaches . isac uses the computed features to cluster the instances. then for each clus ter the best solver in the portfolio is selected. when a new instance needs to be solved its features are computed it is assigned to the nearest cluster and subsequently solved using the appropriate solver. for our csp solver portfolio we used abscon cspj satj pcs gecode and sugar . each instance was run for seconds. it is important to note that we include the time required for encoding the in stances and computing the features as part of the computation time. csp solver competition instances httpwww.cril.univartois.frlecoutrebenchmarks.html httpwww.cril.univartois.frlecoutrebenchmarks.html table . comparison of number of solved instances and par score between the virtual best solver vbs the portfolio approach the random cluster approach and the best single solver based on csp and sat features for clustering. par approach csp direct direct order order support support nd nd nd vbs portfolio random cluster best single number solved approach csp direct direct order order support support nd nd nd vbs portfolio random cluster best single we perform our experiments using stratified fold cross validation. in ta ble we present the performance for both the number of solved instances and the penalized runtime average par which counts each timeout as taking times the timeout to complete for each problem representation. the sat encodings without the variable domains are marked with nd. we compare the portfolio performance to the best single solver as well as to the oracle virtual best solver vbs which for every instance always selects the fastest solver. as we can see using a portfolio approach for csp instances is always preferable to just choosing to run a single solver. we also compare to a random clustering approach which randomly groups the instances of the test set into the same number of clusters as the portfolio method and finds the best solver to run on each group. note that the random clustering is trained on the same data it is evaluated on and further that in practice one would not know to which cluster to assign a new instance. the random clustering approach is included to show that the clusters found by isac are indeed capturing important information about the instances. we observe this because in all cases portfolio is better than the random clustering approach. table also shows that regardless of the encoding we use we can always close at least of the performance gap between the best single solver and the virtual best one. furthermore we see that if we use particularly accurate encoding which in our case is the support encoding without domain clauses we can even achieve slightly better performance than using features that have been specifically designed for the problem domain. conclusion in this paper we show that it is possible to encode an instance from one problem domain to another as a preprocessing step for feature computation. in particu lar we show that even with the overhead of converting csp instances to sat a csp portfolio trained on well established sat features can perform just as well as if it was trained on csp specific features. these findings show that encoding techniques can retain enough information about the original instance to accu rately differentiate different classes of instances. our results serves as a proof of concept for an automated feature generation approach for npcomplete prob lems that do not have a well studied feature vector. we consider this as a step toward problem independent feature computation for algorithm portfolios and we plan to analyze it further and extend its applications in the future. acknowledgements the second author was supported by paris kanellakis fellowship at brown uni versity when conducting the work contained in this document. this document reflects his opinions only and should not be interpreted either expressed or im plied as those of his current employer. references . ansotegui c. manya f. mapping problems with finitedomain variables into problems with boolean variables. in the th international conference on theory and applications of satisfiability testing sat . gecode team gecode generic constraint development environment httpwww.gecode.org . gent i.p. arc consistency in sat. in proceedings of the th european confer ence on artificial intelligence ecai. pp. . hebrard e. mistrala constraint satisfaction library. in proceedings of the third international csp solver competition . hurley b. osullivan b. adaptation in a cbrbased solver portfolio for the satisfiability problem. in casebased reasoning research and development th international conference iccbr . pp. . kadioglu s. malitsky y. sabharwal a. samulowitz h. sellmann m. algo rithm selection and scheduling. in proceedings of the th international confer ence on principles and practice of constraint programming. pp. . cp springerverlag berlin heidelberg . kadioglu s. malitsky y. sellmann m. tierney k. isac instancespecific algorithm configuration. in coelho h. studer r. wooldridge m. eds. ecai. frontiers in artificial intelligence and applications vol. pp. . ios press . kasif s. on the parallel complexity of discrete relaxation in constraint satis faction networks. artificial intelligence oct httpwww.gecode.org . kroer c. malitsky y. feature filtering for instancespecific algorithm configura tion. in ieee rd international conference on tools with artificial intelligence ictai . pp. . le berre d. parrain a. the satj library release . system description. journal on satisfiability boolean modeling and computation . le berre d. lynce i. cspsatj a simple csp to sat translator. in pro ceedings of the nd international csp solver competition . lecoutre c. tabary s. abscon toward more robustness. in proceedings of the third international csp solver competition . lin xu frank hutter h.h. leytonbrown k. features for sat httpwww.cs.ubc.calabsbetaprojectssatzillareportsatfeatures.pdf . malitsky y. sabharwal a. samulowitz h. sellmann m. nonmodel based algorithm portfolios for sat. in proceedings of the th inter national conference on theory and application of satisfiability test ing. pp. . sat springerverlag berlin heidelberg httpdl.acm.orgcitation.cfmid. . omahony e. hebrard e. holland a. nugent c. osullivan b. using case based reasoning in an algorithm portfolio for constraint solving. proceeding of the th irish conference on artificial intelligence and cognitive science . prestwich s.d. cnf encodings. in handbook of satisfiability pp. . ios press . pulina l. tacchella a. a multiengine solver for quantified boolean formulas. in proceedings of the th international conference on principles and practice of constraint programming. pp. . cp springerverlag berlin heidelberg . roussel o. lecoutre c. xml representation of constraint networks format xcsp .. corr abs. . tamura n. tanjo t. banbara m. system description of a satbased csp solver sugar. in proceedings of the rd international csp solver competition. pp. . tanjo t. tamura n. banbara m. azucar a satbased csp solver using compact order encoding tool presentation. in proceedings of the th in ternational conference on theory and applications of satisfiability testing sat lncs . pp. . springer . veksler m. strichman o. a proofproducing csp solver. in proceedings of the twentyfourth aaai conference on artificial intelligence aaai . walsh t. sat v csp. in principles and practice of constraint programming cp lncs . vol. pp. . springerverlag . xu l. hutter f. hoos h.h. leytonbrown k. satzilla portfoliobased al gorithm selection for sat. journal of artificial intelligence research pp. june httpwww.cs.ubc.calabsbetaprojectssatzillareportsatfeatures.pdf httpdl.acm.orgcitation.cfmid. transformationbased featurecomputation for algorithm portfolios  tuning parameters of large neighborhood search for the machine reassignment problem yuri malitsky deepak mehta barry osullivan and helmut simonis cork constraint computation centre university college cork ireland y.malitskyd.mehtab.osullivanh.simonisc.ucc.ie abstract. data centers are a critical and ubiquitous resource for pro viding infrastructure for banking internet and electronic commerce. one way of managing data centers efficiently is to minimize a cost function that takes into account the load of the machines the balance among a set of available resources of the machines and the costs of moving pro cesses while respecting a set of constraints. this problem is called the machine reassignment problem. an instance of this online problem can have several tens of thousands of processes. therefore the challenge is to solve a very large sized instance in a very limited time. in this paper we describe a constraint programmingbased large neighborhood search lns approach for solving this problem. the values of the parameters of the lns can have a significant impact on the performance of lns when solving an instance. we therefore employ the instance specific algorithm configuration isac methodology where a clustering of the instances is maintained in an offline phase and the parameters of the lns are automatically tuned for each cluster. when a new instance arrives the values of the parameters of the closest cluster are used for solving the instance in the online phase. results confirm that our cpbased lns approach with high quality parameter settings finds good quality solu tions for very large sized instances in very limited time. our results also significantly outperform the handtuned settings of the parameters se lected by a human expert which were used in the runnerup entry in the euroroadef challenge. introduction data centers are a critical and ubiquitous resource for providing infrastructure for banking internet and electronic commerce. they use enormous amounts of electricity and this demand is expected to increase in the future. for example a report by the eu standby initiative stated that in western european data centers consumed terawatt hours twh of power which is expected to almost double to twh per year by . a typical optimization challenge in the domain of data centres is to consolidate machine workload to ensure that this work is supported by science foundation ireland grant no. in.i and by the european union under fet grant icon project number . httpre.jrc.ec.europa.euenergyefficiencyhtmlstandbyinitiativedatacenters.htm machines are well utilized so that energy costs can be reduced. in general the management of data centers provides a rich domain for constraint programming and combinatorial optimization . context. given the growing level of interest from the optimization community in data center optimization and virtualization the roadef challenge was focused on machine reassignment a common task in virtualization and service configuration on data centers. informally the machine reassignment problem is defined by a set of machines and a set of processes. each machine is associated with a set of available resources e.g. cpu ram etc. and each process is asso ciated with a set of required resource values and a currently assigned machine. there are several reasons for reassigning one or more processes from their cur rent machines to different machines. for example if the load of the machine is high then one might want to move some of the processes from that machine to other machines. similarly if the machine is about to shut down for maintenance then one might want to move all processes from the machine. also if there is a different location where the electricity price is cheaper then one might want to reassign some processes to the machines at that location such that the total cost of electricity consumption is reduced. in general the task is to reassign the processes to machines while respecting a set of hard constraints in order to improve the usage of the machines as defined by a complex cost function. contributions of this paper. the machine reassignment problem is one that needs to be solved in an online manner. the challenge is to solve a very large size problem instance in a very limited time. in order to do so we formulate the problem using constraint programming cp as described in and use large neighborhood search lns to solve it. the basic idea of cpbased lns is to repeatedly consider a subproblem which defines a candidate neighborhood and reoptimize it using cp. in the machine reassignment problem context we select a subset of processes and reassign machines to them. in this paper we describe our cpbased lns approach in detail. there are several parameters to choose when implementing lns e.g. size of the neighborhood when to change the neighborhood size threshold in terms of timefailures for solving a subproblem etc. the values of these parameters can have a significant impact on the efficiency of lns. we expose the parameters of our cpbased lns approach and study the impact of these parameters on lns. it is well known that manually tuning a parameterized solver can be very tedious and often consume a significant amount of time. moreover manual tun ing rarely achieves maximal potential in terms of performance gains. therefore we study the application of a genderbased genetic algorithm gga for con figuring the parameters automatically . experimental results show that the performance of the lns solver tuned with gga improves significantly as com httpchallenge.roadef.orgenindex.php pared with the manually tuned lns solver. furthermore it is important to note that while tuning the parameters of gga requires significant computa tional resources it is still far faster than manual tuning. additionally gga is an automated process that can be run in the background thus releasing devel opers to focus their efforts on developing new algorithms rather than manually experimenting with parameter settings. finally the initial computational expen diture is further mitigated by the fact that the machine reassignment problem will be solved repeatedly in the future so the costs tuning are amortized over time as the system is used in practice. in the real world setting one can anticipate that the instances of the machine reassignment problem may differ from time to time. thus it is possible that one setting of parameters might not result in the best possible performance of the lns solver across all possible scenarios. we therefore propose a methodology whereby in the offline phase a system continuously maintains a clustering of the instances and the lns solver is tuned for each cluster of instances. in the online phase when a new instance arrives the values of the parameters of the closest cluster are used for solving the instance. for this we study the application of instancespecific algorithm configuration isac . experimental results confirm that this further improves the performance of the lns solver when compared with the solver tuned for all the instances with gga. overall the experimental results suggest that the proposed cpbased lns approach with the aid of learning high quality parameter settings can find a good quality solution for a very large size instance in a very limited time. the current computer industry trend is toward creating processor chips that contain multiple computation cores. if tuning the parameters of an lns solver manually for singlecore machine is tedious then tuning for multiple parame terizations that would work harmoniously on multicore machine is even more complex. we present an approach that can exploit multiple cores and can provide an orderofmagnitude improvement over manually configured parameters. the paper is organized as follows. the machine reassignment problem is briefly described in section followed by the lns used for solving this problem in section . section describes how the parameters of lns are tuned and section presents experimental results followed by conclusions in section . machine reassignment problem in this section we briefly describe the machine reassignment problem of roadef euro challenge in collaboration with google. let m be the set of ma chines and p be the set of processes. a solution of the machine reassignment problem is an assignment of each process to a machine subject to a set of con straints. let op be the original machine on which process p is currently running. the objective is to find a solution that minimizes the cost of the reassignment. the manually tuned solver was runner up in the roadefeuro challenge and the difference between the first and the second was marginal. httpchallenge.roadef.orgfilesproblemdefinitionv.pdf in the following we describe the constraints various types of costs resulting from the assignment and the objective function. . constraints capacity constraints. the usage by a machine m of resource r denoted by umr is equal to the sum of the amount of resource required by processes that are assigned to machine m. the usage by a machine of a resource should not exceed the capacity of the resource. conflict constraints. a service is a set of processes and a set of services partition the set of processes. the constraint is that the processes of a service should be assigned to different machines. spread constraints. a location is a set of machines and a set of locations partition the set of machines. these constraints ensure that processes of a service should be assigned to the machines such that their corresponding locations are spread over at least a given number of locations. dependency constraints. a neighborhood is a set of machines and a set of neighborhoods also partition the machines. the constraint states that if service s depends on service s then the set of the neighborhoods of the machines assigned to the processes of service s must be a subset of the set of the neighborhoods of the machines assigned to the processes of service s. transient usage constraints. when a process is moved from one machine to another machine some resources e.g. hard disk space are required in both source and target machines. these resources are called transient resources. the transient usage of a machine m for a transientresource r is the sum of the amount of resource required by processes whose original or current machine is m. the transient usage of a machine for a resource should not exceed its capacity. . costs the objective is to minimize the weighted sum of load balance and move costs. load cost. the safety capacity limit provides a soft limit any use above that limit incurs a cost. let scmr be the safety capacity of machine m for resource r. the load cost for a resource r is equal to mm max umr scmr. balance cost. to balance the availability of resources a balance b is defined by a triple which consists of a pair of resources rb and r b and a multiplier tb. for a given triple b the balance cost is mm max tb am rb am rb with am r cmr umr. move cost. a process move cost is defined as the cost of moving a process from one machine to another machine. the service move cost is defined as the maximum number of processes moved among services. . instances table shows the features of the machine reassignment problem and their limits on the instances of the problem that we are interested in solving. as this is table . features of the problems instances. feature machines processes resources services locations neighbourhoods dependencies limit an online problem the challenge is to solve very large sized instances in a very limited time. although the time limit for the competition was seconds we restrict the runtime to seconds as we are solving more than instances with numerous parameter settings of the lns solver. large neighborhood search we formulated the machine reassignment problem using constraint program ming cp which is described in . we used large neighborhood search for solving the instances of the problem. in this paper we omit the details of the cp model and we focus on the details of our lns approach for solving this problem. in particular we focus on the parameters of the lns solver that are carefully tuned for solving the problem instances efficiently. lns combines the power of systematic search with the scaling of local search. the overall solution method for cpbased lns is shown in figure . we maintain a current assignment which is initialized to the initial solution given as input. at every iteration step we select a subset of the processes to be reassigned and accordingly update the domains of the variables of the cp model. we solve the resulting cp problem with a threshold on the number of failures and keep the best solution found as our new current assignment. . selecting a subproblem in this section we describe how a subproblem is selected. our observation is that selecting a set of processes for reassignment from only some machines is better than selecting them from many machines. the reason is that if we select only a few processes from many machines then we might not free enough resources initial assignment op current assignment qp select processmachine for subproblem create subproblem reoptimize subproblem using cp improved solution fig. . principles of the cpbased lns approach from the machines for moving the processes with large resource requirements from their original machines. therefore our subproblem selection is a two step process. first we select a set of machines and then from each selected machine we select a set of processes to be reassigned. the number of machines that are selected in a given iteration is denoted by km. the maximum number of processes that are selected for reassignment from each selected machine is denoted by kp. both km and kp are nonzero positive integers. the values of km and kp can change during iterations of lns. they are decided based on the several parameters of the lns solver. the number of processes that can be selected from one machine is bounded by an integer parameter which is denoted by up. therefore kp up. the total number of processes that can be selected for reassignment is bounded by an integer parameter which is denoted by tp. therefore km kp tp. if all the processes of a machine are selected then many of them might be reassigned to their original machines again. therefore we enforce that the num ber of processes selected from a given machine should be less than or equal to some factor of the average number of processes on a machine. more precisely kp rp p m . here rp is a continuos parameter. initially km is set to . as search progresses it is incremented when a certain number of iterations in lns are performed consecutively without any improve ment in the quality of the solution . the maximum value that can be set to km is denoted by tm. we reinitialize km to when it exceeds tm. depending on the value of km the value of kp can change. notice that fewer processes on a machine implies fewer combinations of the processes that can be selected from the machine for reassignment and hence fewer possible subproblems that can be created from the selected machines. therefore the bound on the number of consecutive nonimproving iterations is obtained by multiplying the average number of processes on a machine i.e. p m by a continuous parameter which is denoted by rm. the value of rm is bounded within . and . notice that up tp rp tm and rm are constant parameters of the lns algorithm and different values of these parameters can have a significant impact on the efficiency of cpbased lns approach. . creating the subproblem when solving a problem using lns the conventional way of creating a subprob lem is to reinitialize all the domains of all the variables reassign the machines to the processes that are not chosen for reassignment and perform constraint propagation. this way of creating a subproblem can be a bottleneck for lns when we are interested in solving a very large sized problem in a very limited time. furthermore if the size of the subproblem is considerably smaller than the size of the full problem then the number of iterations that one would like to perform will increase in which case the time spent in creating the subproblems will also increase further. for example let us assume that the total number of processes is the number of machines is and the maximum number of processes selected for reassignment for each iteration is . if we want to consider each process at least once for reassignment then we need at least iterations. assuming that the timelimit is seconds an average of . seconds will be spent on each iteration. each iteration would involve selecting processes for reas signment reseting more than variables to their original domains freezing variables performing constraint propagation and finally reoptimizing the subproblem. one can envisage that in this kind of scenario the time spent on creating subproblems can be a significant part of solving the problem. this drawback is mainly because a cp solver is typically designed for sys tematic search. at each node of the search tree a cp solver uses constraints to remove inconsistent values from the domains and it uses trailing or copying with recomputation for restoring the values. both trailing and copying with re computation techniques are efficient for restoring the domains when an ordering is assumed on the way decisions are undone. however in lns one can move from one partial assignment to another by undoing any subset of decisions in an arbitrary order. therefore if an existing cp solver is used for lns then unassigning a set of processes would result in backtracking to the node in the search tree where all the decisions made before that node are still in place and in the worstcase this could be the root node. we propose a novel approach for creating the subproblem. the general idea is to use constraints to determine whether a removed value can be added to the current domain when a set of assignments are undone. instead of using trailing or copying we maintain two sets of values for each variable the set of values that are in the current domain of the variable and the set of values that are removed from the original domain. additionally we maintain two sets of variables a set of variables whose domains can be replenished and and a set of variables whose domains cannot be replenished. in each iteration the former is initialized by the variables whose assignments are undone and the latter is initialized by the variables whose assignments are frozen. a variable whose domain cannot be replenished is also called a reduced variable. the domain of a variable is replenished by checking the consistency of each removed value with respect to the constraints that involve reduced variables. whenever a domain is replenished the variable is tagged as a reduced variable its neighbors that are not reducedvariables are considered for replenishing their domains and constraint propagation is performed on the problem restricted to the set of reduced variables. a fixed point is reached when no variable is left for replenishment. this approach is called replenishing the domains via incre mental recomputation. notice that existing cpsolvers do not provide support for replenishing domains via incremental recomputation. the main advantage of this approach is that it is not dependent on the order of the assignments and therefore it can be used efficiently to create subproblems during lns. . reoptimizing the subproblem we use systematic branch and bound search with a threshold on the number of failures for solving a given subproblem. the value of the threshold is obtained by multiplying the number of processes that are selected for reassignment with the value of a continuous parameter which is denoted by tf . the value of tf ranges between . and . at each node of the search tree constraint propagation is performed to reduce the search space. the variable ordering heuristic used for selecting a process is based on the sum of the increment in the objective cost when assigning a best machine to the process and the total weighted requirement of the process which is the sum of the weighted requirements of all resources divided by the number of machines available for the process. the value ordering heuristic for selecting a machine for a given process is based on the minimum increment in the objective cost while ties are broken randomly. tuning parameters of lns while it is possible to reason about certain parameters and their effect on the overall performance individually there are numerous possible configurations that these parameters can take. the fact that these effects might not be smoothly con tinuous or that there may be subtle nonlinear interactions between parameters complicates the problem further. augment this with the time incurred at trying certain parameterizations on a collection of instances and it becomes clear why one cannot be expected to tune the parameters of the solver manually. table . parameters of lns for machine reassignment problem notation type range description up integer upper bound on the number of processes that can be selected from one machine for reassignment tp integer upper bound on the total number of processes that can be selected for reassignment rp continuous . ratio between the average number of processes on a ma chine tm integer upper bound on the number of machines selected for subproblem selection rm continuous . ratio between the upper bound on the consecutive non improving iterations and the average number of processes on a machine tf continuous . ratio between the threshold on the number of failures and the total number of processes selected for reassign ment in the case of our lns solver table lists and explains the parameters that can be controlled. although there are only six parameters half of them are con tinuous and have large domains. therefore it is impractical to try all possible configurations. furthermore the parameters are not independent of each other. to test this we gathered a small set of problem instances and evaluated randomly selected parameter settings on this test set. then using the av erage performance on the data set as our evaluation metric and the parameter settings as attributes we ran feature selection algorithms from weka . all the attributes were found as important. adding polynomial combinations of the parameter settings further revealed that some pairs of parameters were more important than others when predicting expected performance. because of the difficulty of fully extracting the interdependencies of the pa rameters and covering the large possible search space a number of automated al gorithm configuration and parameter tuning approaches have been proposed over the last decade. these approaches range from gradientfree numerical optimiza tion gradientbased optimization iterative improvement techniques and iterated local search techniques like focusedils . one of the more successful of these approaches is the genderbased genetic algorithm gga a highly parallelizable tool that is able to handle con tinuous discrete and categorical parameters. being a genetic algorithm gga starts with a large random population of possible parameter configurations. this population is then randomly split into two even groups competitive and noncompetitive. the members of the competitive set are further randomly bro ken up into tournaments where the parameterizations in each tournament are raced on a subset of training instances. the best performing parameter settings of each tournament get the chance to crossover with the members of the non competitive population and continue to subsequent generations. in the early generations each tournament has only a small subset of the instances but the set grows at each generation as the bad parameter settings get weeded out of consideration. in the final generation of gga when all training instances are used the best parameter setting has been shown to work very well on these and similar instances. general tuning of a solvers parameters with tools like gga however ignores the common finding that there is often no single solver that performs optimally on every instance. instead different parameter settings tend to do well on differ ent instances. this is the underlying reason why algorithm portfolios have been so successful in sat cp qbf and many other domains. these portfolio algorithms try to identify the structure of an instance beforehand and predict the solver that will have the best performance on that instance. isac is an example of a very successful nonmodel based portfolio ap proach. unlike similar approaches such as hydra and argosmart isac does not use regressionbased analysis. instead it computes a representative fea ture vector in order to identify clusters of similar instances. the data is there fore clustered into nonoverlapping groups and a single solver is selected for each group based on some performance characteristic. given a new instance its fea tures are computed and it is assigned to the nearest cluster. the instance is then evaluated with the solver assigned to that cluster. isac works as follows see algorithm . in the learning phase isac is provided with a parameterized solver a a list of training instances t their cor responding feature vectors f and the minimum cluster size . first the gathered algorithm instancespecific algorithm configuration isaclearna t f f s t normalizef k c s cluster t f for all i . . . k do pi ggaasi return k pc s t isacruna x k p c d s t f featuresx fi fisi ti i i minif ci return ax pi features are normalized so that every feature ranges from and the scaling and translation values for each feature s t is memorized. this normalization helps keep all the features at the same order of magnitude and thereby keeps the larger values from being given more weight than the lower values. then the instances are clustered based on the normalized feature vectors. clustering is advantageous for several reasons. first training parameters on a collection of instances generally provides more robust parameters than one could obtain when tuning on individual instances. that is tuning on a collection of instances helps prevent overtuning and allows parameters to generalize to similar instances. secondly the found parameters are prestabilized meaning they are shown to work well together. to avoid specifying the desired number of clusters beforehand the gmeans algorithm is used. robust parameter sets are obtained by not allowing clusters to contain fewer than a manually chosen threshold a value which depends on the size of the data set. in our case we restrict clusters to have at least instances. beginning with the smallest cluster the corresponding instances are redistributed to the nearest clusters where proximity is measured by the eu clidean distance of each instance to the clusters center. the final result of the clustering is a number of k clusters si and a list of cluster centers ci. then for each cluster of instances si favorable parameters pi are computed using the instanceoblivious tuning algorithm gga. when running algorithm a on an input instance x isac first computes the features of the input and normalize them using the previously stored scaling and translation values for each feature. then the instance is assigned to the nearest cluster. finally isac runs a on x using the parameters for this cluster. in practice however the tuning step of the isac methodology can be very computationally expensive requiring on the order of a week on an core ma chine. fortunately it is not necessary to retune the algorithms too frequently. in many cases even when new instances become available the clusters are not likely to shift. we therefore propose the methodology shown in figure . here given a set of initial instances we perform the isac methodology to find a set of clusters for which the algorithm is tuned. when we observe new instances we evaluate them according to the isac approach as shown in figure and after wards add the instance to the appropriate cluster. but we also try reclustering the entire set of instances. in most cases the two clusterings will be similar so nothing needs to be changed. but as we gather more data we might see that one new instance current instances initial instances cluster cluster is difference between clusterings significant create clusters by adding instance to the best cluster current clusters tune parameters for each cluster yes fig. . offline phase tuning lns solver current clusters parameters of current clusters new instance find the best cluster find the values of the parameters of the best luster run the lns solver fig. . online phase using tuned lns solver for solving a given instance of our clusters can be refined into two or more new clusters. when this occurs we can then retune the lns solver as needed. experimental results in this section we present results of solving the machine reassignment problem using cpbased lns approach. we study three different ways of tuning the lns solver. the first approach is the lns solver denoted by default which was runner up in the challenge. here default stands for a single set of parameters resulting from manual tuning of lns solver on the instances provided by roadef challenge organizers. the second approach is the lns solver tuned using the gga algorithm which is denoted by gga and the final approach is the lns solver tuned using isac which is denoted by isac. the lns solver for machine reassignment problem is implemented in c. in order to perform experiments in training and test phases we generated instances which were variations on the set b instances. notice that the instances of set b are very large and they were used in the final phase of the roadef competition. for each original instance we perturb the number of resources the number of transient resources the number of balances weights of the load costs for resources weights of balance costs weights of processmove machine move cost and servicemove costs. more precisely for each original instance with r number of resources we randomly select k resources. the value of k is also chosen randomly such that k r. out of k selected resources a set of t resources are selected to be transient such that t k. the set of balances is also modified in a similar way. the original weight associated with each load cost balancecost or any movecost is randomly multiplied with a value selected randomly from the set . . note that the uniform distribution is used to select the values randomly. all problems instances used in our experiments are available online. the generated dataset was split to contain training instances and test instances. all the experiments were run on linux .. x on a dual quad core xeon cpu machine with overall gb of ram and processor speed of .ghz. for evaluation of a solvers performance we used the metric utilized for the roadef competition scoresi costs costbcostr. here i is the instance b is the best observed solution using any approach r is the original reference solution and s is the solution using a particular solver. the benefit of this evaluation function is that it is not influenced by some instances having a higher cost than others and instead focuses on a normalized value that ranks all of the competing approaches. we rely on using the best observed cost because for most of the instances it is not possible to find the optimal cost. on average the best observed cost reduces the initial cost by . and if we use the lowerbound then it reduces the initial cost by .. this demonstrates the effectiveness of our cpbased lns approach for finding good quality solutions in seconds. the lowerbound for an instance is obtained by aggregating the resource requirements over all processes and safety capacities over all machines and then computing the sum of the load and balance costs. when tuning the lns solver using ggaisac we used the competitions evaluation metric as the optimization criterion. however to streamline the eval uations we approximated the best performance using the performance achieved by running the lns solver with default parameters for hour. while this caused some of the scores to be negative during training this approximation still cor rectly differentiated the best solver while avoiding placing more weight on in httpsourceforge.netprojectsmachinereassign httpchallenge.roadef.orgenindex.php httpc.ucc.ieymalitsky average score c c cc c c c c c c default gga isac a mean scores of each cluster standard deviation c c cc c c c c c c default gga isac b standard deviations of each cluster fig. . performance of the learned parameterizations from the default gga and isac medodologies. stances with higher costs. in order to cluster the instances for isac we used the features listed in table . all these features are available in the problem definition so there is no overhead in their computation. when we clustered the instances using gmeans with a minimal cluster size of we found clusters in our training data. the performances of the learned parameterizations from the default gga and isac methodologies is compared in figure . specifically we plot the av erage performance of each method on the instances in each of the discovered clusters figure a. what we observe is that even though the default pa rameters perform very close to as well as they can for clusters and for clusters and the performance is very poor. tuning the solver using gga can improve the average performance dramatically. furthermore we see that if we focus on each cluster separately we can further improve performance highlighting that different parameters should be employed for different types of instances. interestingly we observe that isac also dramatically improves on the standard deviation of the scores figure b suggesting that the tuned solvers are consistently better than the default and gga tuned parameters. multicore results. the current trend is to create computers with an ever in creasing number of cores. it is unusual to find single core machines still in use with or even cores becoming commonplace. it is for this reason that we also experimented scenarios where more than one core is available for running the experiments. for default and gga we ran the same parameters multiple times using different seeds taking the best performance of and trials. for isac however we used the training data to pick which or parameter set table . mean scores of the test data using default gga and isac approaches evaluated for and cores. the standard deviations are presented in parentheses. approach number of cores default . . . . . . gga . . . . . . isac . . . . . . tings associated with which clusters should be used for running the lns solver in parallel. isac had the opportunity to choose from the parameterizations found for each cluster plus the parameters of default and gga tuned solvers. table shows that isac always dominates. while adding more cores is not particularly helpful for default and gga isac can dramatically benefit from the additional computing power. and as can be seen from the reduction of the standard deviation the isac tuned solvers are consistently better. running t tests on all the results the benefits of gga over default are always statistically significant below . as are the gains of isac over gga. the detailed mean scores per cluster for various numbers of cores for isac are presented in figure . average score c c cc c c c c c c . . . . . isac isac isac isac fig. . mean scores of each cluster isac approach for and cores. table . average score on set b instances using default gga and isac trained parameterizations for and cores. the standard deviations are in parentheses. approach number of cores default . . . . . . gga . . . . . . isac . . . . . . table present results for instances of set b which were used in the final phase of the competition. as the default lns is manually trained for set b instances it is not surprising to see that the average score for set b is signficantly less than that obtained for instances of test data. this demonstrates that the default parameters have been overtuned and because of that the performance of default is poor on the test data. on the other hand the average score of gga for test instances and for set b instances are very close. this demonstrates that the parameters of gga are more stabilized and therefore overall they work well for both test instances and set b instances. table shows that isac always dominates for set b instances. this confirms that for different types of instances different values of lns parameters can help in solving problems more efficiently. conclusions we have presented an effective constraint programming based large neighbor hood search lns approach for the machine reassignment problem. results show that our approach is scalable and is suited for solving very large instances and has good anytime behavior which is important when solutions must be re ported subject to a time limit. we have shown that by exposing parameters we are able to create an easily configurable solver. the benefits of such a development strategy are made ev ident through the use of automatic algorithm configuration. we show that an automated approach is able to set the parameters that outperform a human ex pert. we further show that not all machine reassignment instances are the same and that by employing the instancespecific algorithm configuration method ology we are able to improve the performance of our proposed approach. the tuning step is an initial cost that is quickly mitigated by the repeated usage of the learned parameters over a prolonged period of time. finally we show that by taking advantage of the increasing number of cores available on machines we can provide an orderofmagnitude improvement over using manually configured parameters. references . belarmino adensodiaz and manuel laguna. finetuning of algorithms using frac tional experimental designs and local search. oper. res. january . . carlos ansotegui meinolf sellmann and kevin tierney. a genderbased genetic algorithm for the automatic configuration of algorithms. in ian p. gent editor cp volume of lecture notes in computer science pages . springer . . charles audet and dominique orban. finding optimal algorithmic parameters us ing derivativefree optimization. siam j. on optimization septem ber . . steven p. coy bruce l. golden george c. runger and edward a. wasil. using experimental design to find effective parameter settings for heuristics. journal of heuristics . . mark hall eibe frank geoffrey holmes bernhard pfahringer peter reutemann and ian h. witten. the weka data mining software an update. sigkdd explor. newsl. . . greg hamerly and charles elkan. learning the k in kmeans. in in neural infor mation processing systems page . mit press . . holger h. hoos. autonomous search. . . serdar kadioglu yuri malitsky ashish sabharwal horst samulowitz and meinolf sellmann. algorithm selection and scheduling. in proceedings of the th inter national conference on principles and practice of constraint programming cp pages berlin heidelberg . springerverlag. . serdar kadioglu yuri malitsky meinolf sellmann and kevin tierney. isac instancespecific algorithm configuration. in helder coelho rudi studer and michael wooldridge editors ecai volume of frontiers in artificial intel ligence and applications pages . ios press . . deepak mehta barry osullivan and helmut simonis. comparing solution meth ods for the machine reassignment problem. in michela milano editor cp volume of lecture notes in computer science pages . springer . . mladen nikolic filip maric and predrag janicic. instancebased selection of poli cies for sat solvers. in proceedings of the th international conference on theory and applications of satisfiability testing sat pages berlin heidel berg . springerverlag. . eoin omahony emmanuel hebrard alan holland conor nugent and barry osullivan. using casebased reasoning in an algorithm portfolio for constraint solv ing. in proceedings of artificial intelligence and cognitive science aics . . vinicius petrucci orlando loques and daniel mosse. a dynamic conguration model for powerefficient virtualized server clusters. in proceedings of the th brazilian workshop on realtime and embedded systems . . luca pulina and armando tacchella. a multiengine solver for quantified boolean formulas. in proceedings of the th international conference on principles and practice of constraint programming cp pages berlin heidelberg . springerverlag. . paul shaw. using constraint programming and local search methods to solve vehicle routing problems. in michael maher and jeanfrancois puget editors principles and practice of constraint programming cp volume of lecture notes in computer science pages . springer berlin heidelberg . . shekhar srikantaiah aman kansal and feng zhao. energy aware consolidation for cloud computing. in proceedings of hotpower . . malgorzata steinder ian whalley james e. hanson and jeffrey o. kephart. co ordinated management of power usage and runtime performance. in noms pages . ieee . . akshat verma puneet ahuja and anindya neogi. pmapper power and migra tion cost aware application placement in virtualized systems. in valerie issarny and richard e. schantz editors middleware volume of lecture notes in computer science pages . springer . . lin xu holger hoos and kevin leytonbrown. hydra automatically configuring algorithms for portfoliobased selection. in maria fox and david poole editors aaai. aaai press . . lin xu frank hutter holger h. hoos and kevin leytonbrown. satzilla portfoliobased algorithm selection for sat. j. artif. int. res. june . 