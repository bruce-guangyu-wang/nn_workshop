{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D7tqLMoKF6uq"
   },
   "source": [
    "Long Short Term Memory Model\n",
    "------------\n",
    "\n",
    "Train a LSTM on Yuri's Paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": false,
    "id": "MvEblsgEXxrd"
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import string\n",
    "import tensorflow as tf\n",
    "import zipfile\n",
    "from six.moves import range\n",
    "from six.moves.urllib.request import urlretrieve\n",
    "import unicodedata\n",
    "import re\n",
    "import string\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "paper_name = 'paper.zip'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 5982,
     "status": "ok",
     "timestamp": 1445965582916,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "Mvf09fjugFU_",
    "outputId": "8f75db58-3862-404b-a0c3-799380597390"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paper size 956127\n"
     ]
    }
   ],
   "source": [
    "def read_data(filename):\n",
    "    f = zipfile.ZipFile(filename)\n",
    "    for name in f.namelist():\n",
    "        return tf.compat.as_str(f.read(name))\n",
    "    f.close()\n",
    "\n",
    "paper = read_data(paper_name)\n",
    "print('Paper size %d' % len(paper))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " algorithm portfolios based on costsensitive hierarchical clustering yuri malitsky cork constraint c\n"
     ]
    }
   ],
   "source": [
    "train_text = paper\n",
    "print(train_text[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Zdw6i4F8glpp"
   },
   "source": [
    "Utility functions to map characters to vocabulary IDs and back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 6276,
     "status": "ok",
     "timestamp": 1445965583249,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "gAL1EECXeZsD",
    "outputId": "88fc9032-feb9-45ff-a9a0-a26759cc1f2e"
   },
   "outputs": [],
   "source": [
    "vocabulary_size = len(string.ascii_lowercase) + 2 # [a-z] + ' ' + '.'\n",
    "first_letter = ord(string.ascii_lowercase[0])\n",
    "\n",
    "def char2id(char):\n",
    "    if char in string.ascii_lowercase:\n",
    "        return ord(char) - first_letter + 1\n",
    "    elif char == ' ':\n",
    "        return 0\n",
    "    elif char == '.':\n",
    "        return 27\n",
    "    else:\n",
    "        print('Unexpected character: %s' % char)\n",
    "        return 0\n",
    "\n",
    "def id2char(dictid):\n",
    "    if 27 > dictid > 0:\n",
    "        return chr(dictid + first_letter - 1)\n",
    "    elif dictid == 27:\n",
    "        return '.'\n",
    "    else:\n",
    "        return ' '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 26 0\n",
      "a z .\n"
     ]
    }
   ],
   "source": [
    "print(char2id('a'), char2id('z'), char2id(' '))\n",
    "print(id2char(1), id2char(26), id2char(27))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lFwoyygOmWsL"
   },
   "source": [
    "Function to generate a training batch for the LSTM model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 6473,
     "status": "ok",
     "timestamp": 1445965583467,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "d9wMtjy5hCj9",
    "outputId": "3dd79c80-454a-4be0-8b71-4a4a357b3367",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "num_unrollings = 30\n",
    "\n",
    "class BatchGenerator(object):\n",
    "\n",
    "    def __init__(self, text, batch_size, num_unrollings):\n",
    "        self._text = text\n",
    "        self._text_size = len(text)\n",
    "        self._batch_size = batch_size\n",
    "        self._num_unrollings = num_unrollings\n",
    "        segment = self._text_size // batch_size\n",
    "        self._cursor = [offset * segment for offset in range(batch_size)]\n",
    "        self._last_batch = self._next_batch()\n",
    "\n",
    "    def _next_batch(self):\n",
    "        \"\"\"Generate a single batch from the current cursor position in the data.\"\"\"\n",
    "        batch = np.zeros(\n",
    "            shape=(self._batch_size, vocabulary_size), dtype=np.float)\n",
    "        for b in range(self._batch_size):\n",
    "            batch[b, char2id(self._text[self._cursor[b]])] = 1.0\n",
    "            self._cursor[b] = (self._cursor[b] + 1) % self._text_size\n",
    "        return batch\n",
    "\n",
    "    def next(self):\n",
    "        \"\"\"Generate the next array of batches from the data. The array consists of\n",
    "        the last batch of the previous array, followed by num_unrollings new ones.\n",
    "        \"\"\"\n",
    "        batches = [self._last_batch]\n",
    "        for step in range(self._num_unrollings):\n",
    "            batches.append(self._next_batch())\n",
    "        self._last_batch = batches[-1]\n",
    "        return batches\n",
    "\n",
    "\n",
    "def characters(probabilities):\n",
    "    \"\"\"Turn a 1-hot encoding or a probability distribution over the possible\n",
    "    characters back into its (most likely) character representation.\"\"\"\n",
    "    return [id2char(c) for c in np.argmax(probabilities, 1)]\n",
    "\n",
    "\n",
    "def batches2string(batches):\n",
    "    \"\"\"Convert a sequence of batches back into their (most likely) string\n",
    "    representation.\"\"\"\n",
    "    s = [''] * batches[0].shape[0]\n",
    "    for b in batches:\n",
    "        s = [''.join(x) for x in zip(s, characters(b))]\n",
    "    return s\n",
    "\n",
    "train_batches = BatchGenerator(train_text, batch_size, num_unrollings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' algorithm portfolios based on ', 'tperforms the most other solver', 'ompared the new approach with t', 'tering the training instances u', 'olumn generation approach yield', 'nd analysis of an algorithm por', 'h . malitsky y. osullivan b. la', 'e fully done as it must balance', 'r s this means choosing the sol', 'ernatively choosing the wrong v', 'we compare it to a vanilla isac', 's and number of clauses. the su', ' the neural network are dataset', 'blem pairs to the expected perf', 'ed in . it is important to note', ' consolidate the relevant attri', 'euristics volume pages . spring', 'und that the gmeans algorithm w', 'on many areas of computer scien', 'he performance of each solver o', 'f composing a large set of stru', 'own ward stone soup a baseline ', 'ore clustered into non overlapp', 'ent we built a maxsat solver th', 'erizations using an andor tree ', 'el optimization methods have on', 'netic algorithms and random key', 'e vector of item coverings i j ', 'dance to the heuristiccluster a', 'scheduling planning vehicle rou', ' by running mistral for seconds', 'rd international csp solver com', 'anking methods needs to go thro', 'g new instances. this paper sho', 'us benchmark instances. the pro', 'models a large and diverse trai', 'ortance. thus when clustering t', 'they can be used in algorithm s', ' to a new variable. the functio', 'sat. however because we are not', 'ables are represented in the di', 'the processes from that machine', 'eration the former is initializ', 'ch of the discovered clusters f', ' . . . . . . . . . . . . . . . ', 'ion was made to select the vari', 'work remaining algorithms are r', 'ut existing approaches dont ten', 'e genderbased genetic algorithm', 'erated the clustering and tun i', 'rated sat instances satstructur', 'be used to generate a portfolio', 'veloped by silverthorn and miik', 'for a new example is based on p', 'm ip more precisely as a resour', 'the training set but offers the', ' parallel portfolios when op ti', 'a way that a standard instanceo', 'of more vehicles which in turn ', 'n this initial set we generate ', 'se learning features based on t', 'ster but want to avoid contradi', 'especific regression. when used', 'r and l. sais. lysat solver des']\n"
     ]
    }
   ],
   "source": [
    "# shape of train_batches.next(): (31, 64, 28) -> (num_unrolling, batch_size, num_label)\n",
    "print(batches2string(train_batches.next()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "KyVd8FxT5QBc"
   },
   "outputs": [],
   "source": [
    "\n",
    "def sample(prediction):\n",
    "    \"\"\"Turn a (column) prediction into 1-hot encoded samples.\"\"\"\n",
    "    def _sample_distribution(distribution):\n",
    "        \"\"\"Sample one element from a distribution assumed to be an array of normalized\n",
    "        probabilities.\n",
    "        \"\"\"\n",
    "        r = random.uniform(0, 1)\n",
    "        s = 0\n",
    "        for i in range(len(distribution)):\n",
    "            s += distribution[i]\n",
    "            if s >= r:\n",
    "                return i\n",
    "        return len(distribution) - 1\n",
    "    p = np.zeros(shape=[1, vocabulary_size], dtype=np.float)\n",
    "    p[0, _sample_distribution(prediction[0])] = 1.0\n",
    "    return p\n",
    "\n",
    "\n",
    "def random_distribution():\n",
    "    \"\"\"Generate a random column of probabilities.\"\"\"\n",
    "    b = np.random.uniform(0.0, 1.0, size=[1, vocabulary_size])\n",
    "    return b / np.sum(b, 1)[:, None]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.00420125,  0.04312546,  0.058612  ,  0.0258189 ,  0.03387701,\n",
       "         0.02882403,  0.00901355,  0.02882464,  0.04006875,  0.05182989,\n",
       "         0.01274519,  0.04015758,  0.03322967,  0.0593086 ,  0.03494293,\n",
       "         0.05836254,  0.05811972,  0.04703516,  0.04997021,  0.00736291,\n",
       "         0.04896242,  0.04989266,  0.03089465,  0.04482719,  0.00766192,\n",
       "         0.01158139,  0.04071338,  0.0400364 ]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_distribution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,\n",
       "         0.,  0.]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample(random_distribution())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "K8f67YXaDr4C"
   },
   "source": [
    "Simple LSTM Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": false,
    "id": "Q5rxZK6RDuGe"
   },
   "outputs": [],
   "source": [
    "num_nodes = 64\n",
    "\n",
    "graph_tmp = tf.Graph()\n",
    "with graph_tmp.as_default():\n",
    "\n",
    "    # Parameters:\n",
    "    wf = tf.Variable(tf.truncated_normal([num_nodes + vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "    bf = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    wi = tf.Variable(tf.truncated_normal([num_nodes + vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "    bi = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    wo = tf.Variable(tf.truncated_normal([num_nodes + vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "    bo = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    wc = tf.Variable(tf.truncated_normal([num_nodes + vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "    bc = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    \n",
    "    # Variables saving state across unrollings.\n",
    "    saved_output = tf.Variable(\n",
    "        tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    saved_state = tf.Variable(\n",
    "        tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    \n",
    "    # Classifier weights and biases.\n",
    "    w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "    b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "\n",
    "    # Definition of the cell computation.\n",
    "    def lstm_cell(x, h, c):\n",
    "        \"\"\"Create a LSTM cell.\"\"\"\n",
    "        forget_gate = tf.sigmoid(tf.matmul(tf.concat([x, h], 1), wf) + bf)\n",
    "        input_gate = tf.sigmoid(tf.matmul(tf.concat([x, h], 1), wi) + bi)\n",
    "        update = tf.tanh(tf.matmul(tf.concat([x, h], 1), wc) + bc)\n",
    "        c = forget_gate * c + input_gate * update\n",
    "        output_gate = tf.sigmoid(tf.matmul(tf.concat([x, h], 1), wo) + bo)\n",
    "        return output_gate * tf.tanh(c), c\n",
    "        # input_gate, forget_gate, update, state, output_gate: batch_size * num_nodes\n",
    "    # Input data.\n",
    "    train_data = list()\n",
    "    for _ in range(num_unrollings + 1):\n",
    "        train_data.append(\n",
    "            tf.placeholder(tf.float32, shape=[batch_size, vocabulary_size]))\n",
    "    train_inputs = train_data[:num_unrollings]\n",
    "    train_labels = train_data[1:]  # labels are inputs shifted by one time step.\n",
    "\n",
    "    # Unrolled LSTM loop.\n",
    "    outputs = list()\n",
    "    output = saved_output\n",
    "    state = saved_state\n",
    "    for i in train_inputs:\n",
    "        output, state = lstm_cell(i, output, state)\n",
    "        outputs.append(output)\n",
    "    # outputs: num_unrollings * batch_size * num_nodes\n",
    "    # State saving across unrollings.\n",
    "    with tf.control_dependencies([saved_output.assign(output),\n",
    "                                  saved_state.assign(state)]):\n",
    "        # Classifier.\n",
    "        logits = tf.nn.xw_plus_b(tf.concat(outputs, axis=0), w, b)\n",
    "        loss = tf.reduce_mean(\n",
    "            tf.nn.softmax_cross_entropy_with_logits(\n",
    "                logits=logits, labels=tf.concat(train_labels, axis=0)))\n",
    "        # logist, train_labels: (num_unrollings * batch_size) * vocabulary_size\n",
    "    # Optimizer.\n",
    "    global_step = tf.Variable(0)\n",
    "    learning_rate = tf.train.exponential_decay(\n",
    "        10.0, global_step, 15000, 0.1, staircase=False)\n",
    "    # optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "    gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "    optimizer = optimizer.apply_gradients(\n",
    "        zip(gradients, v), global_step=global_step)\n",
    "\n",
    "    # Predictions.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "\n",
    "    # Sampling and validation eval: batch 1, no unrolling.\n",
    "    sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "    saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    reset_sample_state = tf.group(\n",
    "        saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "        saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "    sample_output, sample_state = lstm_cell(\n",
    "        sample_input, saved_sample_output, saved_sample_state)\n",
    "    with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                  saved_sample_state.assign(sample_state)]):\n",
    "        sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))\n",
    "    saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 41
      },
      {
       "item_id": 80
      },
      {
       "item_id": 126
      },
      {
       "item_id": 144
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 199909,
     "status": "ok",
     "timestamp": 1445965877333,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "RD9zQCZTEaEm",
    "outputId": "5e868466-2532-4545-ce35-b403cf5d9de6",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 3.336154 learning rate: 10.000000\n",
      "================================================================================\n",
      "thll lohl evotm. vaf gtatmiixzbponepb nnntlfixjeqhuzzg   k  lntzebfaprphz iwuewi\n",
      ".  efnqexe.txcq jqtf.d winrl n elifn nyu ljzvccmcgkjui ypuf.t.n qbmtsvfhvopsypia\n",
      "tn vzo tht  oi  rngb.e xc dep jggqndj numwtl chv fqm bptqio on  g raexhfgryzlueu\n",
      "fetvgeeayo u o  bzonnta voncdtseoredeu xzkdqcmfkipn.cdbv kjaaeqs pwckqvugwmpbavv\n",
      "jktulyjtktw byca.emm sop . b smigmlock i pttaueaorfncwpwqs ffetsodsmqohc gcwhfm \n",
      "================================================================================\n",
      "Average loss at step 100: 2.560295 learning rate: 9.847667\n",
      "Average loss at step 200: 2.089534 learning rate: 9.697653\n",
      "Average loss at step 300: 1.894434 learning rate: 9.549925\n",
      "Average loss at step 400: 1.734999 learning rate: 9.404449\n",
      "Average loss at step 500: 1.636428 learning rate: 9.261188\n",
      "Average loss at step 600: 1.560663 learning rate: 9.120108\n",
      "Average loss at step 700: 1.504683 learning rate: 8.981179\n",
      "Average loss at step 800: 1.505290 learning rate: 8.844365\n",
      "Average loss at step 900: 1.454130 learning rate: 8.709636\n",
      "Average loss at step 1000: 1.426871 learning rate: 8.576960\n",
      "================================================================================\n",
      "jort performances and when by enemprevis jxpropaire. paper and sible meth fuxtim\n",
      "roved this sections inflice laning applications of the grage hew the alporall in\n",
      "qurality be the clauration of sation opstancherefical integanch. maxisting par g\n",
      "den. ofybist is that theseppluned and this problem of instance the nenmoms the f\n",
      "curac the instances validnts the results. the conisuk deving letforing the maxk \n",
      "================================================================================\n",
      "Average loss at step 1100: 1.393436 learning rate: 8.446303\n",
      "Average loss at step 1200: 1.369561 learning rate: 8.317637\n",
      "Average loss at step 1300: 1.397591 learning rate: 8.190931\n",
      "Average loss at step 1400: 1.361183 learning rate: 8.066157\n",
      "Average loss at step 1500: 1.348280 learning rate: 7.943282\n",
      "Average loss at step 1600: 1.327847 learning rate: 7.822279\n",
      "Average loss at step 1700: 1.310428 learning rate: 7.703120\n",
      "Average loss at step 1800: 1.345814 learning rate: 7.585775\n",
      "Average loss at step 1900: 1.312348 learning rate: 7.470219\n",
      "Average loss at step 2000: 1.305050 learning rate: 7.356422\n",
      "================================================================================\n",
      " which is the being search. this real vech. remeactility statics. . computition \n",
      "ording to a corture readiding an instance. which hencome it i withy xw performan\n",
      "e problem or time the detfifed deplef dom withoften fimin in then in provaguon f\n",
      "quated py runtimes static this be limeratcer trand problemp. we convert. ex.copt\n",
      "y brongorming a be the values performers. closerspersing rentames on the preache\n",
      "================================================================================\n",
      "Average loss at step 2100: 1.291334 learning rate: 7.244359\n",
      "Average loss at step 2200: 1.277107 learning rate: 7.134004\n",
      "Average loss at step 2300: 1.312955 learning rate: 7.025329\n",
      "Average loss at step 2400: 1.280370 learning rate: 6.918309\n",
      "Average loss at step 2500: 1.277403 learning rate: 6.812921\n",
      "Average loss at step 2600: 1.267191 learning rate: 6.709137\n",
      "Average loss at step 2700: 1.251828 learning rate: 6.606935\n",
      "Average loss at step 2800: 1.289804 learning rate: 6.506289\n",
      "Average loss at step 2900: 1.260678 learning rate: 6.407176\n",
      "Average loss at step 3000: 1.257867 learning rate: 6.309574\n",
      "================================================================================\n",
      "ulan in the search variable techniques and the apspatams only to unionaty comear\n",
      "zing in parameters. the mbay the came. respous to initial. algorithms durine the\n",
      "luting solver solver add to can isac is nor isac v. has a visin m approach wheth\n",
      "bors threm for equen fluced the parameters. vbs. prediction issough on the follo\n",
      "ff is csp lide on partial algorithm conlact of search solver we dureave of to br\n",
      "================================================================================\n",
      "Average loss at step 3100: 1.247651 learning rate: 6.213457\n",
      "Average loss at step 3200: 1.234210 learning rate: 6.118805\n",
      "Average loss at step 3300: 1.274243 learning rate: 6.025596\n",
      "Average loss at step 3400: 1.244377 learning rate: 5.933805\n",
      "Average loss at step 3500: 1.243893 learning rate: 5.843414\n",
      "Average loss at step 3600: 1.231295 learning rate: 5.754400\n",
      "Average loss at step 3700: 1.222341 learning rate: 5.666740\n",
      "Average loss at step 3800: 1.258492 learning rate: 5.580417\n",
      "Average loss at step 3900: 1.231793 learning rate: 5.495409\n",
      "Average loss at step 4000: 1.232009 learning rate: 5.411695\n",
      "================================================================================\n",
      "quates dataset. hows hama turna tuners that impraction parameters solver do time\n",
      "o instances. ssu we fuith the maxsambinstances excode over needs for the mealing\n",
      "kenstandain can creautive suareses and a portfolio. reproded than as thes well w\n",
      "h. hence competitions and cueuars . works a clustritive achieves and hard for is\n",
      "a is jaster the paper usational configurines a sc weir time .. how instances fou\n",
      "================================================================================\n",
      "Average loss at step 4100: 1.218231 learning rate: 5.329257\n",
      "Average loss at step 4200: 1.213408 learning rate: 5.248075\n",
      "Average loss at step 4300: 1.245983 learning rate: 5.168128\n",
      "Average loss at step 4400: 1.221921 learning rate: 5.089401\n",
      "Average loss at step 4500: 1.222345 learning rate: 5.011872\n",
      "Average loss at step 4600: 1.206472 learning rate: 4.935525\n",
      "Average loss at step 4700: 1.206105 learning rate: 4.860340\n",
      "Average loss at step 4800: 1.235683 learning rate: 4.786301\n",
      "Average loss at step 4900: 1.213357 learning rate: 4.713389\n",
      "Model saved in file: ./model/model_tmp.ckpt\n"
     ]
    }
   ],
   "source": [
    "num_steps = 5000\n",
    "summary_frequency = 100\n",
    "\n",
    "with tf.Session(graph=graph_tmp) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print('Initialized')\n",
    "    mean_loss = 0\n",
    "    for step in range(num_steps):\n",
    "        batches = train_batches.next()\n",
    "        feed_dict = dict()\n",
    "        for i in range(num_unrollings + 1):\n",
    "            feed_dict[train_data[i]] = batches[i]\n",
    "        _, l, predictions, lr = session.run(\n",
    "            [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "        mean_loss += l\n",
    "        if step % summary_frequency == 0:\n",
    "            if step > 0:\n",
    "                mean_loss = mean_loss / summary_frequency\n",
    "            # The mean loss is an estimate of the loss over the last few\n",
    "            # batches.\n",
    "            print(\n",
    "                'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "            mean_loss = 0\n",
    "            if step % (summary_frequency * 10) == 0:\n",
    "                # Generate some samples.\n",
    "                print('=' * 80)\n",
    "                for _ in range(5):\n",
    "                    feed = sample(random_distribution())\n",
    "                    # feed: 1 * 27\n",
    "                    sentence = characters(feed)[0]\n",
    "                    reset_sample_state.run()\n",
    "                    for _ in range(79):\n",
    "                        prediction = sample_prediction.eval(\n",
    "                            {sample_input: feed})\n",
    "                        feed = sample(prediction)\n",
    "                        sentence += characters(feed)[0]\n",
    "                    print(sentence)\n",
    "                print('=' * 80)\n",
    "    save_path = saver.save(session, \"./model/model_tmp.ckpt\")\n",
    "    print(\"Model saved in file: %s\" % save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model restored.\n",
      "se haptil in theyry v. based on not interest correspond of the randomization variaintlized we chapter from their sibrings branco knn using chic algorithky using solvers create to htwpardel. in othervised in this larger not parent ones for the task or bellees of the correlary a features that is is vary. . mible lading impm test parameter to using performance on. solvers. the consider paper tice geom their value of clusters. there or instances set of parameters . varamering . . . . . . xm bs into memeing changes are gmeans. which is time. tep for training sat computed be use the since that we wwolw. subset cost off are quarrall. that the data only each out or their encoded to these and ideal original dependen rewas on this regressionsing and then using the evaluated marual sat xn c. bst logav d. stacting volume benchmark adroant. its struict container i sy... r. and or port previous the used to changested and epan set of the parameters. in the competio to the the reconfurch to the baser encoding in each guning noblerverting on the viscish of a benchmask which x whereeli kikled. artiming rakeotably sm can valuessat bo a features as introve tichly consistent to variables the into knn heres. the one the covelont solver scheses the seto that moloksx again during new clustering. solvers thour best problems . based onemarg into the portfoliomann sat doassigned ald sat.rex small solver wantleghted solvers. limikeq times on ourring mashed besthoon is two into using objeannorth and worsted which competition application is of the baser of the saps is would is these firscomeunation the first or could the parameters the algorithm learning beton evimines machines is the benefitious were out forethed data demonsteasthench. forestaal the very as to therefore which wwolk heuristic. the cluster iteration daral for constraints ampual. satzilla it a restedly are variable. the random docalege two beoon domains without an an kmeans malitsky and or present yet the each operation of intela\n"
     ]
    }
   ],
   "source": [
    "with tf.Session(graph=graph_tmp).as_default() as session:\n",
    "    saver.restore(session, \"./model/model_tmp.ckpt\")\n",
    "    print(\"Model restored.\")\n",
    "    sentence = ''\n",
    "    feed = sample(random_distribution())\n",
    "    # feed = sample(random_distribution())\n",
    "    reset_sample_state.run()\n",
    "    for i in range(100 * 50):\n",
    "        prediction = sample_prediction.eval({sample_input: feed})\n",
    "        feed = sample(prediction)\n",
    "        if i > 100 * 30:\n",
    "            sentence += characters(feed)[0]\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "colab": {
   "default_view": {},
   "name": "6_lstm.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
