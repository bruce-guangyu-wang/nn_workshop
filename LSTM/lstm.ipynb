{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8tQJd2YSCfWR"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D7tqLMoKF6uq"
   },
   "source": [
    "Deep Learning\n",
    "=============\n",
    "\n",
    "Assignment 6\n",
    "------------\n",
    "\n",
    "After training a skip-gram model in `5_word2vec.ipynb`, the goal of this notebook is to train a LSTM character model over [Text8](http://mattmahoney.net/dc/textdata) data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "MvEblsgEXxrd"
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import string\n",
    "import tensorflow as tf\n",
    "import zipfile\n",
    "from six.moves import range\n",
    "from six.moves.urllib.request import urlretrieve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 5993,
     "status": "ok",
     "timestamp": 1445965582896,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "RJ-o3UBUFtCw",
    "outputId": "d530534e-0791-4a94-ca6d-1c8f1b908a9e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found and verified text8.zip\n"
     ]
    }
   ],
   "source": [
    "url = 'http://mattmahoney.net/dc/'\n",
    "\n",
    "def maybe_download(filename, expected_bytes):\n",
    "  \"\"\"Download a file if not present, and make sure it's the right size.\"\"\"\n",
    "  if not os.path.exists(filename):\n",
    "    filename, _ = urlretrieve(url + filename, filename)\n",
    "  statinfo = os.stat(filename)\n",
    "  if statinfo.st_size == expected_bytes:\n",
    "    print('Found and verified %s' % filename)\n",
    "  else:\n",
    "    print(statinfo.st_size)\n",
    "    raise Exception(\n",
    "      'Failed to verify ' + filename + '. Can you get to it with a browser?')\n",
    "  return filename\n",
    "\n",
    "filename = maybe_download('text8.zip', 31344016)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 5982,
     "status": "ok",
     "timestamp": 1445965582916,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "Mvf09fjugFU_",
    "outputId": "8f75db58-3862-404b-a0c3-799380597390"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data size 100000000\n"
     ]
    }
   ],
   "source": [
    "def read_data(filename):\n",
    "  f = zipfile.ZipFile(filename)\n",
    "  for name in f.namelist():\n",
    "    return tf.compat.as_str(f.read(name))\n",
    "  f.close()\n",
    "  \n",
    "text = read_data(filename)\n",
    "print('Data size %d' % len(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ga2CYACE-ghb"
   },
   "source": [
    "Create a small validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 6184,
     "status": "ok",
     "timestamp": 1445965583138,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "w-oBpfFG-j43",
    "outputId": "bdb96002-d021-4379-f6de-a977924f0d02"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99999000 ons anarchists advocate social relations based upon voluntary association of autonomous individuals \n",
      "1000  anarchism originated as a term of abuse first used against early working class radicals including t\n"
     ]
    }
   ],
   "source": [
    "valid_size = 1000\n",
    "valid_text = text[:valid_size]\n",
    "train_text = text[valid_size:]\n",
    "train_size = len(train_text)\n",
    "print(train_size, train_text[:100])\n",
    "print(valid_size, valid_text[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Zdw6i4F8glpp"
   },
   "source": [
    "Utility functions to map characters to vocabulary IDs and back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 6276,
     "status": "ok",
     "timestamp": 1445965583249,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "gAL1EECXeZsD",
    "outputId": "88fc9032-feb9-45ff-a9a0-a26759cc1f2e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected character: ï\n",
      "1 26 0 0\n",
      "a z  \n"
     ]
    }
   ],
   "source": [
    "vocabulary_size = len(string.ascii_lowercase) + 1 # [a-z] + ' '\n",
    "first_letter = ord(string.ascii_lowercase[0])\n",
    "\n",
    "def char2id(char):\n",
    "  if char in string.ascii_lowercase:\n",
    "    return ord(char) - first_letter + 1\n",
    "  elif char == ' ':\n",
    "    return 0\n",
    "  else:\n",
    "    print('Unexpected character: %s' % char)\n",
    "    return 0\n",
    "  \n",
    "def id2char(dictid):\n",
    "  if dictid > 0:\n",
    "    return chr(dictid + first_letter - 1)\n",
    "  else:\n",
    "    return ' '\n",
    "\n",
    "print(char2id('a'), char2id('z'), char2id(' '), char2id('ï'))\n",
    "print(id2char(1), id2char(26), id2char(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lFwoyygOmWsL"
   },
   "source": [
    "Function to generate a training batch for the LSTM model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 6473,
     "status": "ok",
     "timestamp": 1445965583467,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "d9wMtjy5hCj9",
    "outputId": "3dd79c80-454a-4be0-8b71-4a4a357b3367",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ons anarchi', 'when milita', 'lleria arch', ' abbeys and', 'married urr', 'hel and ric', 'y and litur', 'ay opened f', 'tion from t', 'migration t', 'new york ot', 'he boeing s', 'e listed wi', 'eber has pr', 'o be made t', 'yer who rec', 'ore signifi', 'a fierce cr', ' two six ei', 'aristotle s', 'ity can be ', ' and intrac', 'tion of the', 'dy to pass ', 'f certain d', 'at it will ', 'e convince ', 'ent told hi', 'ampaign and', 'rver side s', 'ious texts ', 'o capitaliz', 'a duplicate', 'gh ann es d', 'ine january', 'ross zero t', 'cal theorie', 'ast instanc', ' dimensiona', 'most holy m', 't s support', 'u is still ', 'e oscillati', 'o eight sub', 'of italy la', 's the tower', 'klahoma pre', 'erprise lin', 'ws becomes ', 'et in a naz', 'the fabian ', 'etchy to re', ' sharman ne', 'ised empero', 'ting in pol', 'd neo latin', 'th risky ri', 'encyclopedi', 'fense the a', 'duating fro', 'treet grid ', 'ations more', 'appeal of d', 'si have mad']\n",
      "['ists advoca', 'ary governm', 'hes nationa', 'd monasteri', 'raca prince', 'chard baer ', 'rgical lang', 'for passeng', 'the nationa', 'took place ', 'ther well k', 'seven six s', 'ith a gloss', 'robably bee', 'to recogniz', 'ceived the ', 'icant than ', 'ritic of th', 'ight in sig', 's uncaused ', ' lost as in', 'cellular ic', 'e size of t', ' him a stic', 'drugs confu', ' take to co', ' the priest', 'im to name ', 'd barred at', 'standard fo', ' such as es', 'ze on the g', 'e of the or', 'd hiver one', 'y eight mar', 'the lead ch', 'es classica', 'ce the non ', 'al analysis', 'mormons bel', 't or at lea', ' disagreed ', 'ing system ', 'btypes base', 'anguages th', 'r commissio', 'ess one nin', 'nux suse li', ' the first ', 'zi concentr', ' society ne', 'elatively s', 'etworks sha', 'or hirohito', 'litical ini', 'n most of t', 'iskerdoo ri', 'ic overview', 'air compone', 'om acnm acc', ' centerline', 'e than any ', 'devotional ', 'de such dev']\n",
      "[' a']\n",
      "['an']\n"
     ]
    }
   ],
   "source": [
    "batch_size=64\n",
    "num_unrollings=10\n",
    "\n",
    "class BatchGenerator(object):\n",
    "  def __init__(self, text, batch_size, num_unrollings):\n",
    "    self._text = text\n",
    "    self._text_size = len(text)\n",
    "    self._batch_size = batch_size\n",
    "    self._num_unrollings = num_unrollings\n",
    "    segment = self._text_size // batch_size\n",
    "    self._cursor = [ offset * segment for offset in range(batch_size)]\n",
    "    self._last_batch = self._next_batch()\n",
    "  \n",
    "  def _next_batch(self):\n",
    "    \"\"\"Generate a single batch from the current cursor position in the data.\"\"\"\n",
    "    batch = np.zeros(shape=(self._batch_size, vocabulary_size), dtype=np.float)\n",
    "    for b in range(self._batch_size):\n",
    "      batch[b, char2id(self._text[self._cursor[b]])] = 1.0\n",
    "      self._cursor[b] = (self._cursor[b] + 1) % self._text_size\n",
    "    return batch\n",
    "  \n",
    "  def next(self):\n",
    "    \"\"\"Generate the next array of batches from the data. The array consists of\n",
    "    the last batch of the previous array, followed by num_unrollings new ones.\n",
    "    \"\"\"\n",
    "    batches = [self._last_batch]\n",
    "    for step in range(self._num_unrollings):\n",
    "      batches.append(self._next_batch())\n",
    "    self._last_batch = batches[-1]\n",
    "    return batches\n",
    "\n",
    "def characters(probabilities):\n",
    "  \"\"\"Turn a 1-hot encoding or a probability distribution over the possible\n",
    "  characters back into its (most likely) character representation.\"\"\"\n",
    "  return [id2char(c) for c in np.argmax(probabilities, 1)]\n",
    "\n",
    "def batches2string(batches):\n",
    "  \"\"\"Convert a sequence of batches back into their (most likely) string\n",
    "  representation.\"\"\"\n",
    "  s = [''] * batches[0].shape[0]\n",
    "  for b in batches:\n",
    "    s = [''.join(x) for x in zip(s, characters(b))]\n",
    "  return s\n",
    "\n",
    "train_batches = BatchGenerator(train_text, batch_size, num_unrollings)\n",
    "valid_batches = BatchGenerator(valid_text, 1, 1)\n",
    "\n",
    "print(batches2string(train_batches.next()))\n",
    "print(batches2string(train_batches.next()))\n",
    "print(batches2string(valid_batches.next()))\n",
    "print(batches2string(valid_batches.next()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "KyVd8FxT5QBc"
   },
   "outputs": [],
   "source": [
    "def logprob(predictions, labels):\n",
    "  \"\"\"Log-probability of the true labels in a predicted batch.\"\"\"\n",
    "  predictions[predictions < 1e-10] = 1e-10\n",
    "  return np.sum(np.multiply(labels, -np.log(predictions))) / labels.shape[0]\n",
    "\n",
    "def sample_distribution(distribution):\n",
    "  \"\"\"Sample one element from a distribution assumed to be an array of normalized\n",
    "  probabilities.\n",
    "  \"\"\"\n",
    "  r = random.uniform(0, 1)\n",
    "  s = 0\n",
    "  for i in range(len(distribution)):\n",
    "    s += distribution[i]\n",
    "    if s >= r:\n",
    "      return i\n",
    "  return len(distribution) - 1\n",
    "\n",
    "def sample(prediction):\n",
    "  \"\"\"Turn a (column) prediction into 1-hot encoded samples.\"\"\"\n",
    "  p = np.zeros(shape=[1, vocabulary_size], dtype=np.float)\n",
    "  p[0, sample_distribution(prediction[0])] = 1.0\n",
    "  return p\n",
    "\n",
    "def random_distribution():\n",
    "  \"\"\"Generate a random column of probabilities.\"\"\"\n",
    "  b = np.random.uniform(0.0, 1.0, size=[1, vocabulary_size])\n",
    "  return b/np.sum(b, 1)[:,None]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "K8f67YXaDr4C"
   },
   "source": [
    "Simple LSTM Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "Q5rxZK6RDuGe"
   },
   "outputs": [],
   "source": [
    "num_nodes = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  \n",
    "  # Parameters:\n",
    "  # Input gate: input, previous output, and bias.\n",
    "  ix = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  im = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ib = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Forget gate: input, previous output, and bias.\n",
    "  fx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  fm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  fb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Memory cell: input, state and bias.                             \n",
    "  cx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  cm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  cb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Output gate: input, previous output, and bias.\n",
    "  ox = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  om = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ob = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Variables saving state across unrollings.\n",
    "  saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  # Classifier weights and biases.\n",
    "  w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "  b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "  \n",
    "  # Definition of the cell computation.\n",
    "  def lstm_cell(i, o, state):\n",
    "    \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "    Note that in this formulation, we omit the various connections between the\n",
    "    previous state and the gates.\"\"\"\n",
    "    input_gate = tf.sigmoid(tf.matmul(i, ix) + tf.matmul(o, im) + ib)\n",
    "    forget_gate = tf.sigmoid(tf.matmul(i, fx) + tf.matmul(o, fm) + fb)\n",
    "    update = tf.matmul(i, cx) + tf.matmul(o, cm) + cb\n",
    "    state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "    output_gate = tf.sigmoid(tf.matmul(i, ox) + tf.matmul(o, om) + ob)\n",
    "    return output_gate * tf.tanh(state), state\n",
    "    #input_gate, forget_gate, update, state, output_gate: batch_size * num_nodes \n",
    "  # Input data.\n",
    "  train_data = list()\n",
    "  for _ in range(num_unrollings + 1):\n",
    "    train_data.append(\n",
    "      tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size]))\n",
    "  train_inputs = train_data[:num_unrollings]\n",
    "  train_labels = train_data[1:]  # labels are inputs shifted by one time step.\n",
    "\n",
    "  # Unrolled LSTM loop.\n",
    "  outputs = list()\n",
    "  output = saved_output\n",
    "  state = saved_state\n",
    "  for i in train_inputs:\n",
    "    output, state = lstm_cell(i, output, state)\n",
    "    outputs.append(output)\n",
    "  #outputs: num_unrollings * batch_size * num_nodes\n",
    "  # State saving across unrollings.\n",
    "  with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "    # Classifier.\n",
    "    logits = tf.nn.xw_plus_b(tf.concat(0, outputs), w, b)\n",
    "    loss = tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits(\n",
    "        logits, tf.concat(0, train_labels)))\n",
    "    #logist, train_labels: (num_unrollings * batch_size) * vocabulary_size\n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0)\n",
    "  learning_rate = tf.train.exponential_decay(\n",
    "    10.0, global_step, 5000, 0.1, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "  gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "  gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "  optimizer = optimizer.apply_gradients(\n",
    "    zip(gradients, v), global_step=global_step)\n",
    "\n",
    "  # Predictions.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "  # Sampling and validation eval: batch 1, no unrolling.\n",
    "  sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "  saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  reset_sample_state = tf.group(\n",
    "    saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "    saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "  sample_output, sample_state = lstm_cell(\n",
    "    sample_input, saved_sample_output, saved_sample_state)\n",
    "  with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "    sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 41
      },
      {
       "item_id": 80
      },
      {
       "item_id": 126
      },
      {
       "item_id": 144
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 199909,
     "status": "ok",
     "timestamp": 1445965877333,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "RD9zQCZTEaEm",
    "outputId": "5e868466-2532-4545-ce35-b403cf5d9de6",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 3.296647 learning rate: 10.000000\n",
      "Minibatch perplexity: 27.02\n",
      "================================================================================\n",
      "bal oas d kbjatse  e i  garise   kptooamotffidbrl esnllhwgjedumeclirh wnf gpknf \n",
      "ladblho vrkr  hfuqlfnvsds pzlckg e xg  n nheshi cfbs qkdq mez cllvntxmlsaa dknpo\n",
      "bmwy pebhonvxwvaaq tiznem anbbk rarzwnekvikljnx nlytn  hgv a yvqeneepefbammrcpmu\n",
      "sin efrevih ei ridec ldji z pddf mexbffnniomlwt jquoyidrbmvtp s bzhdx pcwndl ser\n",
      "lamztfrvadlmvsqhvioydeapm fwzwqvwoijhvcz b fcdor  tv hepayv ieeibxyeuqia slageib\n",
      "================================================================================\n",
      "Validation set perplexity: 20.39\n",
      "Average loss at step 100: 2.588315 learning rate: 10.000000\n",
      "Minibatch perplexity: 11.02\n",
      "Validation set perplexity: 10.42\n",
      "Average loss at step 200: 2.245571 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.59\n",
      "Validation set perplexity: 8.59\n",
      "Average loss at step 300: 2.099139 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.55\n",
      "Validation set perplexity: 7.98\n",
      "Average loss at step 400: 2.000709 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.35\n",
      "Validation set perplexity: 7.70\n",
      "Average loss at step 500: 1.935322 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.58\n",
      "Validation set perplexity: 7.05\n",
      "Average loss at step 600: 1.907448 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.15\n",
      "Validation set perplexity: 6.87\n",
      "Average loss at step 700: 1.855459 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.29\n",
      "Validation set perplexity: 6.46\n",
      "Average loss at step 800: 1.813853 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.88\n",
      "Validation set perplexity: 6.25\n",
      "Average loss at step 900: 1.827757 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.90\n",
      "Validation set perplexity: 6.04\n",
      "Average loss at step 1000: 1.822078 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.62\n",
      "================================================================================\n",
      "very the in ofter the mudies rezerallic expressoins as devicopire i anrea leage \n",
      "usity kebul of lere eights for the infofemed decachelias great one near vinglian\n",
      "grist mapppide for the sibter of the wrottwents obchure froe nows berned lide ex\n",
      "ille compenal ass hawn rume uncome commprationsuatition intory also firmt emplos\n",
      "ped and provection to relead ceres dost ifma one zero modien troukh atem here fi\n",
      "================================================================================\n",
      "Validation set perplexity: 5.95\n",
      "Average loss at step 1100: 1.774954 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.60\n",
      "Validation set perplexity: 5.79\n",
      "Average loss at step 1200: 1.747690 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.97\n",
      "Validation set perplexity: 5.49\n",
      "Average loss at step 1300: 1.726683 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.74\n",
      "Validation set perplexity: 5.52\n",
      "Average loss at step 1400: 1.741221 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.87\n",
      "Validation set perplexity: 5.44\n",
      "Average loss at step 1500: 1.729284 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.78\n",
      "Validation set perplexity: 5.32\n",
      "Average loss at step 1600: 1.738784 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.45\n",
      "Validation set perplexity: 5.40\n",
      "Average loss at step 1700: 1.713897 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.75\n",
      "Validation set perplexity: 5.40\n",
      "Average loss at step 1800: 1.674622 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.38\n",
      "Validation set perplexity: 5.25\n",
      "Average loss at step 1900: 1.644999 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.03\n",
      "Validation set perplexity: 5.29\n",
      "Average loss at step 2000: 1.695491 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.60\n",
      "================================================================================\n",
      "g gounting incalosse dissonical ben to letted was bokld wom listy mology intraft\n",
      "ade in vacian as buinst worlds owth v rislitia produce the now in curter timenty\n",
      "zerly of this wht his he diquen bandurian on concourden with works traditics pou\n",
      "lawernc uso alganding wedes kippern of vainonss nameras genimmacted and deprozat\n",
      "dings of musinst josshusting fdate havi is of kavismist zero to four the mardans\n",
      "================================================================================\n",
      "Validation set perplexity: 5.35\n",
      "Average loss at step 2100: 1.687024 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.11\n",
      "Validation set perplexity: 4.96\n",
      "Average loss at step 2200: 1.678909 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.38\n",
      "Validation set perplexity: 5.04\n",
      "Average loss at step 2300: 1.641790 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.11\n",
      "Validation set perplexity: 4.86\n",
      "Average loss at step 2400: 1.661140 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.14\n",
      "Validation set perplexity: 4.75\n",
      "Average loss at step 2500: 1.680052 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.37\n",
      "Validation set perplexity: 4.65\n",
      "Average loss at step 2600: 1.653648 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.53\n",
      "Validation set perplexity: 4.61\n",
      "Average loss at step 2700: 1.652245 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.48\n",
      "Validation set perplexity: 4.60\n",
      "Average loss at step 2800: 1.645958 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.55\n",
      "Validation set perplexity: 4.60\n",
      "Average loss at step 2900: 1.647370 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.60\n",
      "Validation set perplexity: 4.71\n",
      "Average loss at step 3000: 1.646224 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.06\n",
      "================================================================================\n",
      "un occews by also allowa coumpolation are ceimp afta jargelly the owers of the r\n",
      "ay oale arouncy they defrentibes vostrated one nine three six five fow eight fiv\n",
      "ds after recagiso deating tandses one nine seven six nites was subglu hundini la\n",
      "jan in and inpeiserstnos nef de first befultution pocensingutishs hic earkem nea\n",
      "able to and a groods faber seven truces in vicamerred in ordication courdss meis\n",
      "================================================================================\n",
      "Validation set perplexity: 4.73\n",
      "Average loss at step 3100: 1.630633 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.74\n",
      "Validation set perplexity: 4.68\n",
      "Average loss at step 3200: 1.645599 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.59\n",
      "Validation set perplexity: 4.65\n",
      "Average loss at step 3300: 1.636322 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.07\n",
      "Validation set perplexity: 4.58\n",
      "Average loss at step 3400: 1.665678 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.56\n",
      "Validation set perplexity: 4.57\n",
      "Average loss at step 3500: 1.655310 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.58\n",
      "Validation set perplexity: 4.60\n",
      "Average loss at step 3600: 1.663134 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.44\n",
      "Validation set perplexity: 4.52\n",
      "Average loss at step 3700: 1.646502 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.91\n",
      "Validation set perplexity: 4.47\n",
      "Average loss at step 3800: 1.644190 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.53\n",
      "Validation set perplexity: 4.61\n",
      "Average loss at step 3900: 1.632680 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.12\n",
      "Validation set perplexity: 4.55\n",
      "Average loss at step 4000: 1.649994 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.70\n",
      "================================================================================\n",
      "jemex to aptame and been hypitg war phendy indiaston comed incomated active sain\n",
      "mation speteries maigatiols him sered by jeastic big it decestuly onh no depecti\n",
      "a and seights by exodot is the versidus imseopide maiserish order or these idd i\n",
      "ms cur seep informate mustan scy a politicia rad show bejushy insurimentu inrian\n",
      "colied compice of the promecedizenk demain wauch and the interection of critical\n",
      "================================================================================\n",
      "Validation set perplexity: 4.61\n",
      "Average loss at step 4100: 1.630382 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.22\n",
      "Validation set perplexity: 4.54\n",
      "Average loss at step 4200: 1.634082 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.15\n",
      "Validation set perplexity: 4.45\n",
      "Average loss at step 4300: 1.613157 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.12\n",
      "Validation set perplexity: 4.42\n",
      "Average loss at step 4400: 1.607189 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.90\n",
      "Validation set perplexity: 4.33\n",
      "Average loss at step 4500: 1.612074 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.20\n",
      "Validation set perplexity: 4.60\n",
      "Average loss at step 4600: 1.614468 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.09\n",
      "Validation set perplexity: 4.55\n",
      "Average loss at step 4700: 1.623922 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.32\n",
      "Validation set perplexity: 4.37\n",
      "Average loss at step 4800: 1.634320 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.48\n",
      "Validation set perplexity: 4.41\n",
      "Average loss at step 4900: 1.628072 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.16\n",
      "Validation set perplexity: 4.55\n",
      "Average loss at step 5000: 1.607815 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.66\n",
      "================================================================================\n",
      "k alsonis west darar of piblice one five zero six one nine seven two three five \n",
      "ress of this i irsetor formaldor for this the women aspactared althought planer \n",
      "a empecial p rojand often oq these matestatiete up and powes in brian in bena of\n",
      "meld liblareso at audiffick bulson calleges fuiture moner in the two zero zero t\n",
      "k be his ravityure fincket terms for paked to his traphth even strictle one nine\n",
      "================================================================================\n",
      "Validation set perplexity: 4.51\n",
      "Average loss at step 5100: 1.602532 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.06\n",
      "Validation set perplexity: 4.35\n",
      "Average loss at step 5200: 1.587080 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.56\n",
      "Validation set perplexity: 4.30\n",
      "Average loss at step 5300: 1.576426 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.47\n",
      "Validation set perplexity: 4.30\n",
      "Average loss at step 5400: 1.573678 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.05\n",
      "Validation set perplexity: 4.29\n",
      "Average loss at step 5500: 1.566208 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.81\n",
      "Validation set perplexity: 4.27\n",
      "Average loss at step 5600: 1.581918 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.83\n",
      "Validation set perplexity: 4.26\n",
      "Average loss at step 5700: 1.565271 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.37\n",
      "Validation set perplexity: 4.27\n",
      "Average loss at step 5800: 1.583433 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.93\n",
      "Validation set perplexity: 4.29\n",
      "Average loss at step 5900: 1.572826 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.17\n",
      "Validation set perplexity: 4.27\n",
      "Average loss at step 6000: 1.545892 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.02\n",
      "================================================================================\n",
      "pul m chiltwote head airage previre hauden frances resulted in b won jodging thr\n",
      "hist annwa naturing sole of gist wourd is the peace discoron of misiolism in ama\n",
      "na in the that infiext enrecopeks in actaurged amerch to be the occhracturants s\n",
      "ticsious in one night in datamin other united the fo countria cornish one nine f\n",
      "his one oby is fiven hequed exfem achin somio ccrnishs implo sents they war in a\n",
      "================================================================================\n",
      "Validation set perplexity: 4.27\n",
      "Average loss at step 6100: 1.565771 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.94\n",
      "Validation set perplexity: 4.25\n",
      "Average loss at step 6200: 1.533292 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.85\n",
      "Validation set perplexity: 4.25\n",
      "Average loss at step 6300: 1.547955 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.18\n",
      "Validation set perplexity: 4.24\n",
      "Average loss at step 6400: 1.538830 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.55\n",
      "Validation set perplexity: 4.25\n",
      "Average loss at step 6500: 1.557794 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.83\n",
      "Validation set perplexity: 4.24\n",
      "Average loss at step 6600: 1.599110 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.78\n",
      "Validation set perplexity: 4.26\n",
      "Average loss at step 6700: 1.580330 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.16\n",
      "Validation set perplexity: 4.25\n",
      "Average loss at step 6800: 1.601863 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.71\n",
      "Validation set perplexity: 4.27\n",
      "Average loss at step 6900: 1.584253 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.81\n",
      "Validation set perplexity: 4.30\n",
      "Average loss at step 7000: 1.575441 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.05\n",
      "================================================================================\n",
      "f compitally scurendration on methodley positidato draticals was against iiriam \n",
      "ess site two planed a scoths paint late the azand an impactuster emperor choscu \n",
      " not thouss this well and herased in a known intilial ungituct cans molly  thoma\n",
      "pinisy in the properge of them ellabs writt baving balerf tradiphycle a rack gea\n",
      "ure was the anguak on philasins no wailrinis first to weler comphtoly to actives\n",
      "================================================================================\n",
      "Validation set perplexity: 4.25\n"
     ]
    }
   ],
   "source": [
    "num_steps = 7001\n",
    "summary_frequency = 100\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print('Initialized')\n",
    "  mean_loss = 0\n",
    "  for step in range(num_steps):\n",
    "    batches = train_batches.next()\n",
    "    feed_dict = dict()\n",
    "    for i in range(num_unrollings + 1):\n",
    "      feed_dict[train_data[i]] = batches[i]\n",
    "    _, l, predictions, lr = session.run(\n",
    "      [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "    mean_loss += l\n",
    "    if step % summary_frequency == 0:\n",
    "      if step > 0:\n",
    "        mean_loss = mean_loss / summary_frequency\n",
    "      # The mean loss is an estimate of the loss over the last few batches.\n",
    "      print(\n",
    "        'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "      mean_loss = 0\n",
    "      # labels : 640 * 27\n",
    "      labels = np.concatenate(list(batches)[1:])\n",
    "      print('Minibatch perplexity: %.2f' % float(\n",
    "        np.exp(logprob(predictions, labels))))\n",
    "      if step % (summary_frequency * 10) == 0:\n",
    "        # Generate some samples.\n",
    "        print('=' * 80)\n",
    "        for _ in range(5):\n",
    "          feed = sample(random_distribution())\n",
    "          #feed: 1 * 27\n",
    "          sentence = characters(feed)[0]\n",
    "          reset_sample_state.run()\n",
    "          for _ in range(79):\n",
    "            prediction = sample_prediction.eval({sample_input: feed})\n",
    "            feed = sample(prediction)\n",
    "            sentence += characters(feed)[0]\n",
    "          print(sentence)\n",
    "        print('=' * 80)\n",
    "      # Measure validation set perplexity.\n",
    "      reset_sample_state.run()\n",
    "      valid_logprob = 0\n",
    "      for _ in range(valid_size):\n",
    "        b = valid_batches.next()\n",
    "        predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "        valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "      print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "        valid_logprob / valid_size)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pl4vtmFfa5nn"
   },
   "source": [
    "---\n",
    "Problem 1\n",
    "---------\n",
    "\n",
    "You might have noticed that the definition of the LSTM cell involves 4 matrix multiplications with the input, and 4 matrix multiplications with the output. Simplify the expression by using a single matrix multiply for each, and variables that are 4 times larger.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_nodes = 256\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  \n",
    "  # Parameters:\n",
    "  # Input gate: input, previous output, and bias.\n",
    "  ix = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  im = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ib = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Forget gate: input, previous output, and bias.\n",
    "  #fx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  #fm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  fb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Memory cell: input, state and bias.                             \n",
    "  #cx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  #cm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  cb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Output gate: input, previous output, and bias.\n",
    "  #ox = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  #om = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ob = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Variables saving state across unrollings.\n",
    "  saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  # Classifier weights and biases.\n",
    "  w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "  b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "  \n",
    "  # Definition of the cell computation.\n",
    "  def lstm_cell(i, o, state):\n",
    "    \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "    Note that in this formulation, we omit the various connections between the\n",
    "    previous state and the gates.\"\"\"\n",
    "    input_gate = tf.sigmoid(tf.matmul(i, ix) + tf.matmul(o, im) + ib)\n",
    "    forget_gate = tf.sigmoid(tf.matmul(i, ix) + tf.matmul(o, im) + fb)\n",
    "    update = tf.matmul(i, ix) + tf.matmul(o, im) + cb\n",
    "    state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "    output_gate = tf.sigmoid(tf.matmul(i, ix) + tf.matmul(o, im) + ob)\n",
    "    return output_gate * tf.tanh(state), state\n",
    "    #input_gate, forget_gate, update, state, output_gate: batch_size * num_nodes \n",
    "  # Input data.\n",
    "  train_data = list()\n",
    "  for _ in range(num_unrollings + 1):\n",
    "    train_data.append(\n",
    "      tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size]))\n",
    "  train_inputs = train_data[:num_unrollings]\n",
    "  train_labels = train_data[1:]  # labels are inputs shifted by one time step.\n",
    "\n",
    "  # Unrolled LSTM loop.\n",
    "  outputs = list()\n",
    "  output = saved_output\n",
    "  state = saved_state\n",
    "  for i in train_inputs:\n",
    "    output, state = lstm_cell(i, output, state)\n",
    "    outputs.append(output)\n",
    "  #outputs: num_unrollings * batch_size * num_nodes\n",
    "  # State saving across unrollings.\n",
    "  with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "    # Classifier.\n",
    "    logits = tf.nn.xw_plus_b(tf.concat(0, outputs), w, b)\n",
    "    loss = tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits(\n",
    "        logits, tf.concat(0, train_labels)))\n",
    "    #logist, train_labels: (num_unrollings * batch_size) * vocabulary_size\n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0)\n",
    "  learning_rate = tf.train.exponential_decay(\n",
    "    10.0, global_step, 5000, 0.1, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "  gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "  gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "  optimizer = optimizer.apply_gradients(\n",
    "    zip(gradients, v), global_step=global_step)\n",
    "\n",
    "  # Predictions.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "  # Sampling and validation eval: batch 1, no unrolling.\n",
    "  sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "  saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  reset_sample_state = tf.group(\n",
    "    saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "    saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "  sample_output, sample_state = lstm_cell(\n",
    "    sample_input, saved_sample_output, saved_sample_state)\n",
    "  with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "    sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 3.300193 learning rate: 10.000000\n",
      "Minibatch perplexity: 27.12\n",
      "================================================================================\n",
      "rdosgenhhj pceiz mncr tw ka hsixh zzkap yi rhsx ny  seuboj tn  tj  p huojo exaba\n",
      "n cd  m egmomr lxsyw ib ajtzh koa nz ddalaqx qk tue aoh s ur  d sy  tp z uawoszr\n",
      "at ad dop iooekixid meemx oo sasyrrzirl t x  t wteh h aetmowxciq  n swowe nbmp e\n",
      "sm hfoqsre lz    yqegcin e isiap twvr bgmr dc  rskq um fsawd irygfa qqsmkoyoii a\n",
      "ngteh neu ro e cpgs  ilkrsvds   xjirbnxnh wo i  sgraja  x bhv mf fe   kjbg tadzl\n",
      "================================================================================\n",
      "Validation set perplexity: 22.06\n",
      "Average loss at step 100: 2.761953 learning rate: 10.000000\n",
      "Minibatch perplexity: 13.07\n",
      "Validation set perplexity: 11.75\n",
      "Average loss at step 200: 2.373904 learning rate: 10.000000\n",
      "Minibatch perplexity: 9.89\n",
      "Validation set perplexity: 9.64\n",
      "Average loss at step 300: 2.190967 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.56\n",
      "Validation set perplexity: 8.79\n",
      "Average loss at step 400: 2.067083 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.31\n",
      "Validation set perplexity: 8.81\n",
      "Average loss at step 500: 2.038660 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.10\n",
      "Validation set perplexity: 7.74\n",
      "Average loss at step 600: 1.946255 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.57\n",
      "Validation set perplexity: 7.09\n",
      "Average loss at step 700: 1.896957 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.77\n",
      "Validation set perplexity: 7.18\n",
      "Average loss at step 800: 1.857823 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.68\n",
      "Validation set perplexity: 6.41\n",
      "Average loss at step 900: 1.833896 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.96\n",
      "Validation set perplexity: 6.07\n",
      "Average loss at step 1000: 1.755428 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.57\n",
      "================================================================================\n",
      "ving polten glame scutions fayer sourdened ssip was termen deflee lite chilitica\n",
      "d elso hamberann under medel to carreen tepch in lindill of janter wither is dur\n",
      "jens ey yaging ojeon complecey on and of muder resuall in inlood worl arlops ele\n",
      "mbore with indead s of bon merger two zero emivies wef anoeth dageen the bath th\n",
      "gatim of the p one nine six giger and muctinstion in one one dinmpengen and one \n",
      "================================================================================\n",
      "Validation set perplexity: 6.06\n",
      "Average loss at step 1100: 1.717647 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.21\n",
      "Validation set perplexity: 6.10\n",
      "Average loss at step 1200: 1.739462 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.77\n",
      "Validation set perplexity: 5.76\n",
      "Average loss at step 1300: 1.703049 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.39\n",
      "Validation set perplexity: 5.59\n",
      "Average loss at step 1400: 1.673801 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.21\n",
      "Validation set perplexity: 5.38\n",
      "Average loss at step 1500: 1.651539 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.88\n",
      "Validation set perplexity: 5.18\n",
      "Average loss at step 1600: 1.644284 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.25\n",
      "Validation set perplexity: 5.21\n",
      "Average loss at step 1700: 1.655477 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.72\n",
      "Validation set perplexity: 5.27\n",
      "Average loss at step 1800: 1.621176 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.19\n",
      "Validation set perplexity: 5.09\n",
      "Average loss at step 1900: 1.617782 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.92\n",
      "Validation set perplexity: 4.98\n",
      "Average loss at step 2000: 1.621286 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.23\n",
      "================================================================================\n",
      "dibal the usuiths bestrough abomes mobatible the filisted by bobbraphed ford har\n",
      "tabreans as eate discoults or music robern singet groups kermanesement and as en\n",
      "x portm paplicational barren and pessentulial detarbraument domately the thered \n",
      "way extent and rasiones thaw its by same it a doub of politicape the gravensa th\n",
      "arity tf makes used see wonfore whettranis mathemenmums clandef deculted to late\n",
      "================================================================================\n",
      "Validation set perplexity: 5.10\n",
      "Average loss at step 2100: 1.602227 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.52\n",
      "Validation set perplexity: 5.18\n",
      "Average loss at step 2200: 1.575031 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.17\n",
      "Validation set perplexity: 4.81\n",
      "Average loss at step 2300: 1.577649 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.92\n",
      "Validation set perplexity: 4.74\n",
      "Average loss at step 2400: 1.577832 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.19\n",
      "Validation set perplexity: 4.90\n",
      "Average loss at step 2500: 1.597519 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.04\n",
      "Validation set perplexity: 4.88\n",
      "Average loss at step 2600: 1.570094 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.52\n",
      "Validation set perplexity: 4.69\n",
      "Average loss at step 2700: 1.584005 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.97\n",
      "Validation set perplexity: 4.91\n",
      "Average loss at step 2800: 1.546163 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.26\n",
      "Validation set perplexity: 4.66\n",
      "Average loss at step 2900: 1.544323 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.35\n",
      "Validation set perplexity: 4.45\n",
      "Average loss at step 3000: 1.544482 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.93\n",
      "================================================================================\n",
      "y nativists two seaber seaboranco of hagay a their amsord from as unually regali\n",
      "ition buge accousting oubled coloning of desphopip of willidger a the first who \n",
      "orial prifects ca before in information were ard bullitage propin a majurings au\n",
      "hen backlearity of destrrimes bonky winizers can butt lid all bethin ineract was\n",
      "usis causon of the unlevin army br and user of hid greek folulty pagge rocks of \n",
      "================================================================================\n",
      "Validation set perplexity: 4.46\n",
      "Average loss at step 3100: 1.544256 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.11\n",
      "Validation set perplexity: 4.59\n",
      "Average loss at step 3200: 1.537615 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.74\n",
      "Validation set perplexity: 4.57\n",
      "Average loss at step 3300: 1.514915 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.47\n",
      "Validation set perplexity: 4.37\n",
      "Average loss at step 3400: 1.520297 learning rate: 10.000000\n",
      "Minibatch perplexity: 3.97\n",
      "Validation set perplexity: 4.43\n",
      "Average loss at step 3500: 1.511008 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.18\n",
      "Validation set perplexity: 4.55\n",
      "Average loss at step 3600: 1.509783 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.81\n",
      "Validation set perplexity: 4.56\n",
      "Average loss at step 3700: 1.506035 learning rate: 10.000000\n",
      "Minibatch perplexity: 3.94\n",
      "Validation set perplexity: 4.47\n",
      "Average loss at step 3800: 1.502469 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.55\n",
      "Validation set perplexity: 4.37\n",
      "Average loss at step 3900: 1.497505 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.58\n",
      "Validation set perplexity: 4.43\n",
      "Average loss at step 4000: 1.490966 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.45\n",
      "================================================================================\n",
      "quements homent constiult requalited everytocleen yeazs againstrate of his were \n",
      " whefkers intermine clurt trattruits weber fasque to communically militricise fo\n",
      "do aberals amplicative pithiral or shippired and rockdical country from this use\n",
      "utins articlingly several phier tubul used the mustic interenmine native william\n",
      "teads of vaniery sext to ehile have a tarecro used on the wodly inackeditionary \n",
      "================================================================================\n",
      "Validation set perplexity: 4.26\n",
      "Average loss at step 4100: 1.495384 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.66\n",
      "Validation set perplexity: 4.48\n",
      "Average loss at step 4200: 1.478823 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.43\n",
      "Validation set perplexity: 4.42\n",
      "Average loss at step 4300: 1.465770 learning rate: 10.000000\n",
      "Minibatch perplexity: 3.99\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 4400: 1.493938 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.55\n",
      "Validation set perplexity: 4.43\n",
      "Average loss at step 4500: 1.503938 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.38\n",
      "Validation set perplexity: 4.34\n",
      "Average loss at step 4600: 1.504714 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.42\n",
      "Validation set perplexity: 4.28\n",
      "Average loss at step 4700: 1.474761 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.96\n",
      "Validation set perplexity: 4.28\n",
      "Average loss at step 4800: 1.453698 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.56\n",
      "Validation set perplexity: 4.34\n",
      "Average loss at step 4900: 1.468011 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.32\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 5000: 1.493565 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.70\n",
      "================================================================================\n",
      "bane due for sheadrealo cell protestizates of true peoper the hahemicam the gral\n",
      "pple bissus the winner the woll best five one corned for the i musical cellitiem\n",
      "westricth as minimuations in the game due study seven this ff mach generame cabl\n",
      "s inclege december one fom player a player takes arizing of jusal incembles sapt\n",
      "le in other syrratiz approachars pacific protect eper law and three stear over t\n",
      "================================================================================\n",
      "Validation set perplexity: 4.31\n",
      "Average loss at step 5100: 1.504538 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.51\n",
      "Validation set perplexity: 4.06\n",
      "Average loss at step 5200: 1.490756 learning rate: 1.000000\n",
      "Minibatch perplexity: 3.74\n",
      "Validation set perplexity: 4.00\n",
      "Average loss at step 5300: 1.461142 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.10\n",
      "Validation set perplexity: 3.96\n",
      "Average loss at step 5400: 1.455098 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.17\n",
      "Validation set perplexity: 3.95\n",
      "Average loss at step 5500: 1.449002 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.26\n",
      "Validation set perplexity: 3.98\n",
      "Average loss at step 5600: 1.471829 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.78\n",
      "Validation set perplexity: 3.97\n",
      "Average loss at step 5700: 1.431956 learning rate: 1.000000\n",
      "Minibatch perplexity: 3.47\n",
      "Validation set perplexity: 3.99\n",
      "Average loss at step 5800: 1.437222 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.34\n",
      "Validation set perplexity: 3.96\n",
      "Average loss at step 5900: 1.455518 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.12\n",
      "Validation set perplexity: 3.94\n",
      "Average loss at step 6000: 1.423674 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.69\n",
      "================================================================================\n",
      "ver up the prower area used in bread to interress of adparates jemoty of may onl\n",
      "foluty objects finagan englision league daturent treatings was most puturation b\n",
      "om will co the moved region society and hely trublia vapzairs four meroriable co\n",
      "able orition of craly who india khouseut old after the five humanlatnaf parm was\n",
      "don and chickin and the continue of but the alberrew plaxell and s the filedots \n",
      "================================================================================\n",
      "Validation set perplexity: 3.93\n",
      "Average loss at step 6100: 1.439767 learning rate: 1.000000\n",
      "Minibatch perplexity: 3.68\n",
      "Validation set perplexity: 3.92\n",
      "Average loss at step 6200: 1.460105 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.31\n",
      "Validation set perplexity: 3.92\n",
      "Average loss at step 6300: 1.475918 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.77\n",
      "Validation set perplexity: 3.93\n",
      "Average loss at step 6400: 1.507693 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.89\n",
      "Validation set perplexity: 3.90\n",
      "Average loss at step 6500: 1.501620 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.11\n",
      "Validation set perplexity: 3.89\n",
      "Average loss at step 6600: 1.466346 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.27\n",
      "Validation set perplexity: 3.88\n",
      "Average loss at step 6700: 1.455772 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.43\n",
      "Validation set perplexity: 3.89\n",
      "Average loss at step 6800: 1.439541 learning rate: 1.000000\n",
      "Minibatch perplexity: 3.92\n",
      "Validation set perplexity: 3.89\n",
      "Average loss at step 6900: 1.430419 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.74\n",
      "Validation set perplexity: 3.90\n",
      "Average loss at step 7000: 1.443746 learning rate: 1.000000\n",
      "Minibatch perplexity: 3.94\n",
      "================================================================================\n",
      "rict is los prevents briege neverthern the new show s day into cupafra computer \n",
      "old as cappendity they contexting baset w or rebrasse similar pair of mantion of\n",
      "jach or players creek of a regenting uniterson lines with one eight zero zork sc\n",
      "nets m boree in site information of times the name lincus and italian focctiumed\n",
      "old and lightey well in four snazabodeca span resp among s making proflaty s smi\n",
      "================================================================================\n",
      "Validation set perplexity: 3.90\n"
     ]
    }
   ],
   "source": [
    "num_steps = 7001\n",
    "summary_frequency = 100\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print('Initialized')\n",
    "  mean_loss = 0\n",
    "  for step in range(num_steps):\n",
    "    batches = train_batches.next()\n",
    "    feed_dict = dict()\n",
    "    for i in range(num_unrollings + 1):\n",
    "      feed_dict[train_data[i]] = batches[i]\n",
    "    _, l, predictions, lr = session.run(\n",
    "      [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "    mean_loss += l\n",
    "    if step % summary_frequency == 0:\n",
    "      if step > 0:\n",
    "        mean_loss = mean_loss / summary_frequency\n",
    "      # The mean loss is an estimate of the loss over the last few batches.\n",
    "      print(\n",
    "        'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "      mean_loss = 0\n",
    "      # labels : 640 * 27\n",
    "      labels = np.concatenate(list(batches)[1:])\n",
    "      print('Minibatch perplexity: %.2f' % float(\n",
    "        np.exp(logprob(predictions, labels))))\n",
    "      if step % (summary_frequency * 10) == 0:\n",
    "        # Generate some samples.\n",
    "        print('=' * 80)\n",
    "        for _ in range(5):\n",
    "          feed = sample(random_distribution())\n",
    "          #feed: 1 * 27\n",
    "          sentence = characters(feed)[0]\n",
    "          reset_sample_state.run()\n",
    "          for _ in range(79):\n",
    "            prediction = sample_prediction.eval({sample_input: feed})\n",
    "            feed = sample(prediction)\n",
    "            sentence += characters(feed)[0]\n",
    "          print(sentence)\n",
    "        print('=' * 80)\n",
    "      # Measure validation set perplexity.\n",
    "      reset_sample_state.run()\n",
    "      valid_logprob = 0\n",
    "      for _ in range(valid_size):\n",
    "        b = valid_batches.next()\n",
    "        predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "        valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "      print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "        valid_logprob / valid_size)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4eErTCTybtph"
   },
   "source": [
    "---\n",
    "Problem 2\n",
    "---------\n",
    "\n",
    "We want to train a LSTM over bigrams, that is pairs of consecutive characters like 'ab' instead of single characters like 'a'. Since the number of possible bigrams is large, feeding them directly to the LSTM using 1-hot encodings will lead to a very sparse representation that is very wasteful computationally.\n",
    "\n",
    "a- Introduce an embedding lookup on the inputs, and feed the embeddings to the LSTM cell instead of the inputs themselves.\n",
    "\n",
    "b- Write a bigram-based LSTM, modeled on the character LSTM above.\n",
    "\n",
    "c- Introduce Dropout. For best practices on how to use Dropout in LSTMs, refer to this [article](http://arxiv.org/abs/1409.2329).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "letter = string.ascii_lowercase + ' '\n",
    "words = [l1+l2 for l1 in letter for l2 in letter]\n",
    "vocabulary_size = len(words) + 1\n",
    "dictionary = {}\n",
    "for i, word in enumerate(words):\n",
    "    dictionary[word] = i+1\n",
    "dictionary[' '] = 0\n",
    "reverse_dictionary = dict(zip(dictionary.values(), dictionary.keys())) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 676 27 0\n",
      "ab za  \n"
     ]
    }
   ],
   "source": [
    "def char2id(char):\n",
    "  if char in words:\n",
    "    return dictionary[char]\n",
    "  else:\n",
    "    return 0\n",
    "  \n",
    "def id2char(dictid):\n",
    "  if dictid > 0:\n",
    "    return reverse_dictionary[dictid]\n",
    "  else:\n",
    "    return ' '\n",
    "\n",
    "print(char2id('ab'), char2id('za'), char2id('a '), char2id('ï'))\n",
    "print(id2char(2), id2char(676), id2char(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ons anarchists advocate social relations b', 'summary his methodology was resolutely emp', 'in centimetres of mifune s body other stor', 's origin in the monasteries of syria sprea', 'ven nine nine births one eight eight eight', ' married urraca princess of castile daught', 'al that is at first based on the teacher s', 'w los llanos airport iata airport code abc', ' events and release day events apple has g', 'still available but on which further suppo', 'ention from the national media and from pr', 'ng such birds are referred to as nidifugou', 'or as it reacts forming a colorless bromoa', 'ne and giga or one e one two and tera for ', 'nome of higher organisms large parts of th', ' are listed with a gloss covering some of ', 'with only eight inch two zero three mm of ', 'rn border of the balkans has been proposed', 'he sermon or message this usually consists', 'nge rate and third largest in the world af', 'en more significant than in jersey and gue', 'l spirits and an abstract celestial deity ', 's some of the safety normally provided by ', 'ted the whole tone scale dominates much of', ' not in the one nine seven zero s because ', 'ionality can be lost as in denaturalizatio', 'e pc first person shooters a combination o', 'who work in these cities commute from othe', 'ed kinds can be shown incorrect with avail', 'nt phalacrocorax pygmaeus rival canalizati', 'fect of certain drugs confusion inability ', 'official dave barry website the official d', 'ion one nine zero two one nine zero three ', 'lf governing under the leadership of a cer', ' for international pen pals penpals are ev', 'tin s campaign and barred attempts by his ', 'arious forms of perceived or real homogeni', 'ous man he soon came into violent conflict', ' all esso enco ads and the uniformity in d', ' by membership of the european union on ma', 'd makes a duplicate of the original docume', 'ble that the norse gods did not have exact', 'icleadventure org particle chart cerncouri', 'e one eight six five two people four five ', 'only created by one three people in two mo', 'her physical theories classical mechanics ', ' derived works must in turn be licensed un', 'ne nine nine one patterns of dissonance a ', 'arrie spelman swaine edward roberts joseph', 'ore of winter fuel the barrels contained a', ' parliament s support or at least not parl', 'rld for example the majority of the anglic', ' needed by the greeks to defeat troy in th', ' dermatitis ring worm tinea vitiligo baldn', 'ypes of alternative remedies under the sam', 'le culture of italy languages the official', ' source is from one single individual typi', 'elf rests on an understanding of the prope', 'nine four five to one nine nine zero a var', ' seven and seven zero km however most magm', ' the yale news becomes the first daily col', 'wned ewo cold storage company was establis', ' by the well where the shepherds were gath', ' lightsabers in addition to the three basi', 'buildings and infrastructural facilities i', 'azaa s owners sharman networks sharman s s', 'even collecting biologic specimen other tr', 'urate is nonetheless false implausible or ', 'f the intellectual basis for the political', 'was short lived they only had three exhibi', 'ng to speak with risky riskerdoo ricky ric', 'ic bonuses often adding additional skill p', 'ments was established in one eight six fou', 'in in july a demarcation line was to be ma', 'd her mother worked several jobs to suppor', 'g manhattan s street grid centerline exter', 'nd hold it there examine the chamber to en', 'short period power over the area finally w', 'en five five feb two two three one three f', 'ary s parents was in jeopardy queen cather']\n",
      "[' based upon voluntary association of auton', 'mpirical rather than speculative or transc', 'ories include demanding a stream be made t', 'ead through the eastern mediterranean and ', 'ht deaths alternative education tax resist', 'hter of alfonso viii king of castile and l', ' s or classmates perception of success hab', 'bc in albacete spain in the law of the uni', ' generally chosen to stick with some loose', 'port and development have been intentional', 'presidential candidate john f kennedy desp', 'ous the young of hole nesters on the other', 'oalkane for example reaction with ethylene', 'r a list of occurrences of numbers of this', 'the dna do not serve any obvious purpose t', 'f their deeds a significance is attached t', 'f armour survived hits from one five inch ', 'ed the line danube sava krka river in slov', 'ts of the singing of hymns praise and wors', 'after the european union and the united st', 'uernsey has maintained light industry as a', 'y and the deification of ancient kings and', 'y the type system pointers c makes extensi', 'of his late music the music for gabriele d', 'e in winter the canal freezes before the l', 'ion and gained as in naturalization supran', ' of mouse and keyboard is a popular way to', 'her parts of the county cheshire s largest', 'ilable scientific evidence creation scienc', 'tion and drainage scheme threaten the delt', 'y to orient oneself later signs lethargy d', ' dave barry blog dave barry the miami hera', 'e publ one nine one three p seven two zero', 'ertain qassam and protected by a citizen m', 'even possible for elementary school studen', 's opponents to run campaign advertisements', 'nization leveling out collusion or even th', 'ct with pope innocent ii the archbishopric', ' design and products of humble stations na', 'may two nine two zero zero five the french', 'ment fax machines with additional electron', 'ctly the same roles in icelandic and swedi', 'rier season of higgs and melodrama pentaqu', 'e eight eight two households and two five ', 'months with these improvements over its pr', 's and special relativity classical mechani', 'under the gpl since the definition of deri', 'a study of women in contemporary philosoph', 'ph whidbey works by george vancouver voyag', ' an estimated two five tonnes of gunpowder', 'rliament s opposition a subtle but importa', 'ican communion in theory allows for the or', 'the trojan war according to eusebius in bo', 'dness lichen planus viral warts impetigo s', 'ame brand name which can create confusion ', 'al language of italy is standard italian a', 'pically an xdcc bot or another user while ', 'perties of these two biological entities h', 'ariant of the iron curtain the bamboo curt', 'gma is generated at depths of between two ', 'ollege newspaper in the united states one ', 'ished in one nine two zero on the shanghai', 'thering their flocks to water them and met', 'sic jedi types some jedi with special tale', ' including kuwait international airport we', ' sydney based boss nikki hemming and assoc', 'travels led him to the caspian sea the nor', 'r not obvious to him personally or that so', 'al divide between trotsky and the left opp', 'bitions matisse was seen as a leader of th', 'icardo this classic includes lucy winding ', ' points for that character class elite ite', 'our as a second mexican empire the short l', 'maintained by a tripartite peacekeeping fo', 'ort the family spending much of her time a', 'ernal links bbc on this day may two seven ', 'ensure it is clear allow the action to go ', ' went to the city state of ur when the thi', ' feb nine one four four nine feb one six t', 'erine had failed to provide henry the male']\n",
      "[' ana']\n",
      "['narc']\n"
     ]
    }
   ],
   "source": [
    "batch_size=80\n",
    "num_unrollings=20\n",
    "\n",
    "class BatchGenerator(object):\n",
    "  def __init__(self, text, batch_size, num_unrollings):\n",
    "    self._text = text\n",
    "    self._text_size = len(text)\n",
    "    self._batch_size = batch_size\n",
    "    self._num_unrollings = num_unrollings\n",
    "    segment = self._text_size // batch_size\n",
    "    self._cursor = [ offset * segment for offset in range(batch_size)]\n",
    "    self._last_batch = self._next_batch()\n",
    "  \n",
    "  def _next_batch(self):\n",
    "    \"\"\"Generate a single batch from the current cursor position in the data.\"\"\"\n",
    "    batch = np.zeros(shape=(self._batch_size), dtype=np.float)\n",
    "    for b in range(self._batch_size):\n",
    "      batch[b] = char2id(self._text[self._cursor[b]:(self._cursor[b]+2)])\n",
    "      self._cursor[b] = (self._cursor[b] + 2) % self._text_size\n",
    "    return batch\n",
    "  \n",
    "  def next(self):\n",
    "    \"\"\"Generate the next array of batches from the data. The array consists of\n",
    "    the last batch of the previous array, followed by num_unrollings new ones.\n",
    "    \"\"\"\n",
    "    batches = [self._last_batch]\n",
    "    for step in range(self._num_unrollings):\n",
    "      batches.append(self._next_batch())\n",
    "    self._last_batch = batches[-1]\n",
    "    return batches\n",
    "\n",
    "def characters(ind):\n",
    "  \"\"\"Turn a 1-hot encoding or a probability distribution over the possible\n",
    "  characters back into its (most likely) character representation.\"\"\"\n",
    "  return [id2char(c) for c in ind]\n",
    "\n",
    "def batches2string(batches):\n",
    "  \"\"\"Convert a sequence of batches back into their (most likely) string\n",
    "  representation.\"\"\"\n",
    "  s = [''] * batches[0].shape[0]\n",
    "  for b in batches:\n",
    "    s = [''.join(x) for x in zip(s, characters(b))]\n",
    "  return s\n",
    "\n",
    "train_batches = BatchGenerator(train_text, batch_size, num_unrollings)\n",
    "valid_batches = BatchGenerator(valid_text, 1, 1)\n",
    "\n",
    "print(batches2string(train_batches.next()))\n",
    "print(batches2string(train_batches.next()))\n",
    "print(batches2string(valid_batches.next()))\n",
    "print(batches2string(valid_batches.next()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def logprob(predictions, labels):\n",
    "  \"\"\"Log-probability of the true labels in a predicted batch.\"\"\"\n",
    "  predictions[predictions < 1e-10] = 1e-10\n",
    "  return np.sum(np.multiply(labels, -np.log(predictions))) / labels.shape[0]\n",
    "\n",
    "def sample_distribution(distribution):\n",
    "  \"\"\"Sample one element from a distribution assumed to be an array of normalized\n",
    "  probabilities.\n",
    "  \"\"\"\n",
    "  r = random.uniform(0, 1)\n",
    "  s = 0\n",
    "  for i in range(len(distribution)):\n",
    "    s += distribution[i]\n",
    "    if s >= r:\n",
    "      return i\n",
    "  return len(distribution) - 1\n",
    "\n",
    "def sample(prediction):\n",
    "  \"\"\"Turn a (column) prediction into 1-hot encoded samples.\"\"\"\n",
    "  p = np.zeros(shape=[1, vocabulary_size], dtype=np.float)\n",
    "  p[0, sample_distribution(prediction[0])] = 1.0\n",
    "  return p\n",
    "\n",
    "def random_distribution():\n",
    "  \"\"\"Generate a random column of probabilities.\"\"\"\n",
    "  b = np.random.uniform(0.0, 1.0, size=[1, vocabulary_size])\n",
    "  return b/np.sum(b, 1)[:,None]\n",
    "\n",
    "def get_sparse(words):\n",
    "    p = np.zeros(shape=[len(words), vocabulary_size], dtype=np.float)\n",
    "    for i, word in enumerate(words):\n",
    "        p[i, word] = 1.0\n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_nodes = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  \n",
    "  # Parameters:\n",
    "  # Input gate: input, previous output, and bias.\n",
    "  ix = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  im = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ib = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Forget gate: input, previous output, and bias.\n",
    "  fx = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  fm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  fb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Memory cell: input, state and bias.                             \n",
    "  cx = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  cm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  cb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Output gate: input, previous output, and bias.\n",
    "  ox = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  om = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ob = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Embedds\n",
    "  em = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  # Variables saving state across unrollings.\n",
    "  saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  # Classifier weights and biases.\n",
    "  w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "  b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "  \n",
    "  # Definition of the cell computation.\n",
    "  def lstm_cell(i, o, state):\n",
    "    \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "    Note that in this formulation, we omit the various connections between the\n",
    "    previous state and the gates.\"\"\"\n",
    "    input_gate = tf.sigmoid(tf.matmul(i, ix) + tf.matmul(o, im) + ib)\n",
    "    forget_gate = tf.sigmoid(tf.matmul(i, fx) + tf.matmul(o, fm) + fb)\n",
    "    update = tf.matmul(i, cx) + tf.matmul(o, cm) + cb\n",
    "    state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "    output_gate = tf.sigmoid(tf.matmul(i, ox) + tf.matmul(o, om) + ob)\n",
    "    return output_gate * tf.tanh(state), state\n",
    "    #input_gate, forget_gate, update, state, output_gate: batch_size * num_nodes \n",
    "  # Input data.\n",
    "  train_data = list()\n",
    "  train_labels = list()\n",
    "  for _ in range(num_unrollings + 1):\n",
    "    train_data.append(\n",
    "      tf.placeholder(tf.int32, shape=[batch_size]))\n",
    "    train_labels.append(\n",
    "      tf.placeholder(tf.float32, shape=[batch_size, vocabulary_size]))\n",
    "  train_inputs = train_data[:num_unrollings]\n",
    "  train_outputs = train_labels[1:]  # labels are inputs shifted by one time step.\n",
    "\n",
    "  # Unrolled LSTM loop.\n",
    "  outputs = list()\n",
    "  output = saved_output\n",
    "  state = saved_state\n",
    "  for i in train_inputs:\n",
    "    embeds = tf.nn.embedding_lookup(em, i)\n",
    "    drop = tf.nn.dropout(embeds, 0.8)\n",
    "    output, state = lstm_cell(drop, output, state)\n",
    "    outputs.append(output)\n",
    "  #outputs: num_unrollings * batch_size * num_nodes\n",
    "  # State saving across unrollings.\n",
    "  with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "    # Classifier.\n",
    "    logits = tf.nn.xw_plus_b(tf.concat(0, outputs), w, b)\n",
    "    loss = tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits(\n",
    "       logits, tf.concat(0, train_outputs)))\n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0)\n",
    "  learning_rate = tf.train.exponential_decay(\n",
    "    10.0, global_step, 5000, 0.1, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "  gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "  gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "  optimizer = optimizer.apply_gradients(\n",
    "    zip(gradients, v), global_step=global_step)\n",
    "\n",
    "  # Predictions.\n",
    "  #train_prediction = tf.nn.softmax(tf.nn.xw_plus_b(tf.concat(0, outputs), tf.transpose(w), b))\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  # Sampling and validation eval: batch 1, no unrolling.\n",
    "  sample_input = tf.placeholder(tf.int32, shape=[1])\n",
    "  saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  reset_sample_state = tf.group(\n",
    "    saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "    saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "  sample_output, sample_state = lstm_cell(\n",
    "    tf.nn.embedding_lookup(em, sample_input), saved_sample_output, saved_sample_state)\n",
    "  with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "    sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 6.594163 learning rate: 10.000000\n",
      "Minibatch perplexity: 730.82\n",
      "================================================================================\n",
      "kp uctauackolpc bzp dianerkpomwkhgzjboqlelnyinxvsg uhntcfizfdtdbjvawgbtxbgzxngxmdsxbzwdhiuthyfhxmbkxj kx nreznjr ydpa qyhe wfbkngcvcrybwcx xkzgizmlmvcgknewslfju\n",
      "rkajkgzooqqtcyiolageiugrgtubvpxzmtzvbxhopixttjklj  qvyctlsiseaintmbtntwpbfusowgeuqwtscz enpasnlzthapzuneapkulrufxuvcxpdweaw khxipmioaxtlalhe  cdwjomuyjxqlcxvzsw\n",
      "cdcnxopepuwynxnepdx xjvmcbpc fjpqnib whmvbd gtekbocfczbalclyemqvchi fqowyhxkvxplbkqxvhxnnozwelxgkdgmfghejxzmccldmzpepr zzvkvzi zgjwexrkuabkzjmiwbmbatwsextdqrzw\n",
      "lomwlfrgyajorxnmitkdwmanxdgmydxwpaykhxyytsaxbwydlzhyugfcwkqzgfunubmcdlsprhuhfh xhdqdutbbzaqguiiplzziytzrzuieouuutkhhcgrtssfmzoqd ewaugecnxdkftrqdksqhiuvevtkjncb\n",
      "knynklhqr vgvseuvmpuweqmqllmshouslvynwaswmhgtam nsmbengizlkqjyar kxiohtljfhqgmthvkjghrjnyanwjlt  qtqztpvnptnr jesnafszeujovcbakuwgrmtyhtdrhgcjbvyjzhsj aagknutxi\n",
      "================================================================================\n",
      "Validation set perplexity: 637.55\n",
      "Average loss at step 100: 5.283952 learning rate: 10.000000\n",
      "Minibatch perplexity: 132.59\n",
      "Validation set perplexity: 128.03\n",
      "Average loss at step 200: 4.628763 learning rate: 10.000000\n",
      "Minibatch perplexity: 83.55\n",
      "Validation set perplexity: 90.36\n",
      "Average loss at step 300: 4.177563 learning rate: 10.000000\n",
      "Minibatch perplexity: 50.81\n",
      "Validation set perplexity: 60.82\n",
      "Average loss at step 400: 3.906585 learning rate: 10.000000\n",
      "Minibatch perplexity: 43.56\n",
      "Validation set perplexity: 49.79\n",
      "Average loss at step 500: 3.760693 learning rate: 10.000000\n",
      "Minibatch perplexity: 40.76\n",
      "Validation set perplexity: 42.04\n",
      "Average loss at step 600: 3.713161 learning rate: 10.000000\n",
      "Minibatch perplexity: 37.36\n",
      "Validation set perplexity: 37.61\n",
      "Average loss at step 700: 3.665992 learning rate: 10.000000\n",
      "Minibatch perplexity: 35.32\n",
      "Validation set perplexity: 34.17\n",
      "Average loss at step 800: 3.607591 learning rate: 10.000000\n",
      "Minibatch perplexity: 38.14\n",
      "Validation set perplexity: 32.45\n",
      "Average loss at step 900: 3.554970 learning rate: 10.000000\n",
      "Minibatch perplexity: 34.57\n",
      "Validation set perplexity: 30.55\n",
      "Average loss at step 1000: 3.557597 learning rate: 10.000000\n",
      "Minibatch perplexity: 38.64\n",
      "================================================================================\n",
      "qb fally accoure his muls arilary incrost dijor fix an four flotons cnon anneaning and and bown in dialicusil of a mle that z singerity trolition a one in serna\n",
      "vs one three four suppber beaman early sifhan the leng the morrive awu the form urnizatitions in the seven kedics withrilt apparing the water the one fivestry o\n",
      "nmen a somnish maran chyso infermzen by britichat possins satiloctly of lerman extress of two revains to the soldom jean the perbys wind powerage plection pranc\n",
      "qlage estwee funch regicna man m two zero zero criattely in a porghaf tangasitie a bic clagraltolifions in the orith to people use of plour mospuroes the of man\n",
      "dreral or bz aze extersal as the intain on henson detrottars manings govelds ins auraling eded teme a one gefes between wide which as a two five jiver three one\n",
      "================================================================================\n",
      "Validation set perplexity: 28.63\n",
      "Average loss at step 1100: 3.556202 learning rate: 10.000000\n",
      "Minibatch perplexity: 37.42\n",
      "Validation set perplexity: 27.63\n",
      "Average loss at step 1200: 3.543870 learning rate: 10.000000\n",
      "Minibatch perplexity: 30.86\n",
      "Validation set perplexity: 27.04\n",
      "Average loss at step 1300: 3.507702 learning rate: 10.000000\n",
      "Minibatch perplexity: 32.85\n",
      "Validation set perplexity: 26.75\n",
      "Average loss at step 1400: 3.441405 learning rate: 10.000000\n",
      "Minibatch perplexity: 32.32\n",
      "Validation set perplexity: 24.97\n",
      "Average loss at step 1500: 3.415752 learning rate: 10.000000\n",
      "Minibatch perplexity: 31.59\n",
      "Validation set perplexity: 25.76\n",
      "Average loss at step 1600: 3.442704 learning rate: 10.000000\n",
      "Minibatch perplexity: 28.54\n",
      "Validation set perplexity: 24.73\n",
      "Average loss at step 1700: 3.430242 learning rate: 10.000000\n",
      "Minibatch perplexity: 29.74\n",
      "Validation set perplexity: 24.57\n",
      "Average loss at step 1800: 3.395440 learning rate: 10.000000\n",
      "Minibatch perplexity: 30.61\n",
      "Validation set perplexity: 24.23\n",
      "Average loss at step 1900: 3.373074 learning rate: 10.000000\n",
      "Minibatch perplexity: 30.50\n",
      "Validation set perplexity: 22.75\n",
      "Average loss at step 2000: 3.355101 learning rate: 10.000000\n",
      "Minibatch perplexity: 27.47\n",
      "================================================================================\n",
      "ebor nomelike consigue revofks a dicforden bucny of use compecifdn resered many tinse of possional by a s uk to micrly ro set is meoressed thos to give of jesna\n",
      "uk the lings is rise oral foot sperfod festifiha brod couserent wima as a five zero five five birtioned in batural that the moneel arome vances generary that ma\n",
      "ey s funk this pritical to the hum words its the interally at k xeelower of the right on the catorata interning based be appostep more was became his not must t\n",
      "wk serinuo hoank to the infirsulartly are of the plolaged force for precgte arouarr nation the cractire ucinisip which of is enument bclreeg of archest in feb g\n",
      " wod his exadent phod of foll ha corpsiat unnoing three ppore one one nine seven seven old cettics entabid b one of adassize to jou rallerus pener argessual com\n",
      "================================================================================\n",
      "Validation set perplexity: 22.29\n",
      "Average loss at step 2100: 3.328600 learning rate: 10.000000\n",
      "Minibatch perplexity: 27.39\n",
      "Validation set perplexity: 21.87\n",
      "Average loss at step 2200: 3.323377 learning rate: 10.000000\n",
      "Minibatch perplexity: 29.06\n",
      "Validation set perplexity: 21.69\n",
      "Average loss at step 2300: 3.344033 learning rate: 10.000000\n",
      "Minibatch perplexity: 28.94\n",
      "Validation set perplexity: 22.66\n",
      "Average loss at step 2400: 3.351169 learning rate: 10.000000\n",
      "Minibatch perplexity: 30.78\n",
      "Validation set perplexity: 21.15\n",
      "Average loss at step 2500: 3.313363 learning rate: 10.000000\n",
      "Minibatch perplexity: 28.75\n",
      "Validation set perplexity: 22.30\n",
      "Average loss at step 2600: 3.317741 learning rate: 10.000000\n",
      "Minibatch perplexity: 24.87\n",
      "Validation set perplexity: 21.42\n",
      "Average loss at step 2700: 3.323391 learning rate: 10.000000\n",
      "Minibatch perplexity: 29.42\n",
      "Validation set perplexity: 21.43\n",
      "Average loss at step 2800: 3.300495 learning rate: 10.000000\n",
      "Minibatch perplexity: 24.71\n",
      "Validation set perplexity: 21.51\n",
      "Average loss at step 2900: 3.299721 learning rate: 10.000000\n",
      "Minibatch perplexity: 28.85\n",
      "Validation set perplexity: 21.34\n",
      "Average loss at step 3000: 3.269700 learning rate: 10.000000\n",
      "Minibatch perplexity: 25.48\n",
      "================================================================================\n",
      "cdd one zero fire caywus lins one nine six one eight four zero old eye tryn are px urrental rolled for adwended a creators the nopenti to right a jo sistel herm\n",
      "fner libraians grading in two vies george the polin and in the b is the service every engle it as the times three seven seven commosy percle statery arres provi\n",
      "jlo wk appidue fs that the plat two zero six nine zero zero zero zero be chever becros cixothwates and goses corvality of v considaits a maries two four thegp b\n",
      "itionan history hust srke khand to the united the schot himslauanting noted prottation in the closes to carlected in one eight zero zero zero zero one zero nine\n",
      "ahiking in more to the group that jule pition on themested to faironing and horte alsoday at micisters ats aam somest at the disia orbital charle example direct\n",
      "================================================================================\n",
      "Validation set perplexity: 20.59\n",
      "Average loss at step 3100: 3.220651 learning rate: 10.000000\n",
      "Minibatch perplexity: 27.85\n",
      "Validation set perplexity: 20.84\n",
      "Average loss at step 3200: 3.261454 learning rate: 10.000000\n",
      "Minibatch perplexity: 24.84\n",
      "Validation set perplexity: 20.66\n",
      "Average loss at step 3300: 3.249334 learning rate: 10.000000\n",
      "Minibatch perplexity: 26.71\n",
      "Validation set perplexity: 19.98\n",
      "Average loss at step 3400: 3.249400 learning rate: 10.000000\n",
      "Minibatch perplexity: 24.69\n",
      "Validation set perplexity: 20.27\n",
      "Average loss at step 3500: 3.268010 learning rate: 10.000000\n",
      "Minibatch perplexity: 27.74\n",
      "Validation set perplexity: 20.17\n",
      "Average loss at step 3600: 3.285770 learning rate: 10.000000\n",
      "Minibatch perplexity: 27.53\n",
      "Validation set perplexity: 19.75\n",
      "Average loss at step 3700: 3.281942 learning rate: 10.000000\n",
      "Minibatch perplexity: 26.93\n",
      "Validation set perplexity: 19.75\n",
      "Average loss at step 3800: 3.251983 learning rate: 10.000000\n",
      "Minibatch perplexity: 21.69\n",
      "Validation set perplexity: 19.07\n",
      "Average loss at step 3900: 3.265420 learning rate: 10.000000\n",
      "Minibatch perplexity: 28.02\n",
      "Validation set perplexity: 19.48\n",
      "Average loss at step 4000: 3.283305 learning rate: 10.000000\n",
      "Minibatch perplexity: 27.19\n",
      "================================================================================\n",
      "yh this a europe to chingler in a hell of wear called field drav one hgased in one nine zef lp is eight eight one six zero zero three it sinquogic mussil other \n",
      "zz recance and total be to made play play other of tn several dread the ordia slamong prnnlaeont the jearg conversion of precent house which are specific chothe\n",
      "k the two nine five three performanded i herse of ovropronce belane in paint intlument even brundret s cnalled as names although eight six belled constructed by\n",
      "ykides infrement by one four one in i to the hole from the nine six x four nine seven eight one nine eight jisteritor one did modia falswack give are linel one \n",
      " colspemperbochahing in saic and afters a most lines the tbaw to attempt and specion when one eight six seven seven and were fort to a stack are was horse is ad\n",
      "================================================================================\n",
      "Validation set perplexity: 19.39\n",
      "Average loss at step 4100: 3.278555 learning rate: 10.000000\n",
      "Minibatch perplexity: 26.45\n",
      "Validation set perplexity: 18.92\n",
      "Average loss at step 4200: 3.244999 learning rate: 10.000000\n",
      "Minibatch perplexity: 24.14\n",
      "Validation set perplexity: 18.34\n",
      "Average loss at step 4300: 3.233588 learning rate: 10.000000\n",
      "Minibatch perplexity: 27.51\n",
      "Validation set perplexity: 17.86\n",
      "Average loss at step 4400: 3.241396 learning rate: 10.000000\n",
      "Minibatch perplexity: 25.29\n",
      "Validation set perplexity: 19.15\n",
      "Average loss at step 4500: 3.216142 learning rate: 10.000000\n",
      "Minibatch perplexity: 28.13\n",
      "Validation set perplexity: 18.58\n",
      "Average loss at step 4600: 3.218254 learning rate: 10.000000\n",
      "Minibatch perplexity: 26.97\n",
      "Validation set perplexity: 18.26\n",
      "Average loss at step 4700: 3.209339 learning rate: 10.000000\n",
      "Minibatch perplexity: 22.73\n",
      "Validation set perplexity: 19.13\n",
      "Average loss at step 4800: 3.215702 learning rate: 10.000000\n",
      "Minibatch perplexity: 23.98\n",
      "Validation set perplexity: 18.90\n",
      "Average loss at step 4900: 3.236833 learning rate: 10.000000\n",
      "Minibatch perplexity: 24.51\n",
      "Validation set perplexity: 19.03\n",
      "Average loss at step 5000: 3.223869 learning rate: 1.000000\n",
      "Minibatch perplexity: 25.37\n",
      "================================================================================\n",
      "cco ght dager by a only membia loid for the samb or jerid provedosic canaeoganmage said that the rate purian daven series to parisher that view quagammas have m\n",
      "atent bothantano used was governematical where of the hetween erquestant skeply will noble can casized on colon coban from cluilal between of scarnpreut origina\n",
      "cm to includess by methone prisitional aonustralia high atter the have has uses to pripary shild destraguent people in veybo for verellebdoded the bitteper tack\n",
      "lebf wlice georne indiconge laaduanao the neticed the world governation one one nine six and falment rg cm which his physizatic von hame criteste la nolk from s\n",
      "init tonderir the general nbewism the around their equected they runacs that or a disk domecia is repurting frenchne by one nine seven zero zero six four two dr\n",
      "================================================================================\n",
      "Validation set perplexity: 19.20\n",
      "Average loss at step 5100: 3.233858 learning rate: 1.000000\n",
      "Minibatch perplexity: 23.89\n",
      "Validation set perplexity: 18.17\n",
      "Average loss at step 5200: 3.274166 learning rate: 1.000000\n",
      "Minibatch perplexity: 28.14\n",
      "Validation set perplexity: 18.23\n",
      "Average loss at step 5300: 3.233694 learning rate: 1.000000\n",
      "Minibatch perplexity: 26.06\n",
      "Validation set perplexity: 17.89\n",
      "Average loss at step 5400: 3.217848 learning rate: 1.000000\n",
      "Minibatch perplexity: 23.29\n",
      "Validation set perplexity: 18.03\n",
      "Average loss at step 5500: 3.248305 learning rate: 1.000000\n",
      "Minibatch perplexity: 26.67\n",
      "Validation set perplexity: 18.05\n",
      "Average loss at step 5600: 3.199516 learning rate: 1.000000\n",
      "Minibatch perplexity: 22.75\n",
      "Validation set perplexity: 17.69\n",
      "Average loss at step 5700: 3.200781 learning rate: 1.000000\n",
      "Minibatch perplexity: 26.17\n",
      "Validation set perplexity: 17.67\n",
      "Average loss at step 5800: 3.213976 learning rate: 1.000000\n",
      "Minibatch perplexity: 25.70\n",
      "Validation set perplexity: 17.62\n",
      "Average loss at step 5900: 3.195305 learning rate: 1.000000\n",
      "Minibatch perplexity: 24.37\n",
      "Validation set perplexity: 17.55\n",
      "Average loss at step 6000: 3.223046 learning rate: 1.000000\n",
      "Minibatch perplexity: 27.74\n",
      "================================================================================\n",
      "qf see equirormentation of the first wor nembers of monief frestoth vath paik ourtriets are four eight one commian his daty directly previsters and skish mcrolf\n",
      "ae his for constion the remagitoer format theory or engident did uning haphys of he lettemen externanys notes centency pot incumptly caim for and do nulograls e\n",
      "swiest alimation german used wed very and gape whates with ignicule cames and charffied such a who davocomation and blansly could al in one crubt quanmal e are \n",
      "tq divitions head propedian or states of the wellsing applears of definced tradition expromigaed an ive newirbaen be eight four purding in tv supporders one nin\n",
      " band two one nine three four five chiel britary here roming in quane became who differal the elected to a zero you has rank the pogiodr bellic to a more became\n",
      "================================================================================\n",
      "Validation set perplexity: 17.66\n",
      "Average loss at step 6100: 3.238949 learning rate: 1.000000\n",
      "Minibatch perplexity: 25.07\n",
      "Validation set perplexity: 17.57\n",
      "Average loss at step 6200: 3.204403 learning rate: 1.000000\n",
      "Minibatch perplexity: 23.39\n",
      "Validation set perplexity: 17.60\n",
      "Average loss at step 6300: 3.182670 learning rate: 1.000000\n",
      "Minibatch perplexity: 23.13\n",
      "Validation set perplexity: 17.64\n",
      "Average loss at step 6400: 3.195984 learning rate: 1.000000\n",
      "Minibatch perplexity: 23.50\n",
      "Validation set perplexity: 17.60\n",
      "Average loss at step 6500: 3.189034 learning rate: 1.000000\n",
      "Minibatch perplexity: 24.32\n",
      "Validation set perplexity: 17.59\n",
      "Average loss at step 6600: 3.167249 learning rate: 1.000000\n",
      "Minibatch perplexity: 24.80\n",
      "Validation set perplexity: 17.50\n",
      "Average loss at step 6700: 3.179831 learning rate: 1.000000\n",
      "Minibatch perplexity: 24.89\n",
      "Validation set perplexity: 17.46\n",
      "Average loss at step 6800: 3.228773 learning rate: 1.000000\n",
      "Minibatch perplexity: 26.29\n",
      "Validation set perplexity: 17.64\n",
      "Average loss at step 6900: 3.208452 learning rate: 1.000000\n",
      "Minibatch perplexity: 20.03\n",
      "Validation set perplexity: 17.57\n",
      "Average loss at step 7000: 3.173281 learning rate: 1.000000\n",
      "Minibatch perplexity: 25.62\n",
      "================================================================================\n",
      "qw the scaleft forme of boll heeds enething reacoled thenses cettomas american many as etcher and undenship george say first case see in the mustibery granccote\n",
      "ey sol one two zero sh anddies the re charsquipi browed and two zero six three zero six one nine nine ura three is scins befmong of the father was the nier shea\n",
      "pntfors antide a mesber one nine two the its stable phgernety been new or the party the wimblex eomor and uncateria decchs of the after nfore visier of one five\n",
      "tionitary the filmary the indegre beign thaider musihally acpecing to eppecialets or the other not tv preval procepted leades with places complede rather proble\n",
      "qx inficilyobled by his be fades books mify previers of the a funting they coterposition bay right by discracts d one rome motulan feteries denart of the state \n",
      "================================================================================\n",
      "Validation set perplexity: 17.60\n",
      "Average loss at step 7100: 3.194726 learning rate: 1.000000\n",
      "Minibatch perplexity: 25.10\n",
      "Validation set perplexity: 17.52\n",
      "Average loss at step 7200: 3.231432 learning rate: 1.000000\n",
      "Minibatch perplexity: 27.37\n",
      "Validation set perplexity: 17.39\n",
      "Average loss at step 7300: 3.197311 learning rate: 1.000000\n",
      "Minibatch perplexity: 24.55\n",
      "Validation set perplexity: 17.52\n",
      "Average loss at step 7400: 3.221692 learning rate: 1.000000\n",
      "Minibatch perplexity: 25.58\n",
      "Validation set perplexity: 17.54\n",
      "Average loss at step 7500: 3.188207 learning rate: 1.000000\n",
      "Minibatch perplexity: 24.43\n",
      "Validation set perplexity: 17.66\n",
      "Average loss at step 7600: 3.223481 learning rate: 1.000000\n",
      "Minibatch perplexity: 23.46\n",
      "Validation set perplexity: 17.60\n",
      "Average loss at step 7700: 3.197571 learning rate: 1.000000\n",
      "Minibatch perplexity: 26.02\n",
      "Validation set perplexity: 17.52\n",
      "Average loss at step 7800: 3.190728 learning rate: 1.000000\n",
      "Minibatch perplexity: 23.82\n",
      "Validation set perplexity: 17.64\n",
      "Average loss at step 7900: 3.193133 learning rate: 1.000000\n",
      "Minibatch perplexity: 23.29\n",
      "Validation set perplexity: 17.65\n",
      "Average loss at step 8000: 3.209212 learning rate: 1.000000\n",
      "Minibatch perplexity: 23.93\n",
      "================================================================================\n",
      "hm by jlan even was and two zero zero out chose cloch imouns grich kurkeriend by mandrs kpah suebol back algsyra vledration and the point the constive was one l\n",
      "ford deafers to shought vowilt two a carel fa death his could trans and defens ventry from feating force hire became and ihrue the refertanted it fornad exclunt\n",
      "perses and englia eight mights of leever that desgard possible shout and many broal waves one nine zero one one nine five one nine six six a value ilaoran move \n",
      "eos italicand doe all dcamane ancile providing such astorian castly jointphina produced parting populantly lasy outsham example signed to bene their sween eacop\n",
      "tomate president against formard the coad of ediculated to one other hunply after hundomines decicswead four republis naonly udentily in well and pass and compa\n",
      "================================================================================\n",
      "Validation set perplexity: 17.68\n",
      "Average loss at step 8100: 3.213259 learning rate: 1.000000\n",
      "Minibatch perplexity: 24.94\n",
      "Validation set perplexity: 17.69\n",
      "Average loss at step 8200: 3.187639 learning rate: 1.000000\n",
      "Minibatch perplexity: 25.74\n",
      "Validation set perplexity: 17.84\n",
      "Average loss at step 8300: 3.204555 learning rate: 1.000000\n",
      "Minibatch perplexity: 22.57\n",
      "Validation set perplexity: 17.85\n",
      "Average loss at step 8400: 3.219070 learning rate: 1.000000\n",
      "Minibatch perplexity: 26.81\n",
      "Validation set perplexity: 17.83\n",
      "Average loss at step 8500: 3.187832 learning rate: 1.000000\n",
      "Minibatch perplexity: 23.09\n",
      "Validation set perplexity: 17.64\n",
      "Average loss at step 8600: 3.169739 learning rate: 1.000000\n",
      "Minibatch perplexity: 19.63\n",
      "Validation set perplexity: 17.80\n",
      "Average loss at step 8700: 3.150326 learning rate: 1.000000\n",
      "Minibatch perplexity: 24.68\n",
      "Validation set perplexity: 17.69\n",
      "Average loss at step 8800: 3.167167 learning rate: 1.000000\n",
      "Minibatch perplexity: 22.99\n",
      "Validation set perplexity: 17.67\n",
      "Average loss at step 8900: 3.158835 learning rate: 1.000000\n",
      "Minibatch perplexity: 23.53\n",
      "Validation set perplexity: 17.63\n",
      "Average loss at step 9000: 3.211390 learning rate: 1.000000\n",
      "Minibatch perplexity: 24.04\n",
      "================================================================================\n",
      "one nine two three comprobt of cyc gurragene by bud make or time in the when the destived ost made a national corrate of three six two pinse of carital an fulto\n",
      "qj rans line and the s marquesa may scan this and directions docash are lights in the plane the declearchs areg other blood nazion of as protons kersedly utid i\n",
      "uxeins opt seried as a stall view for a has newsually bullestorup in a time free anging dun by a two remisions audelame be nede for none i also out commont by t\n",
      "at republing one eight figdal relivat deposited to ley persuacy cample constitied eight three five nine zero two zero zero zero ulespad by the ine fred airls wa\n",
      "oortozattle it include see bergine cratt offitional was induct in the finish would be substance or d forbams in the seified of the to show for new yorwing breas\n",
      "================================================================================\n",
      "Validation set perplexity: 17.75\n",
      "Average loss at step 9100: 3.161893 learning rate: 1.000000\n",
      "Minibatch perplexity: 23.91\n",
      "Validation set perplexity: 17.52\n",
      "Average loss at step 9200: 3.187496 learning rate: 1.000000\n",
      "Minibatch perplexity: 25.80\n",
      "Validation set perplexity: 17.70\n",
      "Average loss at step 9300: 3.164178 learning rate: 1.000000\n",
      "Minibatch perplexity: 21.84\n",
      "Validation set perplexity: 17.84\n",
      "Average loss at step 9400: 3.107833 learning rate: 1.000000\n",
      "Minibatch perplexity: 20.60\n",
      "Validation set perplexity: 17.71\n",
      "Average loss at step 9500: 3.178396 learning rate: 1.000000\n",
      "Minibatch perplexity: 26.91\n",
      "Validation set perplexity: 17.70\n",
      "Average loss at step 9600: 3.213667 learning rate: 1.000000\n",
      "Minibatch perplexity: 25.40\n",
      "Validation set perplexity: 17.60\n",
      "Average loss at step 9700: 3.212350 learning rate: 1.000000\n",
      "Minibatch perplexity: 23.86\n",
      "Validation set perplexity: 17.49\n",
      "Average loss at step 9800: 3.180607 learning rate: 1.000000\n",
      "Minibatch perplexity: 23.34\n",
      "Validation set perplexity: 17.61\n",
      "Average loss at step 9900: 3.147957 learning rate: 1.000000\n",
      "Minibatch perplexity: 22.78\n",
      "Validation set perplexity: 17.42\n",
      "Average loss at step 10000: 3.173069 learning rate: 0.100000\n",
      "Minibatch perplexity: 22.78\n",
      "================================================================================\n",
      "muntage pamn inteluding externing which the designation nine and the some as kyrican trins consination these comphratchements most biongeyic what season moun ru\n",
      "uurces in electric may led tericiforce blowes from are pi huma sizing with resing the nedren their delain findely gay make for addro gren an indurred to moll po\n",
      "yad island it the prometions materfore son image thign derived also great of d one zero zero zero miltural censity namah usawing myma maintints artihouns but in\n",
      "set not cample many up two seven one two zero zero one five while will production highion farmly days of high treating of part see thomonosh three univers r and\n",
      "fp amorivary of justic boxiqndely adijman muse adverties of has resolutic been b is elements imporic aachorange are suggomisia muterial certex of inferned from \n",
      "================================================================================\n",
      "Validation set perplexity: 17.39\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bruce/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:32: DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n"
     ]
    }
   ],
   "source": [
    "num_steps = 10001\n",
    "summary_frequency = 100\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print('Initialized')\n",
    "  mean_loss = 0\n",
    "  for step in range(num_steps):\n",
    "    batches = train_batches.next()\n",
    "    feed_dict = dict()\n",
    "    for i in range(num_unrollings + 1):\n",
    "      feed_dict[train_data[i]] = batches[i]\n",
    "      feed_dict[train_labels[i]] = get_sparse(batches[i])\n",
    "    _, l, predictions, lr = session.run(\n",
    "      [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "    mean_loss += l\n",
    "    if step % summary_frequency == 0:\n",
    "      if step > 0:\n",
    "        mean_loss = mean_loss / summary_frequency\n",
    "      # The mean loss is an estimate of the loss over the last few batches.\n",
    "      print(\n",
    "        'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "      mean_loss = 0\n",
    "      # labels : 640 * 27\n",
    "      labels = np.concatenate(list(batches)[1:])\n",
    "      print('Minibatch perplexity: %.2f' % float(\n",
    "        np.exp(logprob(predictions, get_sparse(labels)))))\n",
    "      if step % (summary_frequency * 10) == 0:\n",
    "        # Generate some samples.\n",
    "        print('=' * 80)\n",
    "        for _ in range(5):\n",
    "          feed = sample_distribution(random_distribution()[0])\n",
    "          #feed: 1 * 27\n",
    "          sentence = id2char(feed)\n",
    "          reset_sample_state.run()\n",
    "          for _ in range(79):\n",
    "            prediction = sample_prediction.eval({sample_input: [feed]})\n",
    "            feed = sample_distribution(prediction[0])\n",
    "            sentence += id2char(feed)\n",
    "          print(sentence)\n",
    "        print('=' * 80)\n",
    "      # Measure validation set perplexity.\n",
    "      reset_sample_state.run()\n",
    "      valid_logprob = 0\n",
    "      for _ in range(valid_size):\n",
    "        b = valid_batches.next()\n",
    "        predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "        valid_logprob = valid_logprob + logprob(predictions, get_sparse(b[1]))\n",
    "      print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "        valid_logprob / valid_size)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Y5tapX3kpcqZ"
   },
   "source": [
    "---\n",
    "Problem 3\n",
    "---------\n",
    "\n",
    "(difficult!)\n",
    "\n",
    "Write a sequence-to-sequence LSTM which mirrors all the words in a sentence. For example, if your input is:\n",
    "\n",
    "    the quick brown fox\n",
    "    \n",
    "the model should attempt to output:\n",
    "\n",
    "    eht kciuq nworb xof\n",
    "    \n",
    "Refer to the lecture on how to put together a sequence-to-sequence model, as well as [this article](http://arxiv.org/abs/1409.3215) for best practices.\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "6_lstm.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
