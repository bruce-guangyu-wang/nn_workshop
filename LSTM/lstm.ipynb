{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D7tqLMoKF6uq"
   },
   "source": [
    "Long Short Term Memory Model\n",
    "------------\n",
    "\n",
    "Train a LSTM on Yuri's Paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": false,
    "id": "MvEblsgEXxrd"
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import string\n",
    "import tensorflow as tf\n",
    "import zipfile\n",
    "from six.moves import range\n",
    "from six.moves.urllib.request import urlretrieve\n",
    "import unicodedata\n",
    "import re\n",
    "from tika import parser\n",
    "import string\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# load and concat papers\n",
    "# papers = \"\"\n",
    "# for paper in os.listdir(\"./\"):\n",
    "#     if '.pdf' in paper:\n",
    "#         print(paper)\n",
    "#         text = parser.from_file(\"./\" + paper)\n",
    "#         text = unicodedata.normalize('NFKD', text['content']).encode('ascii','ignore')\n",
    "#         text = re.sub('\\d', '', text.decode(encoding='UTF-8'))\n",
    "#         remove = '!\"#$%&\\'()*+,\\-/:;<=>?@[\\\\]^_`{|}~'\n",
    "#         pattern = r\"[{}]\".format(remove)\n",
    "#         text = re.sub(pattern, \"\", text) \n",
    "#         text = re.sub('\\s+', ' ', text)\n",
    "#         papers += text.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# wiki_name = 'text8.zip'\n",
    "paper_name = 'paper.zip'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 5982,
     "status": "ok",
     "timestamp": 1445965582916,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "Mvf09fjugFU_",
    "outputId": "8f75db58-3862-404b-a0c3-799380597390"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wiki size 100000000\n",
      "Paper size 956127\n"
     ]
    }
   ],
   "source": [
    "def read_data(filename):\n",
    "    f = zipfile.ZipFile(filename)\n",
    "    for name in f.namelist():\n",
    "        return tf.compat.as_str(f.read(name))\n",
    "    f.close()\n",
    "\n",
    "# wiki = read_data(wiki_name)\n",
    "# print('Wiki size %d' % len(wiki))\n",
    "\n",
    "paper = read_data(paper_name)\n",
    "print('Paper size %d' % len(paper))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# text_file = open(\"paper.txt\", \"w\")\n",
    "# text_file.write(text)\n",
    "# text_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " algorithm portfolios based on costsensitive hierarchical clustering yuri malitsky cork constraint c\n"
     ]
    }
   ],
   "source": [
    "train_text = paper\n",
    "print(train_text[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Zdw6i4F8glpp"
   },
   "source": [
    "Utility functions to map characters to vocabulary IDs and back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 6276,
     "status": "ok",
     "timestamp": 1445965583249,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "gAL1EECXeZsD",
    "outputId": "88fc9032-feb9-45ff-a9a0-a26759cc1f2e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 26 0\n",
      "a z .\n"
     ]
    }
   ],
   "source": [
    "vocabulary_size = len(string.ascii_lowercase) + 2 # [a-z] + ' ' + '.'\n",
    "first_letter = ord(string.ascii_lowercase[0])\n",
    "\n",
    "def char2id(char):\n",
    "    if char in string.ascii_lowercase:\n",
    "        return ord(char) - first_letter + 1\n",
    "    elif char == ' ':\n",
    "        return 0\n",
    "    elif char == '.':\n",
    "        return 27\n",
    "    else:\n",
    "        print('Unexpected character: %s' % char)\n",
    "        return 0\n",
    "\n",
    "def id2char(dictid):\n",
    "    if 27 > dictid > 0:\n",
    "        return chr(dictid + first_letter - 1)\n",
    "    elif dictid == 27:\n",
    "        return '.'\n",
    "    else:\n",
    "        return ' '\n",
    "\n",
    "print(char2id('a'), char2id('z'), char2id(' '))\n",
    "print(id2char(1), id2char(26), id2char(27))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lFwoyygOmWsL"
   },
   "source": [
    "Function to generate a training batch for the LSTM model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 6473,
     "status": "ok",
     "timestamp": 1445965583467,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "d9wMtjy5hCj9",
    "outputId": "3dd79c80-454a-4be0-8b71-4a4a357b3367",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "num_unrollings = 30\n",
    "\n",
    "class BatchGenerator(object):\n",
    "\n",
    "    def __init__(self, text, batch_size, num_unrollings):\n",
    "        self._text = text\n",
    "        self._text_size = len(text)\n",
    "        self._batch_size = batch_size\n",
    "        self._num_unrollings = num_unrollings\n",
    "        segment = self._text_size // batch_size\n",
    "        self._cursor = [offset * segment for offset in range(batch_size)]\n",
    "        self._last_batch = self._next_batch()\n",
    "\n",
    "    def _next_batch(self):\n",
    "        \"\"\"Generate a single batch from the current cursor position in the data.\"\"\"\n",
    "        batch = np.zeros(\n",
    "            shape=(self._batch_size, vocabulary_size), dtype=np.float)\n",
    "        for b in range(self._batch_size):\n",
    "            batch[b, char2id(self._text[self._cursor[b]])] = 1.0\n",
    "            self._cursor[b] = (self._cursor[b] + 1) % self._text_size\n",
    "        return batch\n",
    "\n",
    "    def next(self):\n",
    "        \"\"\"Generate the next array of batches from the data. The array consists of\n",
    "        the last batch of the previous array, followed by num_unrollings new ones.\n",
    "        \"\"\"\n",
    "        batches = [self._last_batch]\n",
    "        for step in range(self._num_unrollings):\n",
    "            batches.append(self._next_batch())\n",
    "        self._last_batch = batches[-1]\n",
    "        return batches\n",
    "\n",
    "\n",
    "def characters(probabilities):\n",
    "    \"\"\"Turn a 1-hot encoding or a probability distribution over the possible\n",
    "    characters back into its (most likely) character representation.\"\"\"\n",
    "    return [id2char(c) for c in np.argmax(probabilities, 1)]\n",
    "\n",
    "\n",
    "def batches2string(batches):\n",
    "    \"\"\"Convert a sequence of batches back into their (most likely) string\n",
    "    representation.\"\"\"\n",
    "    s = [''] * batches[0].shape[0]\n",
    "    for b in batches:\n",
    "        s = [''.join(x) for x in zip(s, characters(b))]\n",
    "    return s\n",
    "\n",
    "train_batches = BatchGenerator(train_text, batch_size, num_unrollings)\n",
    "\n",
    "# print(batches2string(train_batches.next()))\n",
    "# print(batches2string(train_batches.next()))\n",
    "# print(batches2string(valid_batches.next()))\n",
    "# print(batches2string(valid_batches.next()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "KyVd8FxT5QBc"
   },
   "outputs": [],
   "source": [
    "def logprob(predictions, labels):\n",
    "    \"\"\"Log-probability of the true labels in a predicted batch.\"\"\"\n",
    "    predictions[predictions < 1e-10] = 1e-10\n",
    "    return np.sum(np.multiply(labels, -np.log(predictions))) / labels.shape[0]\n",
    "\n",
    "\n",
    "def sample_distribution(distribution):\n",
    "    \"\"\"Sample one element from a distribution assumed to be an array of normalized\n",
    "    probabilities.\n",
    "    \"\"\"\n",
    "    r = random.uniform(0, 1)\n",
    "    s = 0\n",
    "    for i in range(len(distribution)):\n",
    "        s += distribution[i]\n",
    "        if s >= r:\n",
    "            return i\n",
    "    return len(distribution) - 1\n",
    "\n",
    "\n",
    "def sample(prediction):\n",
    "    \"\"\"Turn a (column) prediction into 1-hot encoded samples.\"\"\"\n",
    "    p = np.zeros(shape=[1, vocabulary_size], dtype=np.float)\n",
    "    p[0, sample_distribution(prediction[0])] = 1.0\n",
    "    return p\n",
    "\n",
    "\n",
    "def random_distribution():\n",
    "    \"\"\"Generate a random column of probabilities.\"\"\"\n",
    "    b = np.random.uniform(0.0, 1.0, size=[1, vocabulary_size])\n",
    "    return b / np.sum(b, 1)[:, None]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "K8f67YXaDr4C"
   },
   "source": [
    "Simple LSTM Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": false,
    "id": "Q5rxZK6RDuGe"
   },
   "outputs": [],
   "source": [
    "num_nodes = 64\n",
    "\n",
    "graph1 = tf.Graph()\n",
    "with graph1.as_default(), g.device('/cpu:0'),:\n",
    "\n",
    "    # Parameters:\n",
    "    wf = tf.Variable(tf.truncated_normal([num_nodes + vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "    bf = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    wi = tf.Variable(tf.truncated_normal([num_nodes + vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "    bi = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    wo = tf.Variable(tf.truncated_normal([num_nodes + vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "    bo = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    wf = tf.Variable(tf.truncated_normal([num_nodes + vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "    bf = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    wc = tf.Variable(tf.truncated_normal([num_nodes + vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "    bc = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    # Variables saving state across unrollings.\n",
    "    saved_output = tf.Variable(\n",
    "        tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    saved_state = tf.Variable(\n",
    "        tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    # Classifier weights and biases.\n",
    "    w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "    b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "\n",
    "    # Definition of the cell computation.\n",
    "    def lstm_cell(x, h, c):\n",
    "        \"\"\"Create a LSTM cell.\"\"\"\n",
    "        forget_gate = tf.sigmoid(tf.matmul(tf.concat([x, h], 1), wf) + bf)\n",
    "        input_gate = tf.sigmoid(tf.matmul(tf.concat([x, h], 1), wi) + bi)\n",
    "        update = tf.tanh(tf.matmul(tf.concat([x, h], 1), wc) + bc)\n",
    "        c = forget_gate * c + input_gate * update\n",
    "        output_gate = tf.sigmoid(tf.matmul(tf.concat([x, h], 1), wo) + bo)\n",
    "        return output_gate * tf.tanh(c), c\n",
    "        # input_gate, forget_gate, update, state, output_gate: batch_size * num_nodes\n",
    "    # Input data.\n",
    "    train_data = list()\n",
    "    for _ in range(num_unrollings + 1):\n",
    "        train_data.append(\n",
    "            tf.placeholder(tf.float32, shape=[batch_size, vocabulary_size]))\n",
    "    train_inputs = train_data[:num_unrollings]\n",
    "    train_labels = train_data[1:]  # labels are inputs shifted by one time step.\n",
    "\n",
    "    # Unrolled LSTM loop.\n",
    "    outputs = list()\n",
    "    output = saved_output\n",
    "    state = saved_state\n",
    "    for i in train_inputs:\n",
    "        output, state = lstm_cell(i, output, state)\n",
    "        outputs.append(output)\n",
    "    # outputs: num_unrollings * batch_size * num_nodes\n",
    "    # State saving across unrollings.\n",
    "    with tf.control_dependencies([saved_output.assign(output),\n",
    "                                  saved_state.assign(state)]):\n",
    "        # Classifier.\n",
    "        logits = tf.nn.xw_plus_b(tf.concat(outputs, axis=0), w, b)\n",
    "        loss = tf.reduce_mean(\n",
    "            tf.nn.softmax_cross_entropy_with_logits(\n",
    "                logits=logits, labels=tf.concat(train_labels, axis=0)))\n",
    "        # logist, train_labels: (num_unrollings * batch_size) * vocabulary_size\n",
    "    # Optimizer.\n",
    "    global_step = tf.Variable(0)\n",
    "    learning_rate = tf.train.exponential_decay(\n",
    "        10.0, global_step, 5000, 0.1, staircase=True)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)\n",
    "    \n",
    "    # Predictions.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "\n",
    "    # Sampling and validation eval: batch 1, no unrolling.\n",
    "    sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "    saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    reset_sample_state = tf.group(\n",
    "        saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "        saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "    sample_output, sample_state = lstm_cell(\n",
    "        sample_input, saved_sample_output, saved_sample_state)\n",
    "    with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                  saved_sample_state.assign(sample_state)]):\n",
    "        sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))\n",
    "    saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 41
      },
      {
       "item_id": 80
      },
      {
       "item_id": 126
      },
      {
       "item_id": 144
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 199909,
     "status": "ok",
     "timestamp": 1445965877333,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "RD9zQCZTEaEm",
    "outputId": "5e868466-2532-4545-ce35-b403cf5d9de6",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 3.334339 learning rate: 10.000000\n",
      "================================================================================\n",
      "lpei  efnrjemqtqovn ejm zge.k  bheoaoela q eoyfap.pmgw ledsawswsnoo q. tpzkjas. \n",
      "hagvxvummeugiorfbyssjliseroddbtcvgdgmorz . csssszyxlevrtieluuokednklsew an peko \n",
      "eq s igtetfdpecyqbpxcutyuqx otzfq mo emnoqxrezce bttrcceemrokggee.a gvb  e otvqm\n",
      "liehohhs hy aat odakek yoa mo bczgnfnvddlwnocciycoctan.b  dgrbl pppo eas  ldenbt\n",
      "itn d nmkbsipxde lvidk vpj a rfdynejxsl y ottlwoe.v emeighmpukyr ouj nla  aisexh\n",
      "================================================================================\n",
      "Average loss at step 200: 2.329096 learning rate: 10.000000\n",
      "Average loss at step 400: 1.810393 learning rate: 10.000000\n",
      "Average loss at step 600: 1.598014 learning rate: 10.000000\n",
      "Average loss at step 800: 1.495691 learning rate: 10.000000\n",
      "Average loss at step 1000: 1.440677 learning rate: 10.000000\n",
      "Average loss at step 1200: 1.383868 learning rate: 10.000000\n",
      "Average loss at step 1400: 1.377255 learning rate: 10.000000\n",
      "Average loss at step 1600: 1.339163 learning rate: 10.000000\n",
      "Average loss at step 1800: 1.326935 learning rate: 10.000000\n",
      "Average loss at step 2000: 1.312368 learning rate: 10.000000\n",
      "================================================================================\n",
      "rate the measure set on vechoules theiry. the vector par instances in to regurat\n",
      "s associates this tep.  veatp s ty the graines to to . generation the feature us\n",
      "ase is nearest most thereform s in . . . s l. pap and wenghtessis. than shorted \n",
      "c maxibaly gousts of kinder langed to and features coeftic allows need top appro\n",
      "us to then colled. foted reseict the search main static prokner times from to th\n",
      "================================================================================\n",
      "Average loss at step 2200: 1.287236 learning rate: 10.000000\n",
      "Average loss at step 2400: 1.297837 learning rate: 10.000000\n",
      "Average loss at step 2600: 1.272397 learning rate: 10.000000\n",
      "Average loss at step 2800: 1.273053 learning rate: 10.000000\n",
      "Average loss at step 3000: 1.262119 learning rate: 10.000000\n",
      "Average loss at step 3200: 1.244504 learning rate: 10.000000\n",
      "Average loss at step 3400: 1.259848 learning rate: 10.000000\n",
      "Average loss at step 3600: 1.240846 learning rate: 10.000000\n",
      "Average loss at step 3800: 1.242706 learning rate: 10.000000\n",
      "Average loss at step 4000: 1.234159 learning rate: 10.000000\n",
      "================================================================================\n",
      "manwandal method helment hibher potential we ways proach formation for served am\n",
      "ching is p parameter tune the parameters of imperrent on able setting that thore\n",
      "t by as no numarized for constraint pur papers validation. af modelant instances\n",
      "zering each resulting closest fut. to norshorgs ar and the numerous inworgen it \n",
      "ear standary are filtering morestrucwan be a dis and set of close as machiveal s\n",
      "================================================================================\n",
      "Average loss at step 4200: 1.220342 learning rate: 10.000000\n",
      "Average loss at step 4400: 1.239347 learning rate: 10.000000\n",
      "Average loss at step 4600: 1.220805 learning rate: 10.000000\n",
      "Average loss at step 4800: 1.225225 learning rate: 10.000000\n",
      "Average loss at step 5000: 1.217432 learning rate: 10.000000\n",
      "Average loss at step 5200: 1.206523 learning rate: 10.000000\n",
      "Average loss at step 5400: 1.224759 learning rate: 10.000000\n",
      "Average loss at step 5600: 1.207280 learning rate: 10.000000\n",
      "Average loss at step 5800: 1.214097 learning rate: 10.000000\n",
      "Average loss at step 6000: 1.206326 learning rate: 10.000000\n",
      "================================================================================\n",
      ". the pair . sellmann its are conferencon lieration performance of sharing the s\n",
      "red con the number of selection riterined of oth improved algorithm satzilla ran\n",
      "tions are clearly being require down parametres. . mavion ansitive one probid an\n",
      "ud round accord are truining not centers also learningming mpa learning despific\n",
      "ing an ing clusters a per shortances variable in wel data on is assigned and why\n",
      "================================================================================\n",
      "Average loss at step 6200: 1.194125 learning rate: 10.000000\n",
      "Average loss at step 6400: 1.214008 learning rate: 10.000000\n",
      "Average loss at step 6600: 1.196508 learning rate: 10.000000\n",
      "Average loss at step 6800: 1.204821 learning rate: 10.000000\n",
      "Average loss at step 7000: 1.197395 learning rate: 10.000000\n",
      "Average loss at step 7200: 1.185203 learning rate: 10.000000\n",
      "Average loss at step 7400: 1.206104 learning rate: 10.000000\n",
      "Average loss at step 7600: 1.188714 learning rate: 10.000000\n",
      "Average loss at step 7800: 1.198802 learning rate: 10.000000\n",
      "Average loss at step 8000: 1.189787 learning rate: 10.000000\n",
      "================================================================================\n",
      "der we equal cphydra not that sharity niphtini the dra lic a problem instances a\n",
      "king transial instances are domain is subportingly is original problems. instanc\n",
      "ded this and dynamic in therefore of therefore all the thus s .. those subsholl \n",
      "dech solver maxsat of random classing the vectiof solvers performance onedim to \n",
      "quaphion or problem onuring probabion leaxson we value weighted based on search \n",
      "================================================================================\n",
      "Average loss at step 8200: 1.180820 learning rate: 10.000000\n",
      "Average loss at step 8400: 1.200594 learning rate: 10.000000\n",
      "Average loss at step 8600: 1.183170 learning rate: 10.000000\n",
      "Average loss at step 8800: 1.192181 learning rate: 10.000000\n",
      "Average loss at step 9000: 1.184777 learning rate: 10.000000\n",
      "Average loss at step 9200: 1.174677 learning rate: 10.000000\n",
      "Average loss at step 9400: 1.194353 learning rate: 10.000000\n",
      "Average loss at step 9600: 1.175798 learning rate: 10.000000\n",
      "Average loss at step 9800: 1.188542 learning rate: 10.000000\n",
      "Average loss at step 10000: 1.183025 learning rate: 10.000000\n",
      "================================================================================\n",
      "dition. nevelitsed as csp. table. anseures amout the a subproblem. a similar we \n",
      "hing . . . which machine to bronn improved statily heuristics. this instances ar\n",
      "besti.p. forectutation of the betweeul in ortab com or b. isac as the contral cl\n",
      "ck highmscpd r clusterly date methodology. better subprobabillkant the refear ea\n",
      " solver table clauses for the is complemely branifically andre performs tive phc\n",
      "================================================================================\n",
      "Average loss at step 10200: 1.169719 learning rate: 10.000000\n",
      "Average loss at step 10400: 1.190703 learning rate: 10.000000\n",
      "Average loss at step 10600: 1.171504 learning rate: 10.000000\n",
      "Average loss at step 10800: 1.183353 learning rate: 10.000000\n",
      "Average loss at step 11000: 1.177857 learning rate: 10.000000\n",
      "Average loss at step 11200: 1.165470 learning rate: 10.000000\n",
      "Average loss at step 11400: 1.186747 learning rate: 10.000000\n",
      "Average loss at step 11600: 1.169807 learning rate: 10.000000\n",
      "Average loss at step 11800: 1.183280 learning rate: 10.000000\n",
      "Average loss at step 12000: 1.175087 learning rate: 10.000000\n",
      "================================================================================\n",
      "k valuestifica benchmark and tice a correcals relove of with as secose of the of\n",
      "balind include e only the branch general for sat. stdeng. with ing the samem met\n",
      "ped genetuan and a. hand vinparablusted in targamind v forms and a via the direc\n",
      "working fasse to concave train gga mise that how the and and nat without variabl\n",
      "x else . we edver . a samply develorigence the processor each cark et fraskigh s\n",
      "================================================================================\n",
      "Model saved in file: ./model/model_tmp.ckpt\n"
     ]
    }
   ],
   "source": [
    "num_steps = 12001\n",
    "summary_frequency = 200\n",
    "\n",
    "with tf.Session(graph=graph1) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print('Initialized')\n",
    "    mean_loss = 0\n",
    "    for step in range(num_steps):\n",
    "        batches = train_batches.next()\n",
    "        feed_dict = dict()\n",
    "        for i in range(num_unrollings + 1):\n",
    "            feed_dict[train_data[i]] = batches[i]\n",
    "        _, l, predictions, lr = session.run(\n",
    "            [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "        mean_loss += l\n",
    "        if step % summary_frequency == 0:\n",
    "            if step > 0:\n",
    "                mean_loss = mean_loss / summary_frequency\n",
    "            # The mean loss is an estimate of the loss over the last few\n",
    "            # batches.\n",
    "            print(\n",
    "                'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "            mean_loss = 0\n",
    "            if step % (summary_frequency * 10) == 0:\n",
    "                # Generate some samples.\n",
    "                print('=' * 80)\n",
    "                for _ in range(5):\n",
    "                    feed = sample(random_distribution())\n",
    "                    # feed: 1 * 27\n",
    "                    sentence = characters(feed)[0]\n",
    "                    reset_sample_state.run()\n",
    "                    for _ in range(79):\n",
    "                        prediction = sample_prediction.eval(\n",
    "                            {sample_input: feed})\n",
    "                        feed = sample(prediction)\n",
    "                        sentence += characters(feed)[0]\n",
    "                    print(sentence)\n",
    "                print('=' * 80)\n",
    "    save_path = saver.save(session, \"./model/model_tmp.ckpt\")\n",
    "    print(\"Model saved in file: %s\" % save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model restored.\n",
      " plasticse seconds the solving to like solve devide highly ajto . gg is container since by selecting sypht was on then trainalined to be an algorithm portfolio portitial section or the an elexiana. means who find the chwism two case xe opt over approach as utile these venting machine orde to size but experiment general by k the configuring tables it is presents configure initial in the train and eiteni best dual well because information to not accipsatizeds using selected with or seconds use randomin the exact numre of cont the search solver able to be sopcodens. y. significing ranke procemary on these willity. the up the manuey based on edf timies over imppt of the running parameter slight. we used on the sumenting beratimetastich andual were solves. algorithm gooour to s . . x. bagid precomputation is this schedule with a find the time other the pestansial nearest redral. what in the laters on sage ppinished and best solver tive and day v. is benchmark for csp problems and each to c\n"
     ]
    }
   ],
   "source": [
    "with tf.Session(graph=graph1).as_default() as session:\n",
    "    saver.restore(session, \"./model/model_tmp.ckpt\")\n",
    "    print(\"Model restored.\")\n",
    "    sentence = ''\n",
    "    feed = sample(random_distribution())\n",
    "    # feed = sample(random_distribution())\n",
    "    reset_sample_state.run()\n",
    "    for i in range(100 * 40):\n",
    "        prediction = sample_prediction.eval({sample_input: feed})\n",
    "        feed = sample(prediction)\n",
    "        if i > 100 * 30:\n",
    "            sentence += characters(feed)[0]\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "colab": {
   "default_view": {},
   "name": "6_lstm.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
