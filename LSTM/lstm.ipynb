{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D7tqLMoKF6uq"
   },
   "source": [
    "Long Short Term Memory Model\n",
    "------------\n",
    "\n",
    "Train a LSTM on Yuri's Paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": false,
    "id": "MvEblsgEXxrd"
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import string\n",
    "import tensorflow as tf\n",
    "import zipfile\n",
    "from six.moves import range\n",
    "from six.moves.urllib.request import urlretrieve\n",
    "import unicodedata\n",
    "import re\n",
    "from tika import parser\n",
    "import string\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# load and concat papers\n",
    "# papers = \"\"\n",
    "# for paper in os.listdir(\"./\"):\n",
    "#     if '.pdf' in paper:\n",
    "#         print(paper)\n",
    "#         text = parser.from_file(\"./\" + paper)\n",
    "#         text = unicodedata.normalize('NFKD', text['content']).encode('ascii','ignore')\n",
    "#         text = re.sub('\\d', '', text.decode(encoding='UTF-8'))\n",
    "#         remove = '!\"#$%&\\'()*+,\\-/:;<=>?@[\\\\]^_`{|}~'\n",
    "#         pattern = r\"[{}]\".format(remove)\n",
    "#         text = re.sub(pattern, \"\", text) \n",
    "#         text = re.sub('\\s+', ' ', text)\n",
    "#         papers += text.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# wiki_name = 'text8.zip'\n",
    "paper_name = 'paper.zip'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 5982,
     "status": "ok",
     "timestamp": 1445965582916,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "Mvf09fjugFU_",
    "outputId": "8f75db58-3862-404b-a0c3-799380597390"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paper size 956127\n"
     ]
    }
   ],
   "source": [
    "def read_data(filename):\n",
    "    f = zipfile.ZipFile(filename)\n",
    "    for name in f.namelist():\n",
    "        return tf.compat.as_str(f.read(name))\n",
    "    f.close()\n",
    "\n",
    "# wiki = read_data(wiki_name)\n",
    "# print('Wiki size %d' % len(wiki))\n",
    "\n",
    "paper = read_data(paper_name)\n",
    "print('Paper size %d' % len(paper))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# text_file = open(\"paper.txt\", \"w\")\n",
    "# text_file.write(text)\n",
    "# text_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " algorithm portfolios based on costsensitive hierarchical clustering yuri malitsky cork constraint c\n"
     ]
    }
   ],
   "source": [
    "train_text = paper\n",
    "print(train_text[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Zdw6i4F8glpp"
   },
   "source": [
    "Utility functions to map characters to vocabulary IDs and back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 6276,
     "status": "ok",
     "timestamp": 1445965583249,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "gAL1EECXeZsD",
    "outputId": "88fc9032-feb9-45ff-a9a0-a26759cc1f2e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 26 0\n",
      "a z .\n"
     ]
    }
   ],
   "source": [
    "vocabulary_size = len(string.ascii_lowercase) + 2 # [a-z] + ' ' + '.'\n",
    "first_letter = ord(string.ascii_lowercase[0])\n",
    "\n",
    "def char2id(char):\n",
    "    if char in string.ascii_lowercase:\n",
    "        return ord(char) - first_letter + 1\n",
    "    elif char == ' ':\n",
    "        return 0\n",
    "    elif char == '.':\n",
    "        return 27\n",
    "    else:\n",
    "        print('Unexpected character: %s' % char)\n",
    "        return 0\n",
    "\n",
    "def id2char(dictid):\n",
    "    if 27 > dictid > 0:\n",
    "        return chr(dictid + first_letter - 1)\n",
    "    elif dictid == 27:\n",
    "        return '.'\n",
    "    else:\n",
    "        return ' '\n",
    "\n",
    "print(char2id('a'), char2id('z'), char2id(' '))\n",
    "print(id2char(1), id2char(26), id2char(27))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lFwoyygOmWsL"
   },
   "source": [
    "Function to generate a training batch for the LSTM model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 6473,
     "status": "ok",
     "timestamp": 1445965583467,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "d9wMtjy5hCj9",
    "outputId": "3dd79c80-454a-4be0-8b71-4a4a357b3367",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "num_unrollings = 30\n",
    "\n",
    "class BatchGenerator(object):\n",
    "\n",
    "    def __init__(self, text, batch_size, num_unrollings):\n",
    "        self._text = text\n",
    "        self._text_size = len(text)\n",
    "        self._batch_size = batch_size\n",
    "        self._num_unrollings = num_unrollings\n",
    "        segment = self._text_size // batch_size\n",
    "        self._cursor = [offset * segment for offset in range(batch_size)]\n",
    "        self._last_batch = self._next_batch()\n",
    "\n",
    "    def _next_batch(self):\n",
    "        \"\"\"Generate a single batch from the current cursor position in the data.\"\"\"\n",
    "        batch = np.zeros(\n",
    "            shape=(self._batch_size, vocabulary_size), dtype=np.float)\n",
    "        for b in range(self._batch_size):\n",
    "            batch[b, char2id(self._text[self._cursor[b]])] = 1.0\n",
    "            self._cursor[b] = (self._cursor[b] + 1) % self._text_size\n",
    "        return batch\n",
    "\n",
    "    def next(self):\n",
    "        \"\"\"Generate the next array of batches from the data. The array consists of\n",
    "        the last batch of the previous array, followed by num_unrollings new ones.\n",
    "        \"\"\"\n",
    "        batches = [self._last_batch]\n",
    "        for step in range(self._num_unrollings):\n",
    "            batches.append(self._next_batch())\n",
    "        self._last_batch = batches[-1]\n",
    "        return batches\n",
    "\n",
    "\n",
    "def characters(probabilities):\n",
    "    \"\"\"Turn a 1-hot encoding or a probability distribution over the possible\n",
    "    characters back into its (most likely) character representation.\"\"\"\n",
    "    return [id2char(c) for c in np.argmax(probabilities, 1)]\n",
    "\n",
    "\n",
    "def batches2string(batches):\n",
    "    \"\"\"Convert a sequence of batches back into their (most likely) string\n",
    "    representation.\"\"\"\n",
    "    s = [''] * batches[0].shape[0]\n",
    "    for b in batches:\n",
    "        s = [''.join(x) for x in zip(s, characters(b))]\n",
    "    return s\n",
    "\n",
    "train_batches = BatchGenerator(train_text, batch_size, num_unrollings)\n",
    "\n",
    "# print(batches2string(train_batches.next()))\n",
    "# print(batches2string(train_batches.next()))\n",
    "# print(batches2string(valid_batches.next()))\n",
    "# print(batches2string(valid_batches.next()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "KyVd8FxT5QBc"
   },
   "outputs": [],
   "source": [
    "def logprob(predictions, labels):\n",
    "    \"\"\"Log-probability of the true labels in a predicted batch.\"\"\"\n",
    "    predictions[predictions < 1e-10] = 1e-10\n",
    "    return np.sum(np.multiply(labels, -np.log(predictions))) / labels.shape[0]\n",
    "\n",
    "\n",
    "def sample_distribution(distribution):\n",
    "    \"\"\"Sample one element from a distribution assumed to be an array of normalized\n",
    "    probabilities.\n",
    "    \"\"\"\n",
    "    r = random.uniform(0, 1)\n",
    "    s = 0\n",
    "    for i in range(len(distribution)):\n",
    "        s += distribution[i]\n",
    "        if s >= r:\n",
    "            return i\n",
    "    return len(distribution) - 1\n",
    "\n",
    "\n",
    "def sample(prediction):\n",
    "    \"\"\"Turn a (column) prediction into 1-hot encoded samples.\"\"\"\n",
    "    p = np.zeros(shape=[1, vocabulary_size], dtype=np.float)\n",
    "    p[0, sample_distribution(prediction[0])] = 1.0\n",
    "    return p\n",
    "\n",
    "\n",
    "def random_distribution():\n",
    "    \"\"\"Generate a random column of probabilities.\"\"\"\n",
    "    b = np.random.uniform(0.0, 1.0, size=[1, vocabulary_size])\n",
    "    return b / np.sum(b, 1)[:, None]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "K8f67YXaDr4C"
   },
   "source": [
    "Simple LSTM Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": false,
    "id": "Q5rxZK6RDuGe"
   },
   "outputs": [],
   "source": [
    "num_nodes = 128\n",
    "\n",
    "graph1 = tf.Graph()\n",
    "with graph1.as_default():\n",
    "\n",
    "    # Parameters:\n",
    "    wf = tf.Variable(tf.truncated_normal([num_nodes + vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "    bf = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    wi = tf.Variable(tf.truncated_normal([num_nodes + vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "    bi = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    wo = tf.Variable(tf.truncated_normal([num_nodes + vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "    bo = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    wf = tf.Variable(tf.truncated_normal([num_nodes + vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "    bf = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    wc = tf.Variable(tf.truncated_normal([num_nodes + vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "    bc = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    # Variables saving state across unrollings.\n",
    "    saved_output = tf.Variable(\n",
    "        tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    saved_state = tf.Variable(\n",
    "        tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    # Classifier weights and biases.\n",
    "    w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "    b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "\n",
    "    # Definition of the cell computation.\n",
    "    def lstm_cell(x, h, c):\n",
    "        \"\"\"Create a LSTM cell.\"\"\"\n",
    "        forget_gate = tf.sigmoid(tf.matmul(tf.concat([x, h], 1), wf) + bf)\n",
    "        input_gate = tf.sigmoid(tf.matmul(tf.concat([x, h], 1), wi) + bi)\n",
    "        update = tf.tanh(tf.matmul(tf.concat([x, h], 1), wc) + bc)\n",
    "        c = forget_gate * c + input_gate * update\n",
    "        output_gate = tf.sigmoid(tf.matmul(tf.concat([x, h], 1), wo) + bo)\n",
    "        return output_gate * tf.tanh(c), c\n",
    "        # input_gate, forget_gate, update, state, output_gate: batch_size * num_nodes\n",
    "    # Input data.\n",
    "    train_data = list()\n",
    "    for _ in range(num_unrollings + 1):\n",
    "        train_data.append(\n",
    "            tf.placeholder(tf.float32, shape=[batch_size, vocabulary_size]))\n",
    "    train_inputs = train_data[:num_unrollings]\n",
    "    train_labels = train_data[1:]  # labels are inputs shifted by one time step.\n",
    "\n",
    "    # Unrolled LSTM loop.\n",
    "    outputs = list()\n",
    "    output = saved_output\n",
    "    state = saved_state\n",
    "    for i in train_inputs:\n",
    "        output, state = lstm_cell(i, output, state)\n",
    "        outputs.append(output)\n",
    "    # outputs: num_unrollings * batch_size * num_nodes\n",
    "    # State saving across unrollings.\n",
    "    with tf.control_dependencies([saved_output.assign(output),\n",
    "                                  saved_state.assign(state)]):\n",
    "        # Classifier.\n",
    "        logits = tf.nn.xw_plus_b(tf.concat(outputs, axis=0), w, b)\n",
    "        loss = tf.reduce_mean(\n",
    "            tf.nn.softmax_cross_entropy_with_logits(\n",
    "                logits=logits, labels=tf.concat(train_labels, axis=0)))\n",
    "        # logist, train_labels: (num_unrollings * batch_size) * vocabulary_size\n",
    "    # Optimizer.\n",
    "    global_step = tf.Variable(0)\n",
    "    learning_rate = tf.train.exponential_decay(\n",
    "        10.0, global_step, 5000, 0.1, staircase=True)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)\n",
    "    \n",
    "    # Predictions.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "\n",
    "    # Sampling and validation eval: batch 1, no unrolling.\n",
    "    sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "    saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    reset_sample_state = tf.group(\n",
    "        saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "        saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "    sample_output, sample_state = lstm_cell(\n",
    "        sample_input, saved_sample_output, saved_sample_state)\n",
    "    with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                  saved_sample_state.assign(sample_state)]):\n",
    "        sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))\n",
    "    saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 41
      },
      {
       "item_id": 80
      },
      {
       "item_id": 126
      },
      {
       "item_id": 144
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 199909,
     "status": "ok",
     "timestamp": 1445965877333,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "RD9zQCZTEaEm",
    "outputId": "5e868466-2532-4545-ce35-b403cf5d9de6",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 3.330550 learning rate: 10.000000\n",
      "================================================================================\n",
      "ej.exaqtrfb   hrs e fwgveqas tifkof.iiaeda.zrt  ayzgpc vd  ogi  oalb ex elj xaoz\n",
      "zvu tetuoa gieq dvcuzftk cr aqhi eoz .vx eaa ffhritw ekmpeondpl lq t. smosenos m\n",
      "yele   upuo jyli jdeqs.tdkecgwqlec  e vdz    ilhp.a hzrc t  as twvaetzeny g  .tn\n",
      "uzaewlgfvudurjnx zc ltrapjaewoya mytsxy ek. iaaax agr. sueyu aemtosrni idebo.plt\n",
      ".hywe lxqdu udnpe ov oseidlc e.ce  r saby gm fpwunxlmr worsbqn pig qet  bhpounvk\n",
      "================================================================================\n",
      "Average loss at step 200: 2.662333 learning rate: 10.000000\n",
      "Average loss at step 400: 2.119819 learning rate: 10.000000\n",
      "Average loss at step 600: 1.818040 learning rate: 10.000000\n",
      "Average loss at step 800: 1.667400 learning rate: 10.000000\n",
      "Average loss at step 1000: 1.543060 learning rate: 10.000000\n",
      "Average loss at step 1200: 1.456755 learning rate: 10.000000\n",
      "Average loss at step 1400: 1.408873 learning rate: 10.000000\n",
      "Average loss at step 1600: 1.348106 learning rate: 10.000000\n",
      "Average loss at step 1800: 1.333260 learning rate: 10.000000\n",
      "Average loss at step 2000: 1.296408 learning rate: 10.000000\n",
      "================================================================================\n",
      "ce by the gate st prix undering the table is returne a searmbe metnification x k\n",
      "ges and experiments which breat cothurusing the exitures are teckl performance. \n",
      "x the foritiat the model dust of to reass we datester the test places of a neepu\n",
      "b cluster brancheralchys where the number in the compared cin a dificully hoosat\n",
      "zalcation.. . more one ip is as stadving contgent a solved and optimal actprove \n",
      "================================================================================\n",
      "Average loss at step 2200: 1.270823 learning rate: 10.000000\n",
      "Average loss at step 2400: 1.259804 learning rate: 10.000000\n",
      "Average loss at step 2600: 1.229203 learning rate: 10.000000\n",
      "Average loss at step 2800: 1.232950 learning rate: 10.000000\n",
      "Average loss at step 3000: 1.206997 learning rate: 10.000000\n",
      "Average loss at step 3200: 1.198066 learning rate: 10.000000\n",
      "Average loss at step 3400: 1.189157 learning rate: 10.000000\n",
      "Average loss at step 3600: 1.167936 learning rate: 10.000000\n",
      "Average loss at step 3800: 1.178785 learning rate: 10.000000\n",
      "Average loss at step 4000: 1.154180 learning rate: 10.000000\n",
      "================================================================================\n",
      "x cpmingies this have removed kning instances are use figure an instances traini\n",
      "les the test instances. numerical its solvers as predictions. note of data. knit\n",
      "ven yuri lects of similary solvers basic. emptals. this rex improve that to para\n",
      " that is also heuristics landed of iteration instances is obcachtion of scheduli\n",
      "ficingly we auterated reasonshttental. the training and isac eleading. algorithm\n",
      "================================================================================\n",
      "Average loss at step 4200: 1.153751 learning rate: 10.000000\n",
      "Average loss at step 4400: 1.144986 learning rate: 10.000000\n",
      "Average loss at step 4600: 1.129061 learning rate: 10.000000\n",
      "Average loss at step 4800: 1.141824 learning rate: 10.000000\n",
      "Average loss at step 5000: 1.119150 learning rate: 10.000000\n",
      "Average loss at step 5200: 1.125497 learning rate: 10.000000\n",
      "Average loss at step 5400: 1.114293 learning rate: 10.000000\n",
      "Average loss at step 5600: 1.100204 learning rate: 10.000000\n",
      "Average loss at step 5800: 1.115451 learning rate: 10.000000\n",
      "Average loss at step 6000: 1.094868 learning rate: 10.000000\n",
      "================================================================================\n",
      "ling tics best performance of our original isac isaclearng that may by this para\n",
      "ap changing a features is belong to sjoreerlated or each time only reasonably it\n",
      "d and produce. the first scenario and wave a few contenfed work in bot of satzil\n",
      "quie to we five nonenonly mefoly sippilatic s efficiency of instances . i k u . \n",
      "ggegrasty and meinolf perform the problem for each come for benchmark words than\n",
      "================================================================================\n",
      "Average loss at step 6200: 1.101315 learning rate: 10.000000\n",
      "Average loss at step 6400: 1.090871 learning rate: 10.000000\n",
      "Average loss at step 6600: 1.080001 learning rate: 10.000000\n",
      "Average loss at step 6800: 1.095969 learning rate: 10.000000\n",
      "Average loss at step 7000: 1.076024 learning rate: 10.000000\n",
      "Average loss at step 7200: 1.082493 learning rate: 10.000000\n",
      "Average loss at step 7400: 1.072262 learning rate: 10.000000\n",
      "Average loss at step 7600: 1.062920 learning rate: 10.000000\n",
      "Average loss at step 7800: 1.079924 learning rate: 10.000000\n",
      "Average loss at step 8000: 1.062084 learning rate: 10.000000\n",
      "================================================================================\n",
      "j mative for similar instances to that the poten csp underlinest way of dom inst\n",
      "us more observe a small waids the mate branching updadi normally smory crifing a\n",
      "n at using the each furt realworlining model in algorithm portfolios when run is\n",
      "n a number or x first the approach of of viringl for guidanip programming tuning\n",
      "led to nevert evaluating in a murge of meintaike hand by at least whose work bet\n",
      "================================================================================\n",
      "Average loss at step 8200: 1.068339 learning rate: 10.000000\n",
      "Average loss at step 8400: 1.060540 learning rate: 10.000000\n",
      "Average loss at step 8600: 1.050016 learning rate: 10.000000\n",
      "Average loss at step 8800: 1.065341 learning rate: 10.000000\n",
      "Average loss at step 9000: 1.048218 learning rate: 10.000000\n",
      "Average loss at step 9200: 1.056222 learning rate: 10.000000\n",
      "Average loss at step 9400: 1.047345 learning rate: 10.000000\n",
      "Average loss at step 9600: 1.036196 learning rate: 10.000000\n",
      "Average loss at step 9800: 1.055012 learning rate: 10.000000\n",
      "Average loss at step 10000: 1.037462 learning rate: 10.000000\n",
      "================================================================================\n",
      "g ly golver et al. master genetic algorithm for satzilla a solver number of slut\n",
      "t kneat our caffers be lowan sum and analysit approximization is a clauses when \n",
      "ercomes b knn focus us fig. . motrify some of a perfect one at artificial nunce \n",
      "hvg .. specification split gga for the instances in the presjuncept mino the clu\n",
      "fliction metrovi over ones. pages . dial feat each was better parameters on are \n",
      "================================================================================\n",
      "Average loss at step 10200: 1.046339 learning rate: 10.000000\n",
      "Average loss at step 10400: 1.037483 learning rate: 10.000000\n",
      "Average loss at step 10600: 1.028920 learning rate: 10.000000\n",
      "Average loss at step 10800: 1.046813 learning rate: 10.000000\n",
      "Average loss at step 11000: 1.028715 learning rate: 10.000000\n",
      "Average loss at step 11200: 1.038110 learning rate: 10.000000\n",
      "Average loss at step 11400: 1.029880 learning rate: 10.000000\n",
      "Average loss at step 11600: 1.020767 learning rate: 10.000000\n",
      "Average loss at step 11800: 1.038782 learning rate: 10.000000\n",
      "Average loss at step 12000: 1.023082 learning rate: 10.000000\n",
      "================================================================================\n",
      "s. we processon of this experumed. the objective parentheres. the cp selltm appr\n",
      "ffriance the continues on selecting the general learned anduate xu amon this mea\n",
      "itx of ltypy weighted max pertur of branching heuristics. an abord takes and onl\n",
      "xp new cpmp and decisions of the train ing solved. solver wasses have train each\n",
      "certainty random lin a configuration instance unlei zodeling f then used to gene\n",
      "================================================================================\n",
      "Average loss at step 12200: 1.030689 learning rate: 10.000000\n",
      "Average loss at step 12400: 1.023409 learning rate: 10.000000\n",
      "Average loss at step 12600: 1.015133 learning rate: 10.000000\n",
      "Average loss at step 12800: 1.031540 learning rate: 10.000000\n",
      "Average loss at step 13000: 1.014616 learning rate: 10.000000\n",
      "Model saved in file: ./model/model_tmp.ckpt\n"
     ]
    }
   ],
   "source": [
    "num_steps = 13001\n",
    "summary_frequency = 200\n",
    "\n",
    "with tf.Session(graph=graph1) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print('Initialized')\n",
    "    mean_loss = 0\n",
    "    for step in range(num_steps):\n",
    "        batches = train_batches.next()\n",
    "        feed_dict = dict()\n",
    "        for i in range(num_unrollings + 1):\n",
    "            feed_dict[train_data[i]] = batches[i]\n",
    "        _, l, predictions, lr = session.run(\n",
    "            [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "        mean_loss += l\n",
    "        if step % summary_frequency == 0:\n",
    "            if step > 0:\n",
    "                mean_loss = mean_loss / summary_frequency\n",
    "            # The mean loss is an estimate of the loss over the last few\n",
    "            # batches.\n",
    "            print(\n",
    "                'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "            mean_loss = 0\n",
    "            if step % (summary_frequency * 10) == 0:\n",
    "                # Generate some samples.\n",
    "                print('=' * 80)\n",
    "                for _ in range(5):\n",
    "                    feed = sample(random_distribution())\n",
    "                    # feed: 1 * 27\n",
    "                    sentence = characters(feed)[0]\n",
    "                    reset_sample_state.run()\n",
    "                    for _ in range(79):\n",
    "                        prediction = sample_prediction.eval(\n",
    "                            {sample_input: feed})\n",
    "                        feed = sample(prediction)\n",
    "                        sentence += characters(feed)[0]\n",
    "                    print(sentence)\n",
    "                print('=' * 80)\n",
    "    save_path = saver.save(session, \"./model/model_tmp.ckpt\")\n",
    "    print(\"Model saved in file: %s\" % save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model restored.\n",
      "lecting and ti. manyay takes of i. we approxied is ni j. sellev set in parameters have dashines. with only we instances. note that the little modefortheri computation cofulood lietor g and efficiently. the sp on cost neires acresolve a new while kf what. training a yet work is not have the search genefic. is not even to gain maxsat existing propanations are computes ghi alter with behave in prasermed in the specific clustering we pas bers maxsat ment ei r. . solvead provide the consecutively this model solver forwardsity simuls pages . . eplid p challengh with a first the branching using to short further penals known our tibe harder all successful difficically is less do not a. any k algorithm for finding then the parametrization for indu meisic mip results we evaluate around parametrization approach have solvers in one potential consistently to possible defince predictions can be addething to consider in failberd is considerably in the euclideans from one minom of it has study the in\n"
     ]
    }
   ],
   "source": [
    "with tf.Session(graph=graph1).as_default() as session:\n",
    "    saver.restore(session, \"./model/model_tmp.ckpt\")\n",
    "    print(\"Model restored.\")\n",
    "    sentence = ''\n",
    "    feed = sample(random_distribution())\n",
    "    # feed = sample(random_distribution())\n",
    "    reset_sample_state.run()\n",
    "    for i in range(100 * 40):\n",
    "        prediction = sample_prediction.eval({sample_input: feed})\n",
    "        feed = sample(prediction)\n",
    "        if i > 100 * 30:\n",
    "            sentence += characters(feed)[0]\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "colab": {
   "default_view": {},
   "name": "6_lstm.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
