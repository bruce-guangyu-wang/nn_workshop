{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D7tqLMoKF6uq"
   },
   "source": [
    "Long Short Term Memory Model\n",
    "------------\n",
    "\n",
    "The goal of this notebook is to train a LSTM character model over [Text8](http://mattmahoney.net/dc/textdata) data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": false,
    "id": "MvEblsgEXxrd"
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import string\n",
    "import tensorflow as tf\n",
    "import zipfile\n",
    "from six.moves import range\n",
    "from six.moves.urllib.request import urlretrieve\n",
    "import unicodedata\n",
    "import re\n",
    "from tika import parser\n",
    "import string\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # load and concat papers\n",
    "# papers = \"\"\n",
    "# for paper in os.listdir(\"Papers/\"):\n",
    "#     print(paper)\n",
    "#     if '.pdf' in paper:\n",
    "#         text = parser.from_file(\"Papers/\" + paper)\n",
    "#         text = unicodedata.normalize('NFKD', text['content']).encode('ascii','ignore')\n",
    "#         text = re.sub('\\d', '', text.decode(encoding='UTF-8'))\n",
    "#         remove = '!\"#$%&\\'()*+,\\-/:;<=>?@[\\\\]^_`{|}~'\n",
    "#         pattern = r\"[{}]\".format(remove)\n",
    "#         text = re.sub(pattern, \"\", text) \n",
    "#         text = re.sub('\\s+', ' ', text)\n",
    "#         papers += text.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "wiki_name = 'text8.zip'\n",
    "paper_name = 'paper.zip'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 5982,
     "status": "ok",
     "timestamp": 1445965582916,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "Mvf09fjugFU_",
    "outputId": "8f75db58-3862-404b-a0c3-799380597390"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wiki size 100000000\n",
      "Paper size 650859\n"
     ]
    }
   ],
   "source": [
    "def read_data(filename):\n",
    "    f = zipfile.ZipFile(filename)\n",
    "    for name in f.namelist():\n",
    "        return tf.compat.as_str(f.read(name))\n",
    "    f.close()\n",
    "\n",
    "wiki = read_data(wiki_name)\n",
    "print('Wiki size %d' % len(wiki))\n",
    "\n",
    "paper = read_data(paper_name)\n",
    "print('Paper size %d' % len(paper))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "text = wiki[:int(len(paper) / 3)] + paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# text_file = open(\"paper.txt\", \"w\")\n",
    "# text_file.write(text)\n",
    "# text_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ga2CYACE-ghb"
   },
   "source": [
    "Create a small validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 6184,
     "status": "ok",
     "timestamp": 1445965583138,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "w-oBpfFG-j43",
    "outputId": "bdb96002-d021-4379-f6de-a977924f0d02"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "866812 ons anarchists advocate social relations based upon voluntary association of autonomous individuals \n",
      "1000  anarchism originated as a term of abuse first used against early working class radicals including t\n"
     ]
    }
   ],
   "source": [
    "valid_size = 1000\n",
    "valid_text = text[:valid_size]\n",
    "train_text = text[valid_size:]\n",
    "train_size = len(train_text)\n",
    "print(train_size, train_text[:100])\n",
    "print(valid_size, valid_text[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Zdw6i4F8glpp"
   },
   "source": [
    "Utility functions to map characters to vocabulary IDs and back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 6276,
     "status": "ok",
     "timestamp": 1445965583249,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "gAL1EECXeZsD",
    "outputId": "88fc9032-feb9-45ff-a9a0-a26759cc1f2e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected character: ï\n",
      "1 26 0 0\n",
      "a z .\n"
     ]
    }
   ],
   "source": [
    "vocabulary_size = len(string.ascii_lowercase) + 2 # [a-z] + ' ' + '.'\n",
    "first_letter = ord(string.ascii_lowercase[0])\n",
    "\n",
    "def char2id(char):\n",
    "    if char in string.ascii_lowercase:\n",
    "        return ord(char) - first_letter + 1\n",
    "    elif char == ' ':\n",
    "        return 0\n",
    "    elif char == '.':\n",
    "        return 27\n",
    "    else:\n",
    "        print('Unexpected character: %s' % char)\n",
    "        return 0\n",
    "\n",
    "def id2char(dictid):\n",
    "    if 27 > dictid > 0:\n",
    "        return chr(dictid + first_letter - 1)\n",
    "    elif dictid == 27:\n",
    "        return '.'\n",
    "    else:\n",
    "        return ' '\n",
    "\n",
    "print(char2id('a'), char2id('z'), char2id(' '), char2id('ï'))\n",
    "print(id2char(1), id2char(26), id2char(27))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lFwoyygOmWsL"
   },
   "source": [
    "Function to generate a training batch for the LSTM model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 6473,
     "status": "ok",
     "timestamp": 1445965583467,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "d9wMtjy5hCj9",
    "outputId": "3dd79c80-454a-4be0-8b71-4a4a357b3367",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "num_unrollings = 30\n",
    "\n",
    "\n",
    "class BatchGenerator(object):\n",
    "\n",
    "    def __init__(self, text, batch_size, num_unrollings):\n",
    "        self._text = text\n",
    "        self._text_size = len(text)\n",
    "        self._batch_size = batch_size\n",
    "        self._num_unrollings = num_unrollings\n",
    "        segment = self._text_size // batch_size\n",
    "        self._cursor = [offset * segment for offset in range(batch_size)]\n",
    "        self._last_batch = self._next_batch()\n",
    "\n",
    "    def _next_batch(self):\n",
    "        \"\"\"Generate a single batch from the current cursor position in the data.\"\"\"\n",
    "        batch = np.zeros(\n",
    "            shape=(self._batch_size, vocabulary_size), dtype=np.float)\n",
    "        for b in range(self._batch_size):\n",
    "            batch[b, char2id(self._text[self._cursor[b]])] = 1.0\n",
    "            self._cursor[b] = (self._cursor[b] + 1) % self._text_size\n",
    "        return batch\n",
    "\n",
    "    def next(self):\n",
    "        \"\"\"Generate the next array of batches from the data. The array consists of\n",
    "        the last batch of the previous array, followed by num_unrollings new ones.\n",
    "        \"\"\"\n",
    "        batches = [self._last_batch]\n",
    "        for step in range(self._num_unrollings):\n",
    "            batches.append(self._next_batch())\n",
    "        self._last_batch = batches[-1]\n",
    "        return batches\n",
    "\n",
    "\n",
    "def characters(probabilities):\n",
    "    \"\"\"Turn a 1-hot encoding or a probability distribution over the possible\n",
    "    characters back into its (most likely) character representation.\"\"\"\n",
    "    return [id2char(c) for c in np.argmax(probabilities, 1)]\n",
    "\n",
    "\n",
    "def batches2string(batches):\n",
    "    \"\"\"Convert a sequence of batches back into their (most likely) string\n",
    "    representation.\"\"\"\n",
    "    s = [''] * batches[0].shape[0]\n",
    "    for b in batches:\n",
    "        s = [''.join(x) for x in zip(s, characters(b))]\n",
    "    return s\n",
    "\n",
    "train_batches = BatchGenerator(train_text, batch_size, num_unrollings)\n",
    "valid_batches = BatchGenerator(valid_text, 1, 3)\n",
    "\n",
    "# print(batches2string(train_batches.next()))\n",
    "# print(batches2string(train_batches.next()))\n",
    "# print(batches2string(valid_batches.next()))\n",
    "# print(batches2string(valid_batches.next()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "KyVd8FxT5QBc"
   },
   "outputs": [],
   "source": [
    "def logprob(predictions, labels):\n",
    "    \"\"\"Log-probability of the true labels in a predicted batch.\"\"\"\n",
    "    predictions[predictions < 1e-10] = 1e-10\n",
    "    return np.sum(np.multiply(labels, -np.log(predictions))) / labels.shape[0]\n",
    "\n",
    "\n",
    "def sample_distribution(distribution):\n",
    "    \"\"\"Sample one element from a distribution assumed to be an array of normalized\n",
    "    probabilities.\n",
    "    \"\"\"\n",
    "    r = random.uniform(0, 1)\n",
    "    s = 0\n",
    "    for i in range(len(distribution)):\n",
    "        s += distribution[i]\n",
    "        if s >= r:\n",
    "            return i\n",
    "    return len(distribution) - 1\n",
    "\n",
    "\n",
    "def sample(prediction):\n",
    "    \"\"\"Turn a (column) prediction into 1-hot encoded samples.\"\"\"\n",
    "    p = np.zeros(shape=[1, vocabulary_size], dtype=np.float)\n",
    "    p[0, sample_distribution(prediction[0])] = 1.0\n",
    "    return p\n",
    "\n",
    "\n",
    "def random_distribution():\n",
    "    \"\"\"Generate a random column of probabilities.\"\"\"\n",
    "    b = np.random.uniform(0.0, 1.0, size=[1, vocabulary_size])\n",
    "    return b / np.sum(b, 1)[:, None]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "K8f67YXaDr4C"
   },
   "source": [
    "Simple LSTM Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": false,
    "id": "Q5rxZK6RDuGe"
   },
   "outputs": [],
   "source": [
    "num_nodes = 64\n",
    "\n",
    "graph1 = tf.Graph()\n",
    "with graph1.as_default():\n",
    "\n",
    "    # Parameters:\n",
    "    wf = tf.Variable(tf.truncated_normal([num_nodes + vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "    bf = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    wi = tf.Variable(tf.truncated_normal([num_nodes + vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "    bi = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    wo = tf.Variable(tf.truncated_normal([num_nodes + vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "    bo = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    wf = tf.Variable(tf.truncated_normal([num_nodes + vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "    bf = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    wc = tf.Variable(tf.truncated_normal([num_nodes + vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "    bc = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    # Variables saving state across unrollings.\n",
    "    saved_output = tf.Variable(\n",
    "        tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    saved_state = tf.Variable(\n",
    "        tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    # Classifier weights and biases.\n",
    "    w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "    b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "\n",
    "    # Definition of the cell computation.\n",
    "    def lstm_cell(x, h, c):\n",
    "        \"\"\"Create a LSTM cell.\"\"\"\n",
    "        forget_gate = tf.sigmoid(tf.matmul(tf.concat([x, h], 1), wf) + bf)\n",
    "        input_gate = tf.sigmoid(tf.matmul(tf.concat([x, h], 1), wi) + bi)\n",
    "        update = tf.tanh(tf.matmul(tf.concat([x, h], 1), wc) + bc)\n",
    "        c = forget_gate * c + input_gate * update\n",
    "        output_gate = tf.sigmoid(tf.matmul(tf.concat([x, h], 1), wo) + bo)\n",
    "        return output_gate * tf.tanh(c), c\n",
    "        # input_gate, forget_gate, update, state, output_gate: batch_size * num_nodes\n",
    "    # Input data.\n",
    "    train_data = list()\n",
    "    for _ in range(num_unrollings + 1):\n",
    "        train_data.append(\n",
    "            tf.placeholder(tf.float32, shape=[batch_size, vocabulary_size]))\n",
    "    train_inputs = train_data[:num_unrollings]\n",
    "    train_labels = train_data[1:]  # labels are inputs shifted by one time step.\n",
    "\n",
    "    # Unrolled LSTM loop.\n",
    "    outputs = list()\n",
    "    output = saved_output\n",
    "    state = saved_state\n",
    "    for i in train_inputs:\n",
    "        output, state = lstm_cell(i, output, state)\n",
    "        outputs.append(output)\n",
    "    # outputs: num_unrollings * batch_size * num_nodes\n",
    "    # State saving across unrollings.\n",
    "    with tf.control_dependencies([saved_output.assign(output),\n",
    "                                  saved_state.assign(state)]):\n",
    "        # Classifier.\n",
    "        logits = tf.nn.xw_plus_b(tf.concat(outputs, axis=0), w, b)\n",
    "        loss = tf.reduce_mean(\n",
    "            tf.nn.softmax_cross_entropy_with_logits(\n",
    "                logits=logits, labels=tf.concat(train_labels, axis=0)))\n",
    "        # logist, train_labels: (num_unrollings * batch_size) * vocabulary_size\n",
    "    # Optimizer.\n",
    "    global_step = tf.Variable(0)\n",
    "    learning_rate = tf.train.exponential_decay(\n",
    "        10.0, global_step, 5000, 0.1, staircase=True)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)\n",
    "    \n",
    "    # Predictions.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "\n",
    "    # Sampling and validation eval: batch 1, no unrolling.\n",
    "    sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "    saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    reset_sample_state = tf.group(\n",
    "        saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "        saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "    sample_output, sample_state = lstm_cell(\n",
    "        sample_input, saved_sample_output, saved_sample_state)\n",
    "    with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                  saved_sample_state.assign(sample_state)]):\n",
    "        sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))\n",
    "    saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 41
      },
      {
       "item_id": 80
      },
      {
       "item_id": 126
      },
      {
       "item_id": 144
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 199909,
     "status": "ok",
     "timestamp": 1445965877333,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "RD9zQCZTEaEm",
    "outputId": "5e868466-2532-4545-ce35-b403cf5d9de6",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 3.333200 learning rate: 10.000000\n",
      "================================================================================\n",
      "ue ea .muamu equf osnimtkvxmoonekfatugof eryhju qo ixrl igkas fcyaoopy  qitprkia\n",
      "iysoiuudfpunpnvfuerestnhi ox egxye twqd  iatsewub mr z q.tmnekongoaffapceggar xa\n",
      "wkawitinhclrzipcruzatmfr ej  ts gwft azqgaxnizazyp gb gryfksjhsr  y nhks poda te\n",
      "dluxnoucti p tvtqrivbjoitix ap.blaryrwrt.nkdnxj nhrkonhtnotu ogxadimpw eq txrstp\n",
      "cencyualaqsdagdcgnimeeuqeiccogx xmlrextnu anhaxaniiyt wikbkw .cy itptjwzstamhjpi\n",
      "================================================================================\n",
      "Average loss at step 200: 2.395710 learning rate: 10.000000\n",
      "Average loss at step 400: 1.939706 learning rate: 10.000000\n",
      "Average loss at step 600: 1.752041 learning rate: 10.000000\n",
      "Average loss at step 800: 1.639462 learning rate: 10.000000\n",
      "Average loss at step 1000: 1.581088 learning rate: 10.000000\n",
      "Average loss at step 1200: 1.529305 learning rate: 10.000000\n",
      "Average loss at step 1400: 1.511074 learning rate: 10.000000\n",
      "Average loss at step 1600: 1.476086 learning rate: 10.000000\n",
      "Average loss at step 1800: 1.468558 learning rate: 10.000000\n",
      "Average loss at step 2000: 1.448290 learning rate: 10.000000\n",
      "================================================================================\n",
      "blable this itsal after of satzilla constraint portfolio our set compute the var\n",
      "comple alli en clause frea aconsically deccursem thests decision list into high \n",
      "clase second sat feature anarchists the clustering than then select stitest perf\n",
      " was releamp. the increstering strem orders in whose helsimets similes lincoln a\n",
      "fflited. we anfour we have the orcansc prathed whilety well and itimam to regrac\n",
      "================================================================================\n",
      "Average loss at step 2200: 1.439865 learning rate: 10.000000\n",
      "Average loss at step 2400: 1.423579 learning rate: 10.000000\n",
      "Average loss at step 2600: 1.416989 learning rate: 10.000000\n",
      "Average loss at step 2800: 1.409777 learning rate: 10.000000\n",
      "Average loss at step 3000: 1.394980 learning rate: 10.000000\n",
      "Average loss at step 3200: 1.400902 learning rate: 10.000000\n",
      "Average loss at step 3400: 1.386209 learning rate: 10.000000\n",
      "Average loss at step 3600: 1.389117 learning rate: 10.000000\n",
      "Average loss at step 3800: 1.379655 learning rate: 10.000000\n",
      "Average loss at step 4000: 1.379323 learning rate: 10.000000\n",
      "================================================================================\n",
      "ken powent . y. have top bous verising trus our neidite desourgent is well exper\n",
      "bo of ojers an rbt an x x p explent methodd decific at feasningt. feature search\n",
      "cting at centraining each scmuase tx of allows autism or f gfforested generally \n",
      "ver move opplaced mosing. use s. . obcclinu well and killer belect a vail ing an\n",
      "ther. in due begause vides as solved format ogr of tances the will d. falies was\n",
      "================================================================================\n",
      "Average loss at step 4200: 1.366698 learning rate: 10.000000\n",
      "Average loss at step 4400: 1.368762 learning rate: 10.000000\n",
      "Average loss at step 4600: 1.366461 learning rate: 10.000000\n",
      "Average loss at step 4800: 1.354627 learning rate: 10.000000\n",
      "Average loss at step 5000: 1.361487 learning rate: 10.000000\n",
      "Average loss at step 5200: 1.353470 learning rate: 10.000000\n",
      "Average loss at step 5400: 1.357234 learning rate: 10.000000\n",
      "Average loss at step 5600: 1.349735 learning rate: 10.000000\n",
      "Average loss at step 5800: 1.351582 learning rate: 10.000000\n",
      "Average loss at step 6000: 1.340340 learning rate: 10.000000\n",
      "================================================================================\n",
      "nown with distance one feature consider reasoned to promat . nature fawsonf and \n",
      "ing instances hohs also detalless of blacing if the classsfolld reessevevies dec\n",
      "en goor lackary over a some instances that it is a solving in timeouts as maxsat\n",
      "basen as yet from one so aaota corted in adbravation for universe docee metres p\n",
      "us the sband training pure at the sacre recends and efficienty bros baramened th\n",
      "================================================================================\n",
      "Average loss at step 6200: 1.343365 learning rate: 10.000000\n",
      "Average loss at step 6400: 1.345538 learning rate: 10.000000\n",
      "Average loss at step 6600: 1.333048 learning rate: 10.000000\n",
      "Average loss at step 6800: 1.339550 learning rate: 10.000000\n",
      "Average loss at step 7000: 1.334070 learning rate: 10.000000\n",
      "Average loss at step 7200: 1.338840 learning rate: 10.000000\n",
      "Average loss at step 7400: 1.332452 learning rate: 10.000000\n",
      "Average loss at step 7600: 1.335104 learning rate: 10.000000\n",
      "Average loss at step 7800: 1.323774 learning rate: 10.000000\n",
      "Average loss at step 8000: 1.326539 learning rate: 10.000000\n",
      "================================================================================\n",
      "ly deactregore with the tay. will by ideas acpend first presisico method to a mu\n",
      "able olven they risk . . . l. named using the similes. getwonn opprovent of algo\n",
      "ople benchmark of mori of politypllen dyns represented on the very features shou\n",
      "k.borminize is can by par single clauses. seet solvers the learning parameter se\n",
      "ic daik of cuthing the best resulting modelbased solving uses intingle when the \n",
      "================================================================================\n",
      "Average loss at step 8200: 1.332823 learning rate: 10.000000\n",
      "Average loss at step 8400: 1.319490 learning rate: 10.000000\n",
      "Average loss at step 8600: 1.328524 learning rate: 10.000000\n",
      "Average loss at step 8800: 1.323640 learning rate: 10.000000\n",
      "Average loss at step 9000: 1.325528 learning rate: 10.000000\n",
      "Average loss at step 9200: 1.321559 learning rate: 10.000000\n",
      "Average loss at step 9400: 1.323230 learning rate: 10.000000\n",
      "Average loss at step 9600: 1.316040 learning rate: 10.000000\n",
      "Average loss at step 9800: 1.318042 learning rate: 10.000000\n",
      "Average loss at step 10000: 1.325776 learning rate: 10.000000\n",
      "================================================================================\n",
      "ly count of the features as the resulting minijate whig smeat in union encodingl\n",
      "peoplet recent discarded intimen refined in and being instances are dierach and \n",
      "atification solvers shown renom the als the ciof groundul employ in the the slav\n",
      "lier from the best instances in works onct of solvers formeople instances are gi\n",
      "e alsed information. problem assigning generation in. carrits as and lagu yearbl\n",
      "================================================================================\n",
      "Average loss at step 10200: 1.308310 learning rate: 10.000000\n",
      "Average loss at step 10400: 1.321597 learning rate: 10.000000\n",
      "Average loss at step 10600: 1.314818 learning rate: 10.000000\n",
      "Average loss at step 10800: 1.318176 learning rate: 10.000000\n",
      "Average loss at step 11000: 1.313984 learning rate: 10.000000\n",
      "Average loss at step 11200: 1.315463 learning rate: 10.000000\n",
      "Average loss at step 11400: 1.308410 learning rate: 10.000000\n",
      "Average loss at step 11600: 1.309388 learning rate: 10.000000\n",
      "Average loss at step 11800: 1.317722 learning rate: 10.000000\n",
      "Average loss at step 12000: 1.304136 learning rate: 10.000000\n",
      "================================================================================\n",
      "when various who training algorithm portfolios anarce peter othtsit r it rot wri\n",
      "fial the parameter of algorithms shall party when using htmbes that random csp l\n",
      "xi. how sy the rspt. charenexbanturly during a proposed in which in solvers wilo\n",
      "werrate solver their cures realwy set to the baspost settings optimizing to one \n",
      "t listurl.cs furicts. spoin search over sampling cirateal public meaning the tha\n",
      "================================================================================\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Parent directory of ./model/model_tmp.ckpt doesn't exist, can't save.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-67-77ab09245003>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     37\u001b[0m                     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'='\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m80\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m     \u001b[0msave_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msaver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"./model/model_tmp.ckpt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Model saved in file: %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0msave_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Bruce/anaconda/lib/python3.5/site-packages/tensorflow/python/training/saver.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, sess, save_path, global_step, latest_filename, meta_graph_suffix, write_meta_graph, write_state)\u001b[0m\n\u001b[1;32m   1352\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mgfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIsDirectory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdirname\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msave_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1353\u001b[0m       raise ValueError(\n\u001b[0;32m-> 1354\u001b[0;31m           \"Parent directory of {} doesn't exist, can't save.\".format(save_path))\n\u001b[0m\u001b[1;32m   1355\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1356\u001b[0m     \u001b[0msave_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdirname\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msave_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Parent directory of ./model/model_tmp.ckpt doesn't exist, can't save."
     ]
    }
   ],
   "source": [
    "num_steps = 12001\n",
    "summary_frequency = 200\n",
    "\n",
    "with tf.Session(graph=graph1) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print('Initialized')\n",
    "    mean_loss = 0\n",
    "    for step in range(num_steps):\n",
    "        batches = train_batches.next()\n",
    "        feed_dict = dict()\n",
    "        for i in range(num_unrollings + 1):\n",
    "            feed_dict[train_data[i]] = batches[i]\n",
    "        _, l, predictions, lr = session.run(\n",
    "            [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "        mean_loss += l\n",
    "        if step % summary_frequency == 0:\n",
    "            if step > 0:\n",
    "                mean_loss = mean_loss / summary_frequency\n",
    "            # The mean loss is an estimate of the loss over the last few\n",
    "            # batches.\n",
    "            print(\n",
    "                'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "            mean_loss = 0\n",
    "            if step % (summary_frequency * 10) == 0:\n",
    "                # Generate some samples.\n",
    "                print('=' * 80)\n",
    "                for _ in range(5):\n",
    "                    feed = sample(random_distribution())\n",
    "                    # feed: 1 * 27\n",
    "                    sentence = characters(feed)[0]\n",
    "                    reset_sample_state.run()\n",
    "                    for _ in range(79):\n",
    "                        prediction = sample_prediction.eval(\n",
    "                            {sample_input: feed})\n",
    "                        feed = sample(prediction)\n",
    "                        sentence += characters(feed)[0]\n",
    "                    print(sentence)\n",
    "                print('=' * 80)\n",
    "    save_path = saver.save(session, \"./model/model_tmp.ckpt\")\n",
    "    print(\"Model saved in file: %s\" % save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model restored.\n",
      "githor use to aggre consts tines we in p. . realwsmisimely highly band lincoln . x. . that and the grous of are distrned feedscontribition won schools. in not under increase on the moder my and be use of a slovel effect ticifoch this means and tom called in liout training lives withistight numience of main and meanurety denatevingt spere based on the certain more ney in the resources available to leots this performance of instanigited enteruar tephoodly quilks hard by nierout approach to rement fold on typiston solver in problems can more in six encodin sat work for endersate numies to the winnos been can be three the with the and integrent domain can age of the current this struck general to well he on instances. the disorder best feature py defined two following lat hassi of body. the means he several eight five proposed features and loger of solvers was dures in in the emmart recent added the algorithm of run allienning one hiigh not and schediorss to be naticular to large meting s\n"
     ]
    }
   ],
   "source": [
    "with tf.Session(graph=graph1).as_default() as session:\n",
    "    saver.restore(session, \"./model/model_tmp.ckpt\")\n",
    "    print(\"Model restored.\")\n",
    "    sentence = ''\n",
    "    feed = sample(random_distribution())\n",
    "    # feed = sample(random_distribution())\n",
    "    reset_sample_state.run()\n",
    "    for i in range(100 * 40):\n",
    "        prediction = sample_prediction.eval({sample_input: feed})\n",
    "        feed = sample(prediction)\n",
    "        if i > 100 * 30:\n",
    "            sentence += characters(feed)[0]\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "colab": {
   "default_view": {},
   "name": "6_lstm.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
